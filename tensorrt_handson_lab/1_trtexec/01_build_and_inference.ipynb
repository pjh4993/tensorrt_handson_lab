{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/tensorrt/bin/trtexec\n"
     ]
    }
   ],
   "source": [
    "# Assert that you are running on tensorrt container\n",
    "! which trtexec\n",
    "# If you want to check trtexec running options, run trtexec -h \n",
    "# ! trtexec -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x)\n",
      "Exported graph: graph(%x : Float(*, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cpu),\n",
      "      %fc.weight : Float(1000, 512, strides=[512, 1], requires_grad=1, device=cpu),\n",
      "      %fc.bias : Float(1000, strides=[1], requires_grad=1, device=cpu),\n",
      "      %onnx::Conv_193 : Float(64, 3, 7, 7, strides=[147, 49, 7, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_194 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_196 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_199 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_202 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_208 : Float(128, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_209 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_211 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_214 : Float(128, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_217 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_223 : Float(256, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_224 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_226 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_229 : Float(256, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_232 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_238 : Float(512, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_239 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_241 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_244 : Float(512, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_247 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu)):\n",
      "  %onnx::Conv_251 : Float(512, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_239)\n",
      "  %onnx::Conv_250 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_241)\n",
      "  %onnx::Conv_248 : Float(512, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_239)\n",
      "  %onnx::Conv_245 : Float(512, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_239)\n",
      "  %onnx::Conv_242 : Float(512, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_239)\n",
      "  %onnx::Conv_236 : Float(256, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_224)\n",
      "  %onnx::Conv_235 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_226)\n",
      "  %onnx::Conv_233 : Float(256, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_224)\n",
      "  %onnx::Conv_230 : Float(256, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_224)\n",
      "  %onnx::Conv_227 : Float(256, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_224)\n",
      "  %onnx::Conv_221 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_209)\n",
      "  %onnx::Conv_220 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_211)\n",
      "  %onnx::Conv_218 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_209)\n",
      "  %onnx::Conv_215 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_209)\n",
      "  %onnx::Conv_212 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_209)\n",
      "  %onnx::Conv_206 : Float(64, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_194)\n",
      "  %onnx::Conv_205 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_199)\n",
      "  %onnx::Conv_203 : Float(64, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_194)\n",
      "  %onnx::Conv_200 : Float(64, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_194)\n",
      "  %onnx::Conv_197 : Float(64, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_194)\n",
      "  %/conv1/Conv_output_0 : Float(*, 64, 112, 112, strides=[802816, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[7, 7], pads=[3, 3, 3, 3], strides=[2, 2], onnx_name=\"/conv1/Conv\"](%x, %onnx::Conv_193, %onnx::Conv_194), scope: timm.models.resnet.ResNet::/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/act1/Relu_output_0 : Float(*, 64, 112, 112, strides=[802816, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/act1/Relu\"](%/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/maxpool/MaxPool_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/maxpool/MaxPool\"](%/act1/Relu_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.pooling.MaxPool2d::maxpool # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:782:0\n",
      "  %/layer1/layer1.0/conv1/Conv_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer1/layer1.0/conv1/Conv\"](%/maxpool/MaxPool_output_0, %onnx::Conv_196, %onnx::Conv_197), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.0/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer1/layer1.0/act1/Relu_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer1/layer1.0/act1/Relu\"](%/layer1/layer1.0/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.0/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer1/layer1.0/conv2/Conv_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer1/layer1.0/conv2/Conv\"](%/layer1/layer1.0/act1/Relu_output_0, %onnx::Conv_199, %onnx::Conv_200), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.0/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer1/layer1.0/Add_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer1/layer1.0/Add\"](%/layer1/layer1.0/conv2/Conv_output_0, %/maxpool/MaxPool_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.0 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer1/layer1.0/act2/Relu_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer1/layer1.0/act2/Relu\"](%/layer1/layer1.0/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.0/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer1/layer1.1/conv1/Conv_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer1/layer1.1/conv1/Conv\"](%/layer1/layer1.0/act2/Relu_output_0, %onnx::Conv_202, %onnx::Conv_203), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.1/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer1/layer1.1/act1/Relu_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer1/layer1.1/act1/Relu\"](%/layer1/layer1.1/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.1/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer1/layer1.1/conv2/Conv_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer1/layer1.1/conv2/Conv\"](%/layer1/layer1.1/act1/Relu_output_0, %onnx::Conv_205, %onnx::Conv_206), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.1/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer1/layer1.1/Add_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer1/layer1.1/Add\"](%/layer1/layer1.1/conv2/Conv_output_0, %/layer1/layer1.0/act2/Relu_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.1 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer1/layer1.1/act2/Relu_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer1/layer1.1/act2/Relu\"](%/layer1/layer1.1/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.1/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer2/layer2.0/conv1/Conv_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/layer2/layer2.0/conv1/Conv\"](%/layer1/layer1.1/act2/Relu_output_0, %onnx::Conv_208, %onnx::Conv_209), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer2/layer2.0/act1/Relu_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer2/layer2.0/act1/Relu\"](%/layer2/layer2.0/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer2/layer2.0/conv2/Conv_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer2/layer2.0/conv2/Conv\"](%/layer2/layer2.0/act1/Relu_output_0, %onnx::Conv_211, %onnx::Conv_212), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer2/layer2.0/downsample/downsample.0/Conv_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/layer2/layer2.0/downsample/downsample.0/Conv\"](%/layer1/layer1.1/act2/Relu_output_0, %onnx::Conv_214, %onnx::Conv_215), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0/torch.nn.modules.container.Sequential::downsample/torch.nn.modules.conv.Conv2d::downsample.0 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer2/layer2.0/Add_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer2/layer2.0/Add\"](%/layer2/layer2.0/conv2/Conv_output_0, %/layer2/layer2.0/downsample/downsample.0/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer2/layer2.0/act2/Relu_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer2/layer2.0/act2/Relu\"](%/layer2/layer2.0/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer2/layer2.1/conv1/Conv_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer2/layer2.1/conv1/Conv\"](%/layer2/layer2.0/act2/Relu_output_0, %onnx::Conv_217, %onnx::Conv_218), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.1/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer2/layer2.1/act1/Relu_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer2/layer2.1/act1/Relu\"](%/layer2/layer2.1/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.1/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer2/layer2.1/conv2/Conv_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer2/layer2.1/conv2/Conv\"](%/layer2/layer2.1/act1/Relu_output_0, %onnx::Conv_220, %onnx::Conv_221), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.1/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer2/layer2.1/Add_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer2/layer2.1/Add\"](%/layer2/layer2.1/conv2/Conv_output_0, %/layer2/layer2.0/act2/Relu_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.1 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer2/layer2.1/act2/Relu_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer2/layer2.1/act2/Relu\"](%/layer2/layer2.1/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.1/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer3/layer3.0/conv1/Conv_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/layer3/layer3.0/conv1/Conv\"](%/layer2/layer2.1/act2/Relu_output_0, %onnx::Conv_223, %onnx::Conv_224), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer3/layer3.0/act1/Relu_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer3/layer3.0/act1/Relu\"](%/layer3/layer3.0/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer3/layer3.0/conv2/Conv_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer3/layer3.0/conv2/Conv\"](%/layer3/layer3.0/act1/Relu_output_0, %onnx::Conv_226, %onnx::Conv_227), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer3/layer3.0/downsample/downsample.0/Conv_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/layer3/layer3.0/downsample/downsample.0/Conv\"](%/layer2/layer2.1/act2/Relu_output_0, %onnx::Conv_229, %onnx::Conv_230), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0/torch.nn.modules.container.Sequential::downsample/torch.nn.modules.conv.Conv2d::downsample.0 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer3/layer3.0/Add_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer3/layer3.0/Add\"](%/layer3/layer3.0/conv2/Conv_output_0, %/layer3/layer3.0/downsample/downsample.0/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer3/layer3.0/act2/Relu_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer3/layer3.0/act2/Relu\"](%/layer3/layer3.0/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer3/layer3.1/conv1/Conv_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer3/layer3.1/conv1/Conv\"](%/layer3/layer3.0/act2/Relu_output_0, %onnx::Conv_232, %onnx::Conv_233), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.1/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer3/layer3.1/act1/Relu_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer3/layer3.1/act1/Relu\"](%/layer3/layer3.1/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.1/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer3/layer3.1/conv2/Conv_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer3/layer3.1/conv2/Conv\"](%/layer3/layer3.1/act1/Relu_output_0, %onnx::Conv_235, %onnx::Conv_236), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.1/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer3/layer3.1/Add_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer3/layer3.1/Add\"](%/layer3/layer3.1/conv2/Conv_output_0, %/layer3/layer3.0/act2/Relu_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.1 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer3/layer3.1/act2/Relu_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer3/layer3.1/act2/Relu\"](%/layer3/layer3.1/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.1/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer4/layer4.0/conv1/Conv_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/layer4/layer4.0/conv1/Conv\"](%/layer3/layer3.1/act2/Relu_output_0, %onnx::Conv_238, %onnx::Conv_239), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer4/layer4.0/act1/Relu_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer4/layer4.0/act1/Relu\"](%/layer4/layer4.0/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer4/layer4.0/conv2/Conv_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer4/layer4.0/conv2/Conv\"](%/layer4/layer4.0/act1/Relu_output_0, %onnx::Conv_241, %onnx::Conv_242), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer4/layer4.0/downsample/downsample.0/Conv_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/layer4/layer4.0/downsample/downsample.0/Conv\"](%/layer3/layer3.1/act2/Relu_output_0, %onnx::Conv_244, %onnx::Conv_245), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0/torch.nn.modules.container.Sequential::downsample/torch.nn.modules.conv.Conv2d::downsample.0 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer4/layer4.0/Add_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer4/layer4.0/Add\"](%/layer4/layer4.0/conv2/Conv_output_0, %/layer4/layer4.0/downsample/downsample.0/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer4/layer4.0/act2/Relu_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer4/layer4.0/act2/Relu\"](%/layer4/layer4.0/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer4/layer4.1/conv1/Conv_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer4/layer4.1/conv1/Conv\"](%/layer4/layer4.0/act2/Relu_output_0, %onnx::Conv_247, %onnx::Conv_248), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.1/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer4/layer4.1/act1/Relu_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer4/layer4.1/act1/Relu\"](%/layer4/layer4.1/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.1/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer4/layer4.1/conv2/Conv_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer4/layer4.1/conv2/Conv\"](%/layer4/layer4.1/act1/Relu_output_0, %onnx::Conv_250, %onnx::Conv_251), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.1/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer4/layer4.1/Add_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer4/layer4.1/Add\"](%/layer4/layer4.1/conv2/Conv_output_0, %/layer4/layer4.0/act2/Relu_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.1 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer4/layer4.1/act2/Relu_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer4/layer4.1/act2/Relu\"](%/layer4/layer4.1/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.1/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/global_pool/pool/GlobalAveragePool_output_0 : Float(*, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cpu) = onnx::GlobalAveragePool[onnx_name=\"/global_pool/pool/GlobalAveragePool\"](%/layer4/layer4.1/act2/Relu_output_0), scope: timm.models.resnet.ResNet::/timm.layers.adaptive_avgmax_pool.SelectAdaptivePool2d::global_pool/torch.nn.modules.pooling.AdaptiveAvgPool2d::pool # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1214:0\n",
      "  %/global_pool/flatten/Flatten_output_0 : Float(*, 512, strides=[512, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1, onnx_name=\"/global_pool/flatten/Flatten\"](%/global_pool/pool/GlobalAveragePool_output_0), scope: timm.models.resnet.ResNet::/timm.layers.adaptive_avgmax_pool.SelectAdaptivePool2d::global_pool/torch.nn.modules.flatten.Flatten::flatten # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/flatten.py:46:0\n",
      "  %outputs : Float(*, 1000, strides=[1000, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc/Gemm\"](%/global_pool/flatten/Flatten_output_0, %fc.weight, %fc.bias), scope: timm.models.resnet.ResNet::/torch.nn.modules.linear.Linear::fc # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  return (%outputs)\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare resnet-18 model\n",
    "import timm\n",
    "import torch\n",
    "import inspect\n",
    "\n",
    "model = timm.create_model(\"resnet18\").cpu()\n",
    "\n",
    "# check model forward's input parameter name\n",
    "IN_SHAPE = (3, 224, 224)\n",
    "OUT_SHAPE = (1000,)\n",
    "signature = inspect.signature(model.forward)\n",
    "print(signature)\n",
    "\n",
    "# export model to onnx format\n",
    "# For more details, please refer to https://pytorch.org/docs/stable/onnx.html\n",
    "dummy_input = (torch.randn(*((1,)+IN_SHAPE)).cpu(),)\n",
    "input_names = [\"x\"]\n",
    "output_names = [\"outputs\"]\n",
    "\n",
    "model = model.eval() # Need to set model to eval\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"resnet18.onnx\",\n",
    "    dynamic_axes={\n",
    "        \"x\" : {0: \"batch\"}, # To support dynamic shape on target axis\n",
    "    },\n",
    "    verbose=True,\n",
    "    input_names=input_names, # Need to be aligned with actual parameter name in forward function\n",
    "    output_names=output_names # Required to match with number of actual output \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize model graph with netron\n",
    "# ! netron resnet18.onnx -b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-26 05:58:18.251330496 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380031, index: 0, mask: {1, 25, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.251361888 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380032, index: 1, mask: {2, 26, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.256570196 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380053, index: 22, mask: {23, 47, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.260539589 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380052, index: 21, mask: {22, 46, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.260596630 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380036, index: 5, mask: {6, 30, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.264558258 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380037, index: 6, mask: {7, 31, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.270826317 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380051, index: 20, mask: {21, 45, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.270854113 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380050, index: 19, mask: {20, 44, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.270829952 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380038, index: 7, mask: {8, 32, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.288547349 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380035, index: 4, mask: {5, 29, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.296565016 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380043, index: 12, mask: {13, 37, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.300561757 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380044, index: 13, mask: {14, 38, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.304540757 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380045, index: 14, mask: {15, 39, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.308546741 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380046, index: 15, mask: {16, 40, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.312539443 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380047, index: 16, mask: {17, 41, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.316548886 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380048, index: 17, mask: {18, 42, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.270804258 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380033, index: 2, mask: {3, 27, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.320561095 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380034, index: 3, mask: {4, 28, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:58:18.324537926 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 380049, index: 18, mask: {19, 43, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "100%|██████████| 10/10 [00:23<00:00,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# check onnx integrity\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from tqdm import tqdm \n",
    "\n",
    "NUM_TEST = 10\n",
    "B = 16\n",
    "\n",
    "onnx_model = onnx.load(\"resnet18.onnx\")\n",
    "sess = ort.InferenceSession(\n",
    "    onnx_model.SerializeToString(), \n",
    "    providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "mean_diff = 0\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(NUM_TEST)):\n",
    "        input_dict = {\"x\" : torch.randn(*((B,) + IN_SHAPE))}\n",
    "        torch_output = model(**input_dict)\n",
    "        onnx_output = sess.run(output_names, {k : v.numpy() for k, v in input_dict.items()})\n",
    "        mean_diff += (torch_output - torch.from_numpy(onnx_output[0])).square().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.4733e-16)\n"
     ]
    }
   ],
   "source": [
    "print(mean_diff / NUM_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8601] # /opt/tensorrt/bin/trtexec --onnx=resnet18.onnx --minShapes=x:1x3x224x224 --optShapes=x:16x3x224x224 --maxShapes=x:32x3x224x224 --useCudaGraph --saveEngine=resnet18.plan --verbose=true\n",
      "[07/26/2023-05:58:42] [I] === Model Options ===\n",
      "[07/26/2023-05:58:42] [I] Format: ONNX\n",
      "[07/26/2023-05:58:42] [I] Model: resnet18.onnx\n",
      "[07/26/2023-05:58:42] [I] Output:\n",
      "[07/26/2023-05:58:42] [I] === Build Options ===\n",
      "[07/26/2023-05:58:42] [I] Max batch: explicit batch\n",
      "[07/26/2023-05:58:42] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[07/26/2023-05:58:42] [I] minTiming: 1\n",
      "[07/26/2023-05:58:42] [I] avgTiming: 8\n",
      "[07/26/2023-05:58:42] [I] Precision: FP32\n",
      "[07/26/2023-05:58:42] [I] LayerPrecisions: \n",
      "[07/26/2023-05:58:42] [I] Layer Device Types: \n",
      "[07/26/2023-05:58:42] [I] Calibration: \n",
      "[07/26/2023-05:58:42] [I] Refit: Disabled\n",
      "[07/26/2023-05:58:42] [I] Version Compatible: Disabled\n",
      "[07/26/2023-05:58:42] [I] TensorRT runtime: full\n",
      "[07/26/2023-05:58:42] [I] Lean DLL Path: \n",
      "[07/26/2023-05:58:42] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[07/26/2023-05:58:42] [I] Exclude Lean Runtime: Disabled\n",
      "[07/26/2023-05:58:42] [I] Sparsity: Disabled\n",
      "[07/26/2023-05:58:42] [I] Safe mode: Disabled\n",
      "[07/26/2023-05:58:42] [I] Build DLA standalone loadable: Disabled\n",
      "[07/26/2023-05:58:42] [I] Allow GPU fallback for DLA: Disabled\n",
      "[07/26/2023-05:58:42] [I] DirectIO mode: Disabled\n",
      "[07/26/2023-05:58:42] [I] Restricted mode: Disabled\n",
      "[07/26/2023-05:58:42] [I] Skip inference: Disabled\n",
      "[07/26/2023-05:58:42] [I] Save engine: resnet18.plan\n",
      "[07/26/2023-05:58:42] [I] Load engine: \n",
      "[07/26/2023-05:58:42] [I] Profiling verbosity: 0\n",
      "[07/26/2023-05:58:42] [I] Tactic sources: Using default tactic sources\n",
      "[07/26/2023-05:58:42] [I] timingCacheMode: local\n",
      "[07/26/2023-05:58:42] [I] timingCacheFile: \n",
      "[07/26/2023-05:58:42] [I] Heuristic: Disabled\n",
      "[07/26/2023-05:58:42] [I] Preview Features: Use default preview flags.\n",
      "[07/26/2023-05:58:42] [I] MaxAuxStreams: -1\n",
      "[07/26/2023-05:58:42] [I] BuilderOptimizationLevel: -1\n",
      "[07/26/2023-05:58:42] [I] Input(s)s format: fp32:CHW\n",
      "[07/26/2023-05:58:42] [I] Output(s)s format: fp32:CHW\n",
      "[07/26/2023-05:58:42] [I] Input build shape: x=1x3x224x224+16x3x224x224+32x3x224x224\n",
      "[07/26/2023-05:58:42] [I] Input calibration shapes: model\n",
      "[07/26/2023-05:58:42] [I] === System Options ===\n",
      "[07/26/2023-05:58:42] [I] Device: 0\n",
      "[07/26/2023-05:58:42] [I] DLACore: \n",
      "[07/26/2023-05:58:42] [I] Plugins:\n",
      "[07/26/2023-05:58:42] [I] setPluginsToSerialize:\n",
      "[07/26/2023-05:58:42] [I] dynamicPlugins:\n",
      "[07/26/2023-05:58:42] [I] ignoreParsedPluginLibs: 0\n",
      "[07/26/2023-05:58:42] [I] \n",
      "[07/26/2023-05:58:42] [I] === Inference Options ===\n",
      "[07/26/2023-05:58:42] [I] Batch: Explicit\n",
      "[07/26/2023-05:58:42] [I] Input inference shape: x=16x3x224x224\n",
      "[07/26/2023-05:58:42] [I] Iterations: 10\n",
      "[07/26/2023-05:58:42] [I] Duration: 3s (+ 200ms warm up)\n",
      "[07/26/2023-05:58:42] [I] Sleep time: 0ms\n",
      "[07/26/2023-05:58:42] [I] Idle time: 0ms\n",
      "[07/26/2023-05:58:42] [I] Inference Streams: 1\n",
      "[07/26/2023-05:58:42] [I] ExposeDMA: Disabled\n",
      "[07/26/2023-05:58:42] [I] Data transfers: Enabled\n",
      "[07/26/2023-05:58:42] [I] Spin-wait: Disabled\n",
      "[07/26/2023-05:58:42] [I] Multithreading: Disabled\n",
      "[07/26/2023-05:58:42] [I] CUDA Graph: Enabled\n",
      "[07/26/2023-05:58:42] [I] Separate profiling: Disabled\n",
      "[07/26/2023-05:58:42] [I] Time Deserialize: Disabled\n",
      "[07/26/2023-05:58:42] [I] Time Refit: Disabled\n",
      "[07/26/2023-05:58:42] [I] NVTX verbosity: 0\n",
      "[07/26/2023-05:58:42] [I] Persistent Cache Ratio: 0\n",
      "[07/26/2023-05:58:42] [I] Inputs:\n",
      "[07/26/2023-05:58:42] [I] === Reporting Options ===\n",
      "[07/26/2023-05:58:42] [I] Verbose: Enabled\n",
      "[07/26/2023-05:58:42] [I] Averages: 10 inferences\n",
      "[07/26/2023-05:58:42] [I] Percentiles: 90,95,99\n",
      "[07/26/2023-05:58:42] [I] Dump refittable layers:Disabled\n",
      "[07/26/2023-05:58:42] [I] Dump output: Disabled\n",
      "[07/26/2023-05:58:42] [I] Profile: Disabled\n",
      "[07/26/2023-05:58:42] [I] Export timing to JSON file: \n",
      "[07/26/2023-05:58:42] [I] Export output to JSON file: \n",
      "[07/26/2023-05:58:42] [I] Export profile to JSON file: \n",
      "[07/26/2023-05:58:42] [I] \n",
      "[07/26/2023-05:58:42] [I] === Device Information ===\n",
      "[07/26/2023-05:58:42] [I] Selected Device: NVIDIA GeForce RTX 3060\n",
      "[07/26/2023-05:58:42] [I] Compute Capability: 8.6\n",
      "[07/26/2023-05:58:42] [I] SMs: 28\n",
      "[07/26/2023-05:58:42] [I] Device Global Memory: 12044 MiB\n",
      "[07/26/2023-05:58:42] [I] Shared Memory per SM: 100 KiB\n",
      "[07/26/2023-05:58:42] [I] Memory Bus Width: 192 bits (ECC disabled)\n",
      "[07/26/2023-05:58:42] [I] Application Compute Clock Rate: 1.837 GHz\n",
      "[07/26/2023-05:58:42] [I] Application Memory Clock Rate: 7.501 GHz\n",
      "[07/26/2023-05:58:42] [I] \n",
      "[07/26/2023-05:58:42] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[07/26/2023-05:58:42] [I] \n",
      "[07/26/2023-05:58:42] [I] TensorRT version: 8.6.1\n",
      "[07/26/2023-05:58:42] [I] Loading standard plugins\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::BatchedNMS_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::BatchTilePlugin_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::Clip_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::CoordConvAC version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::CropAndResizeDynamic version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::CropAndResize version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::DecodeBbox3DPlugin version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::DetectionLayer_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::EfficientNMS_Explicit_TF_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::EfficientNMS_Implicit_TF_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::EfficientNMS_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::FlattenConcat_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::GenerateDetection_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::GridAnchor_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::GridAnchorRect_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 2\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::LReLU_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::ModulatedDeformConv2d version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::MultilevelCropAndResize_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::MultilevelProposeROI_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::NMSDynamic_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::NMS_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::Normalize_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::PillarScatterPlugin version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::PriorBox_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::ProposalDynamic version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::ProposalLayer_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::Proposal version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::PyramidROIAlign_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::Region_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::Reorg_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::ResizeNearest_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::ROIAlign_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::RPROI_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::ScatterND version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::SpecialSlice_TRT version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::Split version 1\n",
      "[07/26/2023-05:58:42] [V] [TRT] Registered plugin creator - ::VoxelGeneratorPlugin version 1\n",
      "[07/26/2023-05:58:43] [I] [TRT] [MemUsageChange] Init CUDA: CPU +352, GPU +0, now: CPU 367, GPU 4978 (MiB)\n",
      "[07/26/2023-05:58:43] [V] [TRT] Trying to load shared library libnvinfer_builder_resource.so.8.6.1\n",
      "[07/26/2023-05:58:43] [V] [TRT] Loaded shared library libnvinfer_builder_resource.so.8.6.1\n",
      "[07/26/2023-05:58:53] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +1217, GPU +266, now: CPU 1660, GPU 5244 (MiB)\n",
      "[07/26/2023-05:58:53] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "[07/26/2023-05:58:53] [I] Start parsing network model.\n",
      "[07/26/2023-05:58:53] [I] [TRT] ----------------------------------------------------------------\n",
      "[07/26/2023-05:58:53] [I] [TRT] Input filename:   resnet18.onnx\n",
      "[07/26/2023-05:58:53] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[07/26/2023-05:58:53] [I] [TRT] Opset version:    14\n",
      "[07/26/2023-05:58:53] [I] [TRT] Producer name:    pytorch\n",
      "[07/26/2023-05:58:53] [I] [TRT] Producer version: 2.0.0\n",
      "[07/26/2023-05:58:53] [I] [TRT] Domain:           \n",
      "[07/26/2023-05:58:53] [I] [TRT] Model version:    0\n",
      "[07/26/2023-05:58:53] [I] [TRT] Doc string:       \n",
      "[07/26/2023-05:58:53] [I] [TRT] ----------------------------------------------------------------\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::BatchedNMS_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::BatchTilePlugin_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::Clip_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::CoordConvAC version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::CropAndResizeDynamic version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::CropAndResize version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::DecodeBbox3DPlugin version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::DetectionLayer_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::EfficientNMS_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::FlattenConcat_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::GenerateDetection_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::GridAnchor_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::GridAnchorRect_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::InstanceNormalization_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::InstanceNormalization_TRT version 2\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::LReLU_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::ModulatedDeformConv2d version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::NMSDynamic_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::NMS_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::Normalize_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::PillarScatterPlugin version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::PriorBox_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::ProposalDynamic version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::ProposalLayer_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::Proposal version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::PyramidROIAlign_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::Region_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::Reorg_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::ResizeNearest_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::ROIAlign_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::RPROI_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::ScatterND version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::SpecialSlice_TRT version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::Split version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Plugin creator already registered - ::VoxelGeneratorPlugin version 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Adding network input: x with dtype: float32, dimensions: (-1, 3, 224, 224)\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: x for ONNX tensor: x\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: fc.weight\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: fc.bias\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_193\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_194\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_196\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_199\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_202\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_208\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_209\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_211\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_214\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_217\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_223\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_224\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_226\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_229\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_232\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_238\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_239\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_241\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_244\n",
      "[07/26/2023-05:58:53] [V] [TRT] Importing initializer: onnx::Conv_247\n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_0 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_239\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_0 [Identity] inputs: [onnx::Conv_239 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: onnx::Conv_239 for ONNX node: onnx::Conv_239\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_0 for ONNX node: Identity_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_251 for ONNX tensor: onnx::Conv_251\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_0 [Identity] outputs: [onnx::Conv_251 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_1 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_241\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_1 [Identity] inputs: [onnx::Conv_241 -> (512, 512, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: onnx::Conv_241 for ONNX node: onnx::Conv_241\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_1 for ONNX node: Identity_1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_250 for ONNX tensor: onnx::Conv_250\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_1 [Identity] outputs: [onnx::Conv_250 -> (512, 512, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_2 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_239\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_2 [Identity] inputs: [onnx::Conv_239 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_2 for ONNX node: Identity_2\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_248 for ONNX tensor: onnx::Conv_248\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_2 [Identity] outputs: [onnx::Conv_248 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_3 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_239\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_3 [Identity] inputs: [onnx::Conv_239 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_3 for ONNX node: Identity_3\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_245 for ONNX tensor: onnx::Conv_245\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_3 [Identity] outputs: [onnx::Conv_245 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_4 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_239\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_4 [Identity] inputs: [onnx::Conv_239 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_4 for ONNX node: Identity_4\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_242 for ONNX tensor: onnx::Conv_242\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_4 [Identity] outputs: [onnx::Conv_242 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_5 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_224\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_5 [Identity] inputs: [onnx::Conv_224 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: onnx::Conv_224 for ONNX node: onnx::Conv_224\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_5 for ONNX node: Identity_5\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_236 for ONNX tensor: onnx::Conv_236\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_5 [Identity] outputs: [onnx::Conv_236 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_6 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_226\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_6 [Identity] inputs: [onnx::Conv_226 -> (256, 256, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: onnx::Conv_226 for ONNX node: onnx::Conv_226\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_6 for ONNX node: Identity_6\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_235 for ONNX tensor: onnx::Conv_235\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_6 [Identity] outputs: [onnx::Conv_235 -> (256, 256, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_7 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_224\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_7 [Identity] inputs: [onnx::Conv_224 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_7 for ONNX node: Identity_7\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_233 for ONNX tensor: onnx::Conv_233\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_7 [Identity] outputs: [onnx::Conv_233 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_8 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_224\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_8 [Identity] inputs: [onnx::Conv_224 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_8 for ONNX node: Identity_8\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_230 for ONNX tensor: onnx::Conv_230\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_8 [Identity] outputs: [onnx::Conv_230 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_9 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_224\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_9 [Identity] inputs: [onnx::Conv_224 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_9 for ONNX node: Identity_9\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_227 for ONNX tensor: onnx::Conv_227\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_9 [Identity] outputs: [onnx::Conv_227 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_10 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_209\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_10 [Identity] inputs: [onnx::Conv_209 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: onnx::Conv_209 for ONNX node: onnx::Conv_209\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_10 for ONNX node: Identity_10\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_221 for ONNX tensor: onnx::Conv_221\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_10 [Identity] outputs: [onnx::Conv_221 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_11 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_211\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_11 [Identity] inputs: [onnx::Conv_211 -> (128, 128, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: onnx::Conv_211 for ONNX node: onnx::Conv_211\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_11 for ONNX node: Identity_11\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_220 for ONNX tensor: onnx::Conv_220\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_11 [Identity] outputs: [onnx::Conv_220 -> (128, 128, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_12 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_209\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_12 [Identity] inputs: [onnx::Conv_209 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_12 for ONNX node: Identity_12\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_218 for ONNX tensor: onnx::Conv_218\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_12 [Identity] outputs: [onnx::Conv_218 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_13 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_209\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_13 [Identity] inputs: [onnx::Conv_209 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_13 for ONNX node: Identity_13\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_215 for ONNX tensor: onnx::Conv_215\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_13 [Identity] outputs: [onnx::Conv_215 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_14 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_209\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_14 [Identity] inputs: [onnx::Conv_209 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_14 for ONNX node: Identity_14\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_212 for ONNX tensor: onnx::Conv_212\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_14 [Identity] outputs: [onnx::Conv_212 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_15 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_194\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_15 [Identity] inputs: [onnx::Conv_194 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: onnx::Conv_194 for ONNX node: onnx::Conv_194\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_15 for ONNX node: Identity_15\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_206 for ONNX tensor: onnx::Conv_206\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_15 [Identity] outputs: [onnx::Conv_206 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_16 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_199\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_16 [Identity] inputs: [onnx::Conv_199 -> (64, 64, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: onnx::Conv_199 for ONNX node: onnx::Conv_199\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_16 for ONNX node: Identity_16\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_205 for ONNX tensor: onnx::Conv_205\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_16 [Identity] outputs: [onnx::Conv_205 -> (64, 64, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_17 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_194\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_17 [Identity] inputs: [onnx::Conv_194 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_17 for ONNX node: Identity_17\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_203 for ONNX tensor: onnx::Conv_203\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_17 [Identity] outputs: [onnx::Conv_203 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_18 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_194\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_18 [Identity] inputs: [onnx::Conv_194 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_18 for ONNX node: Identity_18\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_200 for ONNX tensor: onnx::Conv_200\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_18 [Identity] outputs: [onnx::Conv_200 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: Identity_19 [Identity]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_194\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_19 [Identity] inputs: [onnx::Conv_194 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: Identity_19 for ONNX node: Identity_19\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: onnx::Conv_197 for ONNX tensor: onnx::Conv_197\n",
      "[07/26/2023-05:58:53] [V] [TRT] Identity_19 [Identity] outputs: [onnx::Conv_197 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /conv1/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: x\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_193\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_194\n",
      "[07/26/2023-05:58:53] [V] [TRT] /conv1/Conv [Conv] inputs: [x -> (-1, 3, 224, 224)[FLOAT]], [onnx::Conv_193 -> (64, 3, 7, 7)[FLOAT]], [onnx::Conv_194 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Convolution input dimensions: (-1, 3, 224, 224)\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /conv1/Conv for ONNX node: /conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Using kernel: (7, 7), strides: (2, 2), prepadding: (3, 3), postpadding: (3, 3), dilations: (1, 1), numOutputs: 64\n",
      "[07/26/2023-05:58:53] [V] [TRT] Convolution output dimensions: (-1, 64, 112, 112)\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /conv1/Conv_output_0 for ONNX tensor: /conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /conv1/Conv [Conv] outputs: [/conv1/Conv_output_0 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /act1/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /act1/Relu [Relu] inputs: [/conv1/Conv_output_0 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /act1/Relu for ONNX node: /act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /act1/Relu_output_0 for ONNX tensor: /act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /act1/Relu [Relu] outputs: [/act1/Relu_output_0 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /maxpool/MaxPool [MaxPool]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /maxpool/MaxPool [MaxPool] inputs: [/act1/Relu_output_0 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /maxpool/MaxPool for ONNX node: /maxpool/MaxPool\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /maxpool/MaxPool_output_0 for ONNX tensor: /maxpool/MaxPool_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /maxpool/MaxPool [MaxPool] outputs: [/maxpool/MaxPool_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer1/layer1.0/conv1/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /maxpool/MaxPool_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_196\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_197\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.0/conv1/Conv [Conv] inputs: [/maxpool/MaxPool_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_196 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_197 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer1/layer1.0/conv1/Conv for ONNX node: /layer1/layer1.0/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer1/layer1.0/conv1/Conv_output_0 for ONNX tensor: /layer1/layer1.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.0/conv1/Conv [Conv] outputs: [/layer1/layer1.0/conv1/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer1/layer1.0/act1/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer1/layer1.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.0/act1/Relu [Relu] inputs: [/layer1/layer1.0/conv1/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer1/layer1.0/act1/Relu for ONNX node: /layer1/layer1.0/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer1/layer1.0/act1/Relu_output_0 for ONNX tensor: /layer1/layer1.0/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.0/act1/Relu [Relu] outputs: [/layer1/layer1.0/act1/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer1/layer1.0/conv2/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer1/layer1.0/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_199\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_200\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.0/conv2/Conv [Conv] inputs: [/layer1/layer1.0/act1/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_199 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_200 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer1/layer1.0/conv2/Conv for ONNX node: /layer1/layer1.0/conv2/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer1/layer1.0/conv2/Conv_output_0 for ONNX tensor: /layer1/layer1.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.0/conv2/Conv [Conv] outputs: [/layer1/layer1.0/conv2/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer1/layer1.0/Add [Add]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer1/layer1.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /maxpool/MaxPool_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.0/Add [Add] inputs: [/layer1/layer1.0/conv2/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], [/maxpool/MaxPool_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer1/layer1.0/Add for ONNX node: /layer1/layer1.0/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer1/layer1.0/Add_output_0 for ONNX tensor: /layer1/layer1.0/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.0/Add [Add] outputs: [/layer1/layer1.0/Add_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer1/layer1.0/act2/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer1/layer1.0/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.0/act2/Relu [Relu] inputs: [/layer1/layer1.0/Add_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer1/layer1.0/act2/Relu for ONNX node: /layer1/layer1.0/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer1/layer1.0/act2/Relu_output_0 for ONNX tensor: /layer1/layer1.0/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.0/act2/Relu [Relu] outputs: [/layer1/layer1.0/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer1/layer1.1/conv1/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer1/layer1.0/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_202\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_203\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.1/conv1/Conv [Conv] inputs: [/layer1/layer1.0/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_202 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_203 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer1/layer1.1/conv1/Conv for ONNX node: /layer1/layer1.1/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer1/layer1.1/conv1/Conv_output_0 for ONNX tensor: /layer1/layer1.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.1/conv1/Conv [Conv] outputs: [/layer1/layer1.1/conv1/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer1/layer1.1/act1/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer1/layer1.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.1/act1/Relu [Relu] inputs: [/layer1/layer1.1/conv1/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer1/layer1.1/act1/Relu for ONNX node: /layer1/layer1.1/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer1/layer1.1/act1/Relu_output_0 for ONNX tensor: /layer1/layer1.1/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.1/act1/Relu [Relu] outputs: [/layer1/layer1.1/act1/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer1/layer1.1/conv2/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer1/layer1.1/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_205\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_206\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.1/conv2/Conv [Conv] inputs: [/layer1/layer1.1/act1/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_205 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_206 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer1/layer1.1/conv2/Conv for ONNX node: /layer1/layer1.1/conv2/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer1/layer1.1/conv2/Conv_output_0 for ONNX tensor: /layer1/layer1.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.1/conv2/Conv [Conv] outputs: [/layer1/layer1.1/conv2/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer1/layer1.1/Add [Add]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer1/layer1.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer1/layer1.0/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.1/Add [Add] inputs: [/layer1/layer1.1/conv2/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], [/layer1/layer1.0/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer1/layer1.1/Add for ONNX node: /layer1/layer1.1/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer1/layer1.1/Add_output_0 for ONNX tensor: /layer1/layer1.1/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.1/Add [Add] outputs: [/layer1/layer1.1/Add_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer1/layer1.1/act2/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer1/layer1.1/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.1/act2/Relu [Relu] inputs: [/layer1/layer1.1/Add_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer1/layer1.1/act2/Relu for ONNX node: /layer1/layer1.1/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer1/layer1.1/act2/Relu_output_0 for ONNX tensor: /layer1/layer1.1/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer1/layer1.1/act2/Relu [Relu] outputs: [/layer1/layer1.1/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer2/layer2.0/conv1/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer1/layer1.1/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_208\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_209\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.0/conv1/Conv [Conv] inputs: [/layer1/layer1.1/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_208 -> (128, 64, 3, 3)[FLOAT]], [onnx::Conv_209 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Convolution input dimensions: (-1, 64, 56, 56)\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer2/layer2.0/conv1/Conv for ONNX node: /layer2/layer2.0/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128\n",
      "[07/26/2023-05:58:53] [V] [TRT] Convolution output dimensions: (-1, 128, 28, 28)\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer2/layer2.0/conv1/Conv_output_0 for ONNX tensor: /layer2/layer2.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.0/conv1/Conv [Conv] outputs: [/layer2/layer2.0/conv1/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer2/layer2.0/act1/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer2/layer2.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.0/act1/Relu [Relu] inputs: [/layer2/layer2.0/conv1/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer2/layer2.0/act1/Relu for ONNX node: /layer2/layer2.0/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer2/layer2.0/act1/Relu_output_0 for ONNX tensor: /layer2/layer2.0/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.0/act1/Relu [Relu] outputs: [/layer2/layer2.0/act1/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer2/layer2.0/conv2/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer2/layer2.0/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_211\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_212\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.0/conv2/Conv [Conv] inputs: [/layer2/layer2.0/act1/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_211 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_212 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer2/layer2.0/conv2/Conv for ONNX node: /layer2/layer2.0/conv2/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer2/layer2.0/conv2/Conv_output_0 for ONNX tensor: /layer2/layer2.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.0/conv2/Conv [Conv] outputs: [/layer2/layer2.0/conv2/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer2/layer2.0/downsample/downsample.0/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer1/layer1.1/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_214\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_215\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv [Conv] inputs: [/layer1/layer1.1/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_214 -> (128, 64, 1, 1)[FLOAT]], [onnx::Conv_215 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer2/layer2.0/downsample/downsample.0/Conv for ONNX node: /layer2/layer2.0/downsample/downsample.0/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer2/layer2.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /layer2/layer2.0/downsample/downsample.0/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv [Conv] outputs: [/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer2/layer2.0/Add [Add]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer2/layer2.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer2/layer2.0/downsample/downsample.0/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.0/Add [Add] inputs: [/layer2/layer2.0/conv2/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], [/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer2/layer2.0/Add for ONNX node: /layer2/layer2.0/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer2/layer2.0/Add_output_0 for ONNX tensor: /layer2/layer2.0/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.0/Add [Add] outputs: [/layer2/layer2.0/Add_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer2/layer2.0/act2/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer2/layer2.0/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.0/act2/Relu [Relu] inputs: [/layer2/layer2.0/Add_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer2/layer2.0/act2/Relu for ONNX node: /layer2/layer2.0/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer2/layer2.0/act2/Relu_output_0 for ONNX tensor: /layer2/layer2.0/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.0/act2/Relu [Relu] outputs: [/layer2/layer2.0/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer2/layer2.1/conv1/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer2/layer2.0/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_217\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_218\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.1/conv1/Conv [Conv] inputs: [/layer2/layer2.0/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_217 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_218 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer2/layer2.1/conv1/Conv for ONNX node: /layer2/layer2.1/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer2/layer2.1/conv1/Conv_output_0 for ONNX tensor: /layer2/layer2.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.1/conv1/Conv [Conv] outputs: [/layer2/layer2.1/conv1/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer2/layer2.1/act1/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer2/layer2.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.1/act1/Relu [Relu] inputs: [/layer2/layer2.1/conv1/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer2/layer2.1/act1/Relu for ONNX node: /layer2/layer2.1/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer2/layer2.1/act1/Relu_output_0 for ONNX tensor: /layer2/layer2.1/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.1/act1/Relu [Relu] outputs: [/layer2/layer2.1/act1/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer2/layer2.1/conv2/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer2/layer2.1/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_220\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_221\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.1/conv2/Conv [Conv] inputs: [/layer2/layer2.1/act1/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_220 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_221 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer2/layer2.1/conv2/Conv for ONNX node: /layer2/layer2.1/conv2/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer2/layer2.1/conv2/Conv_output_0 for ONNX tensor: /layer2/layer2.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.1/conv2/Conv [Conv] outputs: [/layer2/layer2.1/conv2/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer2/layer2.1/Add [Add]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer2/layer2.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer2/layer2.0/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.1/Add [Add] inputs: [/layer2/layer2.1/conv2/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], [/layer2/layer2.0/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer2/layer2.1/Add for ONNX node: /layer2/layer2.1/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer2/layer2.1/Add_output_0 for ONNX tensor: /layer2/layer2.1/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.1/Add [Add] outputs: [/layer2/layer2.1/Add_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer2/layer2.1/act2/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer2/layer2.1/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.1/act2/Relu [Relu] inputs: [/layer2/layer2.1/Add_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer2/layer2.1/act2/Relu for ONNX node: /layer2/layer2.1/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer2/layer2.1/act2/Relu_output_0 for ONNX tensor: /layer2/layer2.1/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer2/layer2.1/act2/Relu [Relu] outputs: [/layer2/layer2.1/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer3/layer3.0/conv1/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer2/layer2.1/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_223\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_224\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.0/conv1/Conv [Conv] inputs: [/layer2/layer2.1/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_223 -> (256, 128, 3, 3)[FLOAT]], [onnx::Conv_224 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Convolution input dimensions: (-1, 128, 28, 28)\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer3/layer3.0/conv1/Conv for ONNX node: /layer3/layer3.0/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256\n",
      "[07/26/2023-05:58:53] [V] [TRT] Convolution output dimensions: (-1, 256, 14, 14)\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer3/layer3.0/conv1/Conv_output_0 for ONNX tensor: /layer3/layer3.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.0/conv1/Conv [Conv] outputs: [/layer3/layer3.0/conv1/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer3/layer3.0/act1/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer3/layer3.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.0/act1/Relu [Relu] inputs: [/layer3/layer3.0/conv1/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer3/layer3.0/act1/Relu for ONNX node: /layer3/layer3.0/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer3/layer3.0/act1/Relu_output_0 for ONNX tensor: /layer3/layer3.0/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.0/act1/Relu [Relu] outputs: [/layer3/layer3.0/act1/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer3/layer3.0/conv2/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer3/layer3.0/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_226\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_227\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.0/conv2/Conv [Conv] inputs: [/layer3/layer3.0/act1/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_226 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_227 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer3/layer3.0/conv2/Conv for ONNX node: /layer3/layer3.0/conv2/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer3/layer3.0/conv2/Conv_output_0 for ONNX tensor: /layer3/layer3.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.0/conv2/Conv [Conv] outputs: [/layer3/layer3.0/conv2/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer3/layer3.0/downsample/downsample.0/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer2/layer2.1/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_229\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_230\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv [Conv] inputs: [/layer2/layer2.1/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_229 -> (256, 128, 1, 1)[FLOAT]], [onnx::Conv_230 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer3/layer3.0/downsample/downsample.0/Conv for ONNX node: /layer3/layer3.0/downsample/downsample.0/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer3/layer3.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /layer3/layer3.0/downsample/downsample.0/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv [Conv] outputs: [/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer3/layer3.0/Add [Add]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer3/layer3.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer3/layer3.0/downsample/downsample.0/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.0/Add [Add] inputs: [/layer3/layer3.0/conv2/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], [/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer3/layer3.0/Add for ONNX node: /layer3/layer3.0/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer3/layer3.0/Add_output_0 for ONNX tensor: /layer3/layer3.0/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.0/Add [Add] outputs: [/layer3/layer3.0/Add_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer3/layer3.0/act2/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer3/layer3.0/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.0/act2/Relu [Relu] inputs: [/layer3/layer3.0/Add_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer3/layer3.0/act2/Relu for ONNX node: /layer3/layer3.0/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer3/layer3.0/act2/Relu_output_0 for ONNX tensor: /layer3/layer3.0/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.0/act2/Relu [Relu] outputs: [/layer3/layer3.0/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer3/layer3.1/conv1/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer3/layer3.0/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_232\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_233\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.1/conv1/Conv [Conv] inputs: [/layer3/layer3.0/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_232 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_233 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer3/layer3.1/conv1/Conv for ONNX node: /layer3/layer3.1/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer3/layer3.1/conv1/Conv_output_0 for ONNX tensor: /layer3/layer3.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.1/conv1/Conv [Conv] outputs: [/layer3/layer3.1/conv1/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer3/layer3.1/act1/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer3/layer3.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.1/act1/Relu [Relu] inputs: [/layer3/layer3.1/conv1/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer3/layer3.1/act1/Relu for ONNX node: /layer3/layer3.1/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer3/layer3.1/act1/Relu_output_0 for ONNX tensor: /layer3/layer3.1/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.1/act1/Relu [Relu] outputs: [/layer3/layer3.1/act1/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer3/layer3.1/conv2/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer3/layer3.1/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_235\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_236\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.1/conv2/Conv [Conv] inputs: [/layer3/layer3.1/act1/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_235 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_236 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer3/layer3.1/conv2/Conv for ONNX node: /layer3/layer3.1/conv2/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer3/layer3.1/conv2/Conv_output_0 for ONNX tensor: /layer3/layer3.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.1/conv2/Conv [Conv] outputs: [/layer3/layer3.1/conv2/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer3/layer3.1/Add [Add]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer3/layer3.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer3/layer3.0/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.1/Add [Add] inputs: [/layer3/layer3.1/conv2/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], [/layer3/layer3.0/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer3/layer3.1/Add for ONNX node: /layer3/layer3.1/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer3/layer3.1/Add_output_0 for ONNX tensor: /layer3/layer3.1/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.1/Add [Add] outputs: [/layer3/layer3.1/Add_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer3/layer3.1/act2/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer3/layer3.1/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.1/act2/Relu [Relu] inputs: [/layer3/layer3.1/Add_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer3/layer3.1/act2/Relu for ONNX node: /layer3/layer3.1/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer3/layer3.1/act2/Relu_output_0 for ONNX tensor: /layer3/layer3.1/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer3/layer3.1/act2/Relu [Relu] outputs: [/layer3/layer3.1/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer4/layer4.0/conv1/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer3/layer3.1/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_238\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_239\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.0/conv1/Conv [Conv] inputs: [/layer3/layer3.1/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_238 -> (512, 256, 3, 3)[FLOAT]], [onnx::Conv_239 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Convolution input dimensions: (-1, 256, 14, 14)\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer4/layer4.0/conv1/Conv for ONNX node: /layer4/layer4.0/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 512\n",
      "[07/26/2023-05:58:53] [V] [TRT] Convolution output dimensions: (-1, 512, 7, 7)\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer4/layer4.0/conv1/Conv_output_0 for ONNX tensor: /layer4/layer4.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.0/conv1/Conv [Conv] outputs: [/layer4/layer4.0/conv1/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer4/layer4.0/act1/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer4/layer4.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.0/act1/Relu [Relu] inputs: [/layer4/layer4.0/conv1/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer4/layer4.0/act1/Relu for ONNX node: /layer4/layer4.0/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer4/layer4.0/act1/Relu_output_0 for ONNX tensor: /layer4/layer4.0/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.0/act1/Relu [Relu] outputs: [/layer4/layer4.0/act1/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer4/layer4.0/conv2/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer4/layer4.0/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_241\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_242\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.0/conv2/Conv [Conv] inputs: [/layer4/layer4.0/act1/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], [onnx::Conv_241 -> (512, 512, 3, 3)[FLOAT]], [onnx::Conv_242 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer4/layer4.0/conv2/Conv for ONNX node: /layer4/layer4.0/conv2/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer4/layer4.0/conv2/Conv_output_0 for ONNX tensor: /layer4/layer4.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.0/conv2/Conv [Conv] outputs: [/layer4/layer4.0/conv2/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer4/layer4.0/downsample/downsample.0/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer3/layer3.1/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_244\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_245\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv [Conv] inputs: [/layer3/layer3.1/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_244 -> (512, 256, 1, 1)[FLOAT]], [onnx::Conv_245 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer4/layer4.0/downsample/downsample.0/Conv for ONNX node: /layer4/layer4.0/downsample/downsample.0/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer4/layer4.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /layer4/layer4.0/downsample/downsample.0/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv [Conv] outputs: [/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer4/layer4.0/Add [Add]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer4/layer4.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer4/layer4.0/downsample/downsample.0/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.0/Add [Add] inputs: [/layer4/layer4.0/conv2/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], [/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer4/layer4.0/Add for ONNX node: /layer4/layer4.0/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer4/layer4.0/Add_output_0 for ONNX tensor: /layer4/layer4.0/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.0/Add [Add] outputs: [/layer4/layer4.0/Add_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer4/layer4.0/act2/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer4/layer4.0/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.0/act2/Relu [Relu] inputs: [/layer4/layer4.0/Add_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer4/layer4.0/act2/Relu for ONNX node: /layer4/layer4.0/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer4/layer4.0/act2/Relu_output_0 for ONNX tensor: /layer4/layer4.0/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.0/act2/Relu [Relu] outputs: [/layer4/layer4.0/act2/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer4/layer4.1/conv1/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer4/layer4.0/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_247\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_248\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.1/conv1/Conv [Conv] inputs: [/layer4/layer4.0/act2/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], [onnx::Conv_247 -> (512, 512, 3, 3)[FLOAT]], [onnx::Conv_248 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer4/layer4.1/conv1/Conv for ONNX node: /layer4/layer4.1/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer4/layer4.1/conv1/Conv_output_0 for ONNX tensor: /layer4/layer4.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.1/conv1/Conv [Conv] outputs: [/layer4/layer4.1/conv1/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer4/layer4.1/act1/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer4/layer4.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.1/act1/Relu [Relu] inputs: [/layer4/layer4.1/conv1/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer4/layer4.1/act1/Relu for ONNX node: /layer4/layer4.1/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer4/layer4.1/act1/Relu_output_0 for ONNX tensor: /layer4/layer4.1/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.1/act1/Relu [Relu] outputs: [/layer4/layer4.1/act1/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer4/layer4.1/conv2/Conv [Conv]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer4/layer4.1/act1/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_250\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: onnx::Conv_251\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.1/conv2/Conv [Conv] inputs: [/layer4/layer4.1/act1/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], [onnx::Conv_250 -> (512, 512, 3, 3)[FLOAT]], [onnx::Conv_251 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer4/layer4.1/conv2/Conv for ONNX node: /layer4/layer4.1/conv2/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer4/layer4.1/conv2/Conv_output_0 for ONNX tensor: /layer4/layer4.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.1/conv2/Conv [Conv] outputs: [/layer4/layer4.1/conv2/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer4/layer4.1/Add [Add]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer4/layer4.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer4/layer4.0/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.1/Add [Add] inputs: [/layer4/layer4.1/conv2/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], [/layer4/layer4.0/act2/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer4/layer4.1/Add for ONNX node: /layer4/layer4.1/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer4/layer4.1/Add_output_0 for ONNX tensor: /layer4/layer4.1/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.1/Add [Add] outputs: [/layer4/layer4.1/Add_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /layer4/layer4.1/act2/Relu [Relu]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer4/layer4.1/Add_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.1/act2/Relu [Relu] inputs: [/layer4/layer4.1/Add_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /layer4/layer4.1/act2/Relu for ONNX node: /layer4/layer4.1/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /layer4/layer4.1/act2/Relu_output_0 for ONNX tensor: /layer4/layer4.1/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /layer4/layer4.1/act2/Relu [Relu] outputs: [/layer4/layer4.1/act2/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /global_pool/pool/GlobalAveragePool [GlobalAveragePool]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /layer4/layer4.1/act2/Relu_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /global_pool/pool/GlobalAveragePool [GlobalAveragePool] inputs: [/layer4/layer4.1/act2/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] GlobalAveragePool operators are implemented via Reduce layers rather than Pooling layers\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /global_pool/pool/GlobalAveragePool for ONNX node: /global_pool/pool/GlobalAveragePool\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /global_pool/pool/GlobalAveragePool_output_0 for ONNX tensor: /global_pool/pool/GlobalAveragePool_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /global_pool/pool/GlobalAveragePool [GlobalAveragePool] outputs: [/global_pool/pool/GlobalAveragePool_output_0 -> (-1, 512, 1, 1)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /global_pool/flatten/Flatten [Flatten]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /global_pool/pool/GlobalAveragePool_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /global_pool/flatten/Flatten [Flatten] inputs: [/global_pool/pool/GlobalAveragePool_output_0 -> (-1, 512, 1, 1)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [W] [TRT] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /global_pool/flatten/Flatten for ONNX node: /global_pool/flatten/Flatten\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: /global_pool/flatten/Flatten_output_0 for ONNX tensor: /global_pool/flatten/Flatten_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] /global_pool/flatten/Flatten [Flatten] outputs: [/global_pool/flatten/Flatten_output_0 -> (-1, 512)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Parsing node: /fc/Gemm [Gemm]\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: /global_pool/flatten/Flatten_output_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: fc.weight\n",
      "[07/26/2023-05:58:53] [V] [TRT] Searching for input: fc.bias\n",
      "[07/26/2023-05:58:53] [V] [TRT] /fc/Gemm [Gemm] inputs: [/global_pool/flatten/Flatten_output_0 -> (-1, 512)[FLOAT]], [fc.weight -> (1000, 512)[FLOAT]], [fc.bias -> (1000)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: fc.weight for ONNX node: fc.weight\n",
      "[07/26/2023-05:58:53] [V] [TRT] Using opA: 0 opB: 1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: /fc/Gemm for ONNX node: /fc/Gemm\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering layer: fc.bias for ONNX node: fc.bias\n",
      "[07/26/2023-05:58:53] [V] [TRT] Registering tensor: outputs_1 for ONNX tensor: outputs\n",
      "[07/26/2023-05:58:53] [V] [TRT] /fc/Gemm [Gemm] outputs: [outputs -> (-1, 1000)[FLOAT]], \n",
      "[07/26/2023-05:58:53] [V] [TRT] Marking outputs_1 as output: outputs\n",
      "[07/26/2023-05:58:53] [I] Finished parsing network model. Parse time: 0.107325\n",
      "[07/26/2023-05:58:53] [V] [TRT] Original: 81 layers\n",
      "[07/26/2023-05:58:53] [V] [TRT] After dead-layer removal: 81 layers\n",
      "[07/26/2023-05:58:53] [V] [TRT] Graph construction completed in 0.0020201 seconds.\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_0 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_2\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_2 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_3\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_3 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_4\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_4 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_1 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_5\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_5 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_7\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_7 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_8\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_8 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_9\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_9 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_6\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_6 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_10\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_10 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_12\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_12 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_13\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_13 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_14\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_14 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_11\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_11 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_15\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_15 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_17\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_17 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_18\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_18 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_19\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_19 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: IdentityToCastTransform on Identity_16\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_16 from IDENTITY to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConstShuffleFusion on fc.bias\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConstShuffleFusion: Fusing fc.bias with (Unnamed Layer* 84) [Shuffle]\n",
      "[07/26/2023-05:58:53] [V] [TRT] After Myelin optimization: 80 layers\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: MatMulToConvTransform on /fc/Gemm\n",
      "[07/26/2023-05:58:53] [V] [TRT] Convert layer type of /fc/Gemm from MATRIX_MULTIPLY to CONVOLUTION\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ShuffleShuffleFusion on /global_pool/flatten/Flatten\n",
      "[07/26/2023-05:58:53] [V] [TRT] ShuffleShuffleFusion: Fusing /global_pool/flatten/Flatten with reshape_before_/fc/Gemm\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ShuffleErasure on /global_pool/flatten/Flatten + reshape_before_/fc/Gemm\n",
      "[07/26/2023-05:58:53] [V] [TRT] Removing /global_pool/flatten/Flatten + reshape_before_/fc/Gemm\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReshapeBiasAddFusion on /fc/Gemm\n",
      "[07/26/2023-05:58:53] [V] [TRT] Applying ScaleNodes fusions.\n",
      "[07/26/2023-05:58:53] [V] [TRT] After scale fusion: 77 layers\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_0\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_0 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_2\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_2 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_3\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_3 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_4\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_4 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_1\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_1 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_5\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_5 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_7\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_7 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_8\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_8 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_9\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_9 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_6\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_6 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_10\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_10 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_12\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_12 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_13\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_13 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_14\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_14 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_11\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_11 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_15\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_15 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_17\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_17 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_18\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_18 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_19\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_19 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: CastToCopyTransform on Identity_16\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of Identity_16 from CAST to CAST\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConstantSplit on onnx::Conv_239\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConstantSplit on onnx::Conv_224\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConstantSplit on onnx::Conv_209\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConstantSplit on onnx::Conv_194\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /conv1/Conv with /act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.0/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.0/conv1/Conv with /layer1/layer1.0/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvEltwiseSumFusion on /layer1/layer1.0/conv2/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer1/layer1.0/conv2/Conv with /layer1/layer1.0/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add with /layer1/layer1.0/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.1/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.1/conv1/Conv with /layer1/layer1.1/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvEltwiseSumFusion on /layer1/layer1.1/conv2/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer1/layer1.1/conv2/Conv with /layer1/layer1.1/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add with /layer1/layer1.1/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.0/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.0/conv1/Conv with /layer2/layer2.0/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvEltwiseSumFusion on /layer2/layer2.0/downsample/downsample.0/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer2/layer2.0/downsample/downsample.0/Conv with /layer2/layer2.0/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add with /layer2/layer2.0/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.1/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.1/conv1/Conv with /layer2/layer2.1/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvEltwiseSumFusion on /layer2/layer2.1/conv2/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer2/layer2.1/conv2/Conv with /layer2/layer2.1/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add with /layer2/layer2.1/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.0/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.0/conv1/Conv with /layer3/layer3.0/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvEltwiseSumFusion on /layer3/layer3.0/downsample/downsample.0/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer3/layer3.0/downsample/downsample.0/Conv with /layer3/layer3.0/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add with /layer3/layer3.0/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.1/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.1/conv1/Conv with /layer3/layer3.1/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvEltwiseSumFusion on /layer3/layer3.1/conv2/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer3/layer3.1/conv2/Conv with /layer3/layer3.1/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add with /layer3/layer3.1/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.0/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.0/conv1/Conv with /layer4/layer4.0/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvEltwiseSumFusion on /layer4/layer4.0/downsample/downsample.0/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer4/layer4.0/downsample/downsample.0/Conv with /layer4/layer4.0/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add with /layer4/layer4.0/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.1/conv1/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.1/conv1/Conv with /layer4/layer4.1/act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvEltwiseSumFusion on /layer4/layer4.1/conv2/Conv\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer4/layer4.1/conv2/Conv with /layer4/layer4.1/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add\n",
      "[07/26/2023-05:58:53] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add with /layer4/layer4.1/act2/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] Running: ReduceToPoolingFusion on /global_pool/pool/GlobalAveragePool\n",
      "[07/26/2023-05:58:53] [V] [TRT] Swap the layer type of /global_pool/pool/GlobalAveragePool from REDUCE to POOLING\n",
      "[07/26/2023-05:58:53] [V] [TRT] After dupe layer removal: 64 layers\n",
      "[07/26/2023-05:58:53] [V] [TRT] After final dead-layer removal: 64 layers\n",
      "[07/26/2023-05:58:53] [V] [TRT] After tensor merging: 64 layers\n",
      "[07/26/2023-05:58:53] [V] [TRT] After vertical fusions: 64 layers\n",
      "[07/26/2023-05:58:53] [V] [TRT] After dupe layer removal: 64 layers\n",
      "[07/26/2023-05:58:53] [V] [TRT] After final dead-layer removal: 64 layers\n",
      "[07/26/2023-05:58:53] [V] [TRT] After tensor merging: 64 layers\n",
      "[07/26/2023-05:58:53] [V] [TRT] After slice removal: 64 layers\n",
      "[07/26/2023-05:58:53] [V] [TRT] After concat removal: 64 layers\n",
      "[07/26/2023-05:58:53] [V] [TRT] Trying to split Reshape and strided tensor\n",
      "[07/26/2023-05:58:53] [I] [TRT] Graph optimization time: 0.018293 seconds.\n",
      "[07/26/2023-05:58:53] [V] [TRT] Building graph using backend strategy 2\n",
      "[07/26/2023-05:58:53] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[07/26/2023-05:58:53] [V] [TRT] Constructing optimization profile number 0 [1/1].\n",
      "[07/26/2023-05:58:53] [V] [TRT] Applying generic optimizations to the graph for inference.\n",
      "[07/26/2023-05:58:53] [V] [TRT] Reserving memory for host IO tensors. Host: 0 bytes\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_239\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_239_clone_1\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_239_clone_2\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_239_clone_3\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_241\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(4608,9,3,1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_224\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_224_clone_1\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_224_clone_2\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_224_clone_3\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_226\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(2304,9,3,1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_209\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_209_clone_1\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_209_clone_2\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_209_clone_3\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_211\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1152,9,3,1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_194\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_194_clone_1\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_194_clone_2\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_194_clone_3\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for onnx::Conv_199\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination:  -> Float(576,9,3,1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for /conv1/Conv + /act1/Relu\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination: Float(150528,50176,224,1) -> Float(802816,12544,112,1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 1.07915\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.597577\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.783913\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 1.12272\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.799013\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 1.14352\n",
      "[07/26/2023-05:58:53] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.65683\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.789943\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.728795\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.597138\n",
      "[07/26/2023-05:58:53] [V] [TRT] /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0926781 seconds. Fastest Tactic: 0xf64396b97c889179 Time: 0.597138\n",
      "[07/26/2023-05:58:53] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:53] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:53] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CudnnConvolution[0x80000000])\n",
      "[07/26/2023-05:58:53] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xf64396b97c889179\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination: Float(150528,1,672,3) -> Float(802816,1,7168,64) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 1.50426\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.80384\n",
      "[07/26/2023-05:58:53] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 2.61408\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.560421\n",
      "[07/26/2023-05:58:53] [V] [TRT] Fast skip Tactic:0x0a143be7a52f301a which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 2.23744\n",
      "[07/26/2023-05:58:53] [V] [TRT] Fast skip Tactic:0xa6448a1e79f1ca6f which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 1.77254\n",
      "[07/26/2023-05:58:53] [V] [TRT] /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0460497 seconds. Fastest Tactic: 0xf231cca3335919a4 Time: 0.560421\n",
      "[07/26/2023-05:58:53] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:53] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xf231cca3335919a4\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,224,1) -> Float(802816,12544,112,1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 3.60755\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 3.20351\n",
      "[07/26/2023-05:58:53] [V] [TRT] /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.065947 seconds. Fastest Tactic: 0xe0a307ffe0ffb6a5 Time: 3.20351\n",
      "[07/26/2023-05:58:53] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:53] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xe0a307ffe0ffb6a5\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,224,1) -> Float(200704,1:4,1792,16) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 2.24241\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 2.54669\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.625371\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.707877\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.557088\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.849627\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.592165\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.673061\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.935351\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.675977\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.656238\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.535406\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.487666\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.485083\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.628882\n",
      "[07/26/2023-05:58:53] [V] [TRT] Fast skip Tactic:0x1acd4f006848c62b which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 1.02195\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.663113\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.588507\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.634587\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.674231\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.629321\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.589678\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.564078\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.598738\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.654775\n",
      "[07/26/2023-05:58:53] [V] [TRT] Fast skip Tactic:0x65e41d81f093b482 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 3.27373\n",
      "[07/26/2023-05:58:53] [V] [TRT] Fast skip Tactic:0x22cadc265a3b2e32 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 1.16736\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.727333\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.580901\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.586606\n",
      "[07/26/2023-05:58:53] [V] [TRT] /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.26825 seconds. Fastest Tactic: 0x9cb304e2edbc1221 Time: 0.485083\n",
      "[07/26/2023-05:58:53] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:53] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9cb304e2edbc1221\n",
      "[07/26/2023-05:58:53] [V] [TRT] =============== Computing costs for /maxpool/MaxPool\n",
      "[07/26/2023-05:58:53] [V] [TRT] *************** Autotuning format combination: Float(802816,12544,112,1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:58:53] [V] [TRT] --------------- Timing Runner: /maxpool/MaxPool (CudnnPooling[0x80000005])\n",
      "[07/26/2023-05:58:53] [V] [TRT] CudnnPooling has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:53] [V] [TRT] --------------- Timing Runner: /maxpool/MaxPool (CaskPooling[0x8000002f])\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads855 Tactic: 0xf86a4e1f189f4821 Time: 0.515365\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads1017 Tactic: 0x7b9e5e445528b90a Time: 0.217527\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll2_tThreads841 Tactic: 0xdcecadaa3ad74a2c Time: 0.337248\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll5_tThreads841 Tactic: 0x7bd883ae684d33e0 Time: 0.273714\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads791 Tactic: 0xa23e43dae6aa4fa6 Time: 0.251758\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads855 Tactic: 0x76d52bcd240dc832 Time: 0.240933\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_nd_NCDHW_kMAX_kGENERIC_3D_POOLING_MODE_kFLOAT_0 Tactic: 0x5faf4a0a8a5670ed Time: 0.193536\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll6_tThreads225 Tactic: 0xdb90d0acdc9fc4e1 Time: 0.441637\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads1017 Tactic: 0x4cf88ed475f74f6e Time: 0.460215\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads791 Tactic: 0x8bb5080c88a2b679 Time: 0.226743\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads513 Tactic: 0xcb3875826530ea38 Time: 0.245321\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll4_tThreads225 Tactic: 0xf21b9b7ab2973d3e Time: 0.418523\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_custom_tP4_tQ32_tRS3_tUV2 Tactic: 0x2639d3932b27ac67 Time: 0.193728\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll4_tThreads255 Tactic: 0xfcb5fcaa68fff7ac Time: 0.350062\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads855 Tactic: 0x5f5e601b4a0531ed Time: 0.23173\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll1_tThreads255 Tactic: 0x720a9978546d77bf Time: 0.36352\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads1017 Tactic: 0x7647ea605d1f4493 Time: 0.223351\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads855 Tactic: 0xd1e105c97697b1fe Time: 0.265801\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll1_tThreads225 Tactic: 0x7ca4fea88e05bd2d Time: 0.434469\n",
      "[07/26/2023-05:58:53] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll1_tThreads841 Tactic: 0x28ce1402b45cc05e Time: 0.526482\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll6_tThreads255 Tactic: 0xd53eb77c06f70e73 Time: 0.365714\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll3_tThreads225 Tactic: 0x552fb57ee00d44f2 Time: 0.415598\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll6_tThreads841 Tactic: 0x8ffa3a06e6c6b992 Time: 0.271781\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads1017 Tactic: 0x6df482284d70bfa1 Time: 0.216942\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads513 Tactic: 0x169187fc85b39995 Time: 0.271067\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll5_tThreads255 Tactic: 0x211c0ed4887c8401 Time: 0.356494\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads1017 Tactic: 0x5a9252b86daf49c5 Time: 0.284087\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll3_tThreads841 Tactic: 0x01455fd4da543981 Time: 0.291109\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Max Tactic: 0xb59f9cfb90407c92 Time: 0.195584\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads513 Tactic: 0xe2b33e540b3813e7 Time: 0.412942\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads513 Tactic: 0xb1a5a9f8d729e059 Time: 0.239177\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads791 Tactic: 0xd8a39fa054b345c7 Time: 0.347136\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll2_tThreads255 Tactic: 0x862820d0dae6fdcd Time: 0.347136\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads791 Tactic: 0x2c812608da38cfb5 Time: 0.581193\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads513 Tactic: 0x6c0c5b8637aa93f4 Time: 0.240786\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads791 Tactic: 0x7f97b1a406293c0b Time: 0.237275\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads855 Tactic: 0xab7cd9b3c48ebb9f Time: 0.233742\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads513 Tactic: 0x4587105059a26a2b Time: 0.238153\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll2_tThreads225 Tactic: 0x88864700008e375f Time: 0.41472\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads1017 Tactic: 0x574be69c6598b45c Time: 0.245906\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads791 Tactic: 0x050a6ddeb430366a Time: 0.281454\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads855 Tactic: 0x0c48f7b79614c253 Time: 0.316562\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll3_tThreads255 Tactic: 0x5b81d2ae3a658e60 Time: 0.347282\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll5_tThreads225 Tactic: 0x2fb2690452144e93 Time: 0.426862\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll4_tThreads841 Tactic: 0xa67171d088ce404d Time: 0.276773\n",
      "[07/26/2023-05:58:54] [V] [TRT] /maxpool/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.176347 seconds. Fastest Tactic: 0x5faf4a0a8a5670ed Time: 0.193536\n",
      "[07/26/2023-05:58:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x5faf4a0a8a5670ed\n",
      "[07/26/2023-05:58:54] [V] [TRT] *************** Autotuning format combination: Float(200704,1:4,1792,16) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:58:54] [V] [TRT] --------------- Timing Runner: /maxpool/MaxPool (CaskPooling[0x8000002f])\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_NOT_PROPAGATE_NAN_3D Tactic: 0xfa211b1cdd504de0 Time: 0.204663\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_PROPAGATE_NAN_3D Tactic: 0xe9d01a2a900075cb Time: 0.247954\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_NOT_PROPAGATE_NAN_2D Tactic: 0xaec8628e8180bced Time: 0.203483\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_PROPAGATE_NAN_3D Tactic: 0xd76bac5638836f8a Time: 0.220453\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_PROPAGATE_NAN_2D Tactic: 0x8382d5c464539e87 Time: 0.215333\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NHWC_Max_CAlign4 Tactic: 0x22fb1bb4a70e340d Time: 0.233033\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_NOT_PROPAGATE_NAN_3D Tactic: 0x2c7251cbae30cf74 Time: 0.232741\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_NOT_PROPAGATE_NAN_2D Tactic: 0x789b2859f2e03e79 Time: 0.193481\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_PROPAGATE_NAN_2D Tactic: 0xbd3963b8ccd084c6 Time: 0.193829\n",
      "[07/26/2023-05:58:54] [V] [TRT] /maxpool/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.0259938 seconds. Fastest Tactic: 0x789b2859f2e03e79 Time: 0.193481\n",
      "[07/26/2023-05:58:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x789b2859f2e03e79\n",
      "[07/26/2023-05:58:54] [V] [TRT] =============== Computing costs for /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu\n",
      "[07/26/2023-05:58:54] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:58:54] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.858258\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.464603\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.481426\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.848311\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.567735\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.889417\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.596978\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.422766\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.904338\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.860745\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.454363\n",
      "[07/26/2023-05:58:54] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.30563\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.896\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.742149\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.632393\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.55925\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.881079\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.788352\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.496055\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.410624\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.369856\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.549189\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.499858\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.476123\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.494153\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.713874\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.415305\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.321975\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.504686\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.7936\n",
      "[07/26/2023-05:58:54] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.23156 seconds. Fastest Tactic: 0x0190806602534cfd Time: 0.321975\n",
      "[07/26/2023-05:58:54] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0190806602534cfd\n",
      "[07/26/2023-05:58:54] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:58:54] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 1.51026\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.525605\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.906386\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.437586\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.800018\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.791849\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.468114\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.447877\n",
      "[07/26/2023-05:58:54] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.0185\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.865426\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.623616\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.803986\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.854016\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.873326\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.498811\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.659456\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.49664\n",
      "[07/26/2023-05:58:54] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.134601 seconds. Fastest Tactic: 0xf48db81f02eca9ee Time: 0.437586\n",
      "[07/26/2023-05:58:54] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xf48db81f02eca9ee\n",
      "[07/26/2023-05:58:54] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:58:54] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.657262\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.694272\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.630629\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.640293\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.613522\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.605769\n",
      "[07/26/2023-05:58:54] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0447023 seconds. Fastest Tactic: 0x3104d85fecdd547c Time: 0.605769\n",
      "[07/26/2023-05:58:54] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3104d85fecdd547c\n",
      "[07/26/2023-05:58:54] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:58:54] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.821856\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.818633\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.47685\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.520338\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.488887\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.444855\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.328411\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.870254\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.345966\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.507758\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.896731\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.625664\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.618789\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.361179\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.344357\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.343333\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.610304\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 0.716654\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.609134\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.409307\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.583387\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.485669\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.502784\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.400562\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.585289\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.33792\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.584558\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.392046\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.591931\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.622738\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 0.918094\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.521509\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.578706\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.488594\n",
      "[07/26/2023-05:58:54] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.226574 seconds. Fastest Tactic: 0x3a8712b17741b582 Time: 0.328411\n",
      "[07/26/2023-05:58:54] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3a8712b17741b582\n",
      "[07/26/2023-05:58:54] [V] [TRT] =============== Computing costs for /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu\n",
      "[07/26/2023-05:58:54] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(1), Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:58:54] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.864549\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.47221\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.487424\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.855479\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.906094\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.908873\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.888101\n",
      "[07/26/2023-05:58:54] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.32608\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.884736\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.74507\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.646729\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.936814\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.872887\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.528969\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.431259\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.577829\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.522094\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.522386\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.735378\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.513047\n",
      "[07/26/2023-05:58:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.826807\n",
      "[07/26/2023-05:58:54] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.182645 seconds. Fastest Tactic: 0x94119b4c514b211a Time: 0.431259\n",
      "[07/26/2023-05:58:54] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a\n",
      "[07/26/2023-05:58:54] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(1), Float(200704,1,3584,64) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:58:54] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 1.57637\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.53365\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.921545\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.458459\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.781897\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.780142\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.587922\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.456119\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.990939\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.845093\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.648507\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.836901\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.894098\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.901559\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.512585\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.662519\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.498688\n",
      "[07/26/2023-05:58:55] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.153863 seconds. Fastest Tactic: 0x1da91d865428f237 Time: 0.456119\n",
      "[07/26/2023-05:58:55] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:55] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1da91d865428f237\n",
      "[07/26/2023-05:58:55] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1), Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:58:55] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.649655\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.740087\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.690944\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.643218\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.665746\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.618203\n",
      "[07/26/2023-05:58:55] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.047546 seconds. Fastest Tactic: 0x3104d85fecdd547c Time: 0.618203\n",
      "[07/26/2023-05:58:55] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:55] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3104d85fecdd547c\n",
      "[07/26/2023-05:58:55] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1), Float(50176,1:4,896,16) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:58:55] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.824306\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.824466\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.482304\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.529701\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.506149\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.457143\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.347899\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.876983\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.350501\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.518729\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.90229\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.660773\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.651995\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.363227\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.358107\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.354889\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.613083\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 0.721042\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.631081\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.407991\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.594505\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.493129\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.517463\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.387365\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.615131\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.357481\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.590848\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.401115\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.618958\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.653019\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 0.92789\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.528768\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.607232\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.498807\n",
      "[07/26/2023-05:58:55] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.228317 seconds. Fastest Tactic: 0x3a8712b17741b582 Time: 0.347899\n",
      "[07/26/2023-05:58:55] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:55] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3a8712b17741b582\n",
      "[07/26/2023-05:58:55] [V] [TRT] =============== Computing costs for /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/act1/Relu\n",
      "[07/26/2023-05:58:55] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:58:55] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:58:55] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:58:55] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:58:55] [V] [TRT] =============== Computing costs for /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu\n",
      "[07/26/2023-05:58:55] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(576,9,3,1), Float(1), Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:58:55] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.864402\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.471941\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.487424\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.855502\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.914139\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.917211\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.902437\n",
      "[07/26/2023-05:58:55] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.3527\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.885029\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.722213\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.618496\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.874203\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.85109\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.525751\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.429842\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.574171\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.52224\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.528969\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.732453\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.518144\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.842167\n",
      "[07/26/2023-05:58:55] [V] [TRT] /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.181424 seconds. Fastest Tactic: 0x94119b4c514b211a Time: 0.429842\n",
      "[07/26/2023-05:58:55] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:55] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a\n",
      "[07/26/2023-05:58:55] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(576,9,3,1), Float(1), Float(200704,1,3584,64) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:58:55] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 1.57813\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.539355\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.931401\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.46475\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.789358\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.786432\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.592457\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.463287\n",
      "[07/26/2023-05:58:55] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.00147\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.853138\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.623762\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.821687\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.854162\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.859575\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.493129\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.673303\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.50816\n",
      "[07/26/2023-05:58:55] [V] [TRT] /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.143224 seconds. Fastest Tactic: 0x1da91d865428f237 Time: 0.463287\n",
      "[07/26/2023-05:58:55] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:55] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1da91d865428f237\n",
      "[07/26/2023-05:58:55] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(576,9,3,1), Float(1), Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:58:55] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.661504\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.756151\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.710217\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.653605\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.686949\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.62581\n",
      "[07/26/2023-05:58:55] [V] [TRT] /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0481811 seconds. Fastest Tactic: 0x3104d85fecdd547c Time: 0.62581\n",
      "[07/26/2023-05:58:55] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:55] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3104d85fecdd547c\n",
      "[07/26/2023-05:58:55] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(576,9,3,1), Float(1), Float(50176,1:4,896,16) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:58:55] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.834153\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.832366\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.490331\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.537454\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.50571\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.46347\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.353719\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.882222\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.356498\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.522537\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.91019\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.614875\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.608402\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.373175\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.365865\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.36352\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.61835\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 0.744887\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.645705\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.420864\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.606107\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.503223\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.516974\n",
      "[07/26/2023-05:58:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.395698\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.610158\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.362994\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.598162\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.408594\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.62581\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.660626\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 0.935643\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.535735\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.607378\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.507319\n",
      "[07/26/2023-05:58:56] [V] [TRT] /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.224091 seconds. Fastest Tactic: 0x3a8712b17741b582 Time: 0.353719\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3a8712b17741b582\n",
      "[07/26/2023-05:58:56] [V] [TRT] =============== Computing costs for /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu\n",
      "[07/26/2023-05:58:56] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.251758\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.245467\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.247662\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.250002\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.358839\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.453925\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.346405\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.457143\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.250002\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.289989\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.655081\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.249125\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.374327\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.309102\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.310688\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.443685\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.230985\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.251173\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.288768\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.255305\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.280137\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.282039\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.374345\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.266386\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.255269\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.231863\n",
      "[07/26/2023-05:58:56] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.138039 seconds. Fastest Tactic: 0x4efce38acc876f5c Time: 0.230985\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CudnnConvolution[0x80000000])\n",
      "[07/26/2023-05:58:56] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x4efce38acc876f5c\n",
      "[07/26/2023-05:58:56] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.448512\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.273262\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.468114\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.257024\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.393801\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.389705\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.274578\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.254976\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.497079\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.419392\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.322999\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.388626\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.252896\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.429618\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.253074\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.324315\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.251058\n",
      "[07/26/2023-05:58:56] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0871397 seconds. Fastest Tactic: 0x94a7db94ba744c45 Time: 0.251058\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94a7db94ba744c45\n",
      "[07/26/2023-05:58:56] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.186514\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.194999\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.186222\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.190171\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.182811\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.184905\n",
      "[07/26/2023-05:58:56] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.020217 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.182811\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:58:56] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.282153\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.281893\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.243273\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.26507\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.284379\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.230546\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.175543\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.439296\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.186075\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.294181\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.462702\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.335579\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.330167\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.191049\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.181102\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.180224\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.345527\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 0.358546\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.179739\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.258194\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.317147\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.245321\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.292041\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.254683\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.33115\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.182565\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.307493\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.207433\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.177298\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.185344\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 0.477801\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.299945\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.323438\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.252928\n",
      "[07/26/2023-05:58:56] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.131693 seconds. Fastest Tactic: 0x3a8712b17741b582 Time: 0.175543\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3a8712b17741b582\n",
      "[07/26/2023-05:58:56] [V] [TRT] =============== Computing costs for /layer2/layer2.0/conv2/Conv\n",
      "[07/26/2023-05:58:56] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.487968\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.477769\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.482304\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.479086\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.626249\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.871717\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.592311\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.450322\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.891026\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.477623\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.502345\n",
      "[07/26/2023-05:58:56] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.28323\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.47893\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.702318\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.598309\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.541545\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.855186\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.436663\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.481573\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.425545\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.36981\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.54667\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.498715\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.464165\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.539355\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.701733\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.414738\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.357815\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.488009\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.438126\n",
      "[07/26/2023-05:58:56] [V] [TRT] /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.236472 seconds. Fastest Tactic: 0x0190806602534cfd Time: 0.357815\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0190806602534cfd\n",
      "[07/26/2023-05:58:56] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.842313\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.514779\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.459191\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.760393\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.760247\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.410624\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.463872\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.984064\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.817006\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.762574\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.47733\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.884571\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.498542\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.648827\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.491374\n",
      "[07/26/2023-05:58:56] [V] [TRT] /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.141151 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.410624\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/26/2023-05:58:56] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.35211\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.37045\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.352256\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.357979\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.335287\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.340421\n",
      "[07/26/2023-05:58:56] [V] [TRT] /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0292145 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.335287\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:58:56] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.471927\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.472649\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.469701\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.502889\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.332069\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.325778\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.344795\n",
      "[07/26/2023-05:58:56] [V] [TRT] /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0406279 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.325778\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:58:56] [V] [TRT] =============== Computing costs for /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu\n",
      "[07/26/2023-05:58:56] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(1), Float(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:58:56] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.07432\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.0776114\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.0742194\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24 Time: 0.0739474\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.0806286\n",
      "[07/26/2023-05:58:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675 Time: 0.0938994\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.0790674\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.0727771\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd Time: 0.073232\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.106293\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.0756297\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.0739611\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e Time: 0.0726263\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303 Time: 0.074096\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.0803406\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338 Time: 0.0880663\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.0756503\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.0766537\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.07424\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0757371\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.0722857\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96 Time: 0.0745851\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.0746834\n",
      "[07/26/2023-05:58:57] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.068363 seconds. Fastest Tactic: 0xa31d27de74b895ff Time: 0.0722857\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa31d27de74b895ff\n",
      "[07/26/2023-05:58:57] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(1), Float(100352,1,3584,128) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.0651779\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.0787017\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.0722171\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484 Time: 0.0654872\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86 Time: 0.0716069\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.111305\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb Time: 0.0705585\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.0718994\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.0799451\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.0659627\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.06144\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d Time: 0.064512\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.0664655\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.0651947\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.0729234\n",
      "[07/26/2023-05:58:57] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0445211 seconds. Fastest Tactic: 0x7121ec1db3f80c67 Time: 0.06144\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7121ec1db3f80c67\n",
      "[07/26/2023-05:58:57] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1), Float(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.0720457\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_alignc4 Tactic: 0xc8ad2c0ce0af5623 Time: 0.0697295\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.0711543\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.0610987\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x1d144cf9675b8d6f Time: 0.0613303\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.0614888\n",
      "[07/26/2023-05:58:57] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0188392 seconds. Fastest Tactic: 0xe0a307ffe0ffb6a5 Time: 0.0610987\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xe0a307ffe0ffb6a5\n",
      "[07/26/2023-05:58:57] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1), Float(25088,1:4,896,32) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.0643657\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.064317\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.0797989\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4 Time: 0.0644145\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.0692998\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7 Time: 0.0787543\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.0575832\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.0546621\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.0792869\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3 Time: 0.0652434\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.0550034\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.0699246\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.0762057\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.0753371\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.0754971\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.0545158\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.0542232\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.054272\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.0777509\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b Time: 0.0616838\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 0.0705935\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.0623756\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.0579291\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.0656792\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.0632381\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.0682819\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.0575878\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.0757029\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.0544183\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.0676358\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.05632\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.062371\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 0.0732891\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.0691931\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.0772869\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.0606598\n",
      "[07/26/2023-05:58:57] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0947443 seconds. Fastest Tactic: 0x72a5d05b1bb165ef Time: 0.0542232\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x72a5d05b1bb165ef\n",
      "[07/26/2023-05:58:57] [V] [TRT] =============== Computing costs for /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu\n",
      "[07/26/2023-05:58:57] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.488302\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.477938\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.482199\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.479707\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.626542\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.871909\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.592311\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.450853\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.891904\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.478208\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.502199\n",
      "[07/26/2023-05:58:57] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.28512\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.478062\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.703319\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.59904\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.543259\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.857111\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.438272\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.481573\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.426423\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.371127\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.547986\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.500151\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.464325\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.540965\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.702318\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.414427\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.358693\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.490208\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.440759\n",
      "[07/26/2023-05:58:57] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.212219 seconds. Fastest Tactic: 0x0190806602534cfd Time: 0.358693\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0190806602534cfd\n",
      "[07/26/2023-05:58:57] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.843191\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.514779\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.458898\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.760393\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.761664\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.422766\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.469426\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.991122\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.830171\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.790235\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.477623\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.883273\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.500594\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.648631\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.49152\n",
      "[07/26/2023-05:58:57] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.118185 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.422766\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/26/2023-05:58:57] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.354597\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.373143\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.35488\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.351918\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.336165\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.341138\n",
      "[07/26/2023-05:58:57] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0287526 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.336165\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:58:57] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.472064\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.473047\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.469723\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.503223\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.332361\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.326363\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.344795\n",
      "[07/26/2023-05:58:57] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0569125 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.326363\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:58:57] [V] [TRT] =============== Computing costs for /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu\n",
      "[07/26/2023-05:58:57] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(1152,9,3,1), Float(1), Float(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.491666\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.482011\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.4864\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.483328\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.877861\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.893659\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.483579\n",
      "[07/26/2023-05:58:57] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.31994\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.504805\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.727918\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.625664\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.893659\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.475429\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.505125\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.397605\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.575296\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.521947\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.564078\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.718002\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.492837\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.461824\n",
      "[07/26/2023-05:58:57] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.143011 seconds. Fastest Tactic: 0x94119b4c514b211a Time: 0.397605\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a\n",
      "[07/26/2023-05:58:57] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(1152,9,3,1), Float(1), Float(100352,1,3584,128) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.895561\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.546437\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.499273\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.789358\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.787209\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.495031\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.506002\n",
      "[07/26/2023-05:58:57] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.02422\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.855918\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.798135\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.497641\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.886784\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.515657\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.679058\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.520631\n",
      "[07/26/2023-05:58:57] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.111833 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.495031\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/26/2023-05:58:57] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1152,9,3,1), Float(1), Float(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.392777\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.426569\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.406967\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.401115\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.386194\n",
      "[07/26/2023-05:58:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.377856\n",
      "[07/26/2023-05:58:57] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0302986 seconds. Fastest Tactic: 0x3104d85fecdd547c Time: 0.377856\n",
      "[07/26/2023-05:58:57] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3104d85fecdd547c\n",
      "[07/26/2023-05:58:57] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1152,9,3,1), Float(1), Float(25088,1:4,896,32) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.512731\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.506002\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.501614\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.536137\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.372443\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.365568\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.385769\n",
      "[07/26/2023-05:58:58] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0390782 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.365568\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:58:58] [V] [TRT] =============== Computing costs for /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu\n",
      "[07/26/2023-05:58:58] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.247515\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.289499\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.285696\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.245321\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.334702\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.513637\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.368795\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.453097\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.242542\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.268878\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.691346\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.242542\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.376955\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.305445\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.339675\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.470601\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.220485\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.24811\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.344942\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.294766\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.290249\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.274871\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.37888\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.264777\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.251758\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.221193\n",
      "[07/26/2023-05:58:58] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.146003 seconds. Fastest Tactic: 0x4efce38acc876f5c Time: 0.220485\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CudnnConvolution[0x80000000])\n",
      "[07/26/2023-05:58:58] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x4efce38acc876f5c\n",
      "[07/26/2023-05:58:58] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.237861\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.262875\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.233765\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.415013\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.383269\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.205531\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.230546\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.522094\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.440613\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.382537\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.230254\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.425093\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.242542\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.356987\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.236398\n",
      "[07/26/2023-05:58:58] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0904927 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.205531\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/26/2023-05:58:58] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.177042\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.168229\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.164864\n",
      "[07/26/2023-05:58:58] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0103715 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.164864\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:58:58] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.251173\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.251282\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.269751\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.288475\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.167497\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.165157\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.174519\n",
      "[07/26/2023-05:58:58] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0273413 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.165157\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:58:58] [V] [TRT] =============== Computing costs for /layer3/layer3.0/conv2/Conv\n",
      "[07/26/2023-05:58:58] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.492398\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.579877\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.559104\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.476745\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.61952\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.991525\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.692169\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.467278\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.890149\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.470638\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.497225\n",
      "[07/26/2023-05:58:58] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.37011\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.470702\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.701586\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.596407\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.647168\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.910139\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.428032\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.481573\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.443392\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.367909\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.654089\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.591287\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.544622\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.533463\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.701879\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.493861\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.355621\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.4864\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.429605\n",
      "[07/26/2023-05:58:58] [V] [TRT] /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.278061 seconds. Fastest Tactic: 0x0190806602534cfd Time: 0.355621\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0190806602534cfd\n",
      "[07/26/2023-05:58:58] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.434469\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.51083\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.44661\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.803401\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.752933\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.393801\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.442222\n",
      "[07/26/2023-05:58:58] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.02912\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.858405\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.752347\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.44427\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.837778\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.475429\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.698565\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.466944\n",
      "[07/26/2023-05:58:58] [V] [TRT] /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.134542 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.393801\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/26/2023-05:58:58] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.339822\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.322121\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.315392\n",
      "[07/26/2023-05:58:58] [V] [TRT] /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0164094 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.315392\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:58:58] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.465627\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.46475\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.52853\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.56149\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.321682\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.31611\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.335579\n",
      "[07/26/2023-05:58:58] [V] [TRT] /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.046095 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.31611\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:58:58] [V] [TRT] =============== Computing costs for /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu\n",
      "[07/26/2023-05:58:58] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(1), Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.058173\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.0675733\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.0581379\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24 Time: 0.0581745\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.0731429\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675 Time: 0.0467017\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.0665676\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.0463943\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd Time: 0.050307\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.0965326\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.0589775\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.0541257\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e Time: 0.0462263\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303 Time: 0.0482743\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.0759954\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338 Time: 0.0582476\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.0626453\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.0672914\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.0492449\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0632442\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.0492495\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96 Time: 0.0547109\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.0484587\n",
      "[07/26/2023-05:58:58] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0666103 seconds. Fastest Tactic: 0xc0b05b61d128e46e Time: 0.0462263\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc0b05b61d128e46e\n",
      "[07/26/2023-05:58:58] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(1), Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.046632\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.0596358\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484 Time: 0.0445074\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86 Time: 0.0483749\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.0567756\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb Time: 0.0486644\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.0485669\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.0720457\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.0528031\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d Time: 0.042824\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.0581044\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.0441429\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.054467\n",
      "[07/26/2023-05:58:58] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0333235 seconds. Fastest Tactic: 0x55d80c17b1cd982d Time: 0.042824\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x55d80c17b1cd982d\n",
      "[07/26/2023-05:58:58] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1), Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.0488107\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_alignc4 Tactic: 0xc8ad2c0ce0af5623 Time: 0.0458091\n",
      "[07/26/2023-05:58:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.0462263\n",
      "[07/26/2023-05:58:58] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00912111 seconds. Fastest Tactic: 0xc8ad2c0ce0af5623 Time: 0.0458091\n",
      "[07/26/2023-05:58:58] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc8ad2c0ce0af5623\n",
      "[07/26/2023-05:58:58] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1), Float(12544,1:4,896,64) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.0433486\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.0434834\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.055392\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4 Time: 0.0430811\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7 Time: 0.0542202\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3 Time: 0.0447634\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b Time: 0.0416389\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.0420206\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.0421303\n",
      "[07/26/2023-05:58:59] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0250696 seconds. Fastest Tactic: 0x130df49cb195156b Time: 0.0416389\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:59] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x130df49cb195156b\n",
      "[07/26/2023-05:58:59] [V] [TRT] =============== Computing costs for /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu\n",
      "[07/26/2023-05:58:59] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.492544\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.579931\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.55925\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.476453\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.619666\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.991081\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.692078\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.466505\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.89013\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.470601\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.497664\n",
      "[07/26/2023-05:58:59] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.36499\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.470309\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.701586\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.596553\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.647168\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.911214\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.427886\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.481573\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.443703\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.367762\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.654336\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.590866\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.543419\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.533353\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.701586\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.493861\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.355035\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.48656\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.429202\n",
      "[07/26/2023-05:58:59] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.295075 seconds. Fastest Tactic: 0x0190806602534cfd Time: 0.355035\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:59] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0190806602534cfd\n",
      "[07/26/2023-05:58:59] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.433865\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.51083\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.446171\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.80352\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.752494\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.393801\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.442222\n",
      "[07/26/2023-05:58:59] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.02947\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.857527\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.752494\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.43915\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.858697\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.475488\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.693541\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.462409\n",
      "[07/26/2023-05:58:59] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.137254 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.393801\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:59] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/26/2023-05:58:59] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.339383\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.32139\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.314807\n",
      "[07/26/2023-05:58:59] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0245529 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.314807\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:59] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:58:59] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.465627\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.464603\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.52853\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.561591\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.321536\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.315685\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.335287\n",
      "[07/26/2023-05:58:59] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0464689 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.315685\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:59] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:58:59] [V] [TRT] =============== Computing costs for /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu\n",
      "[07/26/2023-05:58:59] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(2304,9,3,1), Float(1), Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.494446\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.582071\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.561006\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.479232\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.993573\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.89088\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.473819\n",
      "[07/26/2023-05:58:59] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.37011\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.473568\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.702757\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.610304\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.932873\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.447342\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.509806\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.409413\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.692955\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.619081\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.562373\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.722798\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.494446\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.446903\n",
      "[07/26/2023-05:58:59] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.145714 seconds. Fastest Tactic: 0x94119b4c514b211a Time: 0.409413\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:59] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a\n",
      "[07/26/2023-05:58:59] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(2304,9,3,1), Float(1), Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.538624\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.620544\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.559488\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.908581\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.855479\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.506002\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.550181\n",
      "[07/26/2023-05:58:59] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.12538\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.957321\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.855771\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.553253\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.946615\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.586313\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.812795\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.578162\n",
      "[07/26/2023-05:58:59] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.117019 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.506002\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:59] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/26/2023-05:58:59] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(2304,9,3,1), Float(1), Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.46475\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.447195\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.437102\n",
      "[07/26/2023-05:58:59] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0167339 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.437102\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:59] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:58:59] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(2304,9,3,1), Float(1), Float(12544,1:4,896,64) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.573294\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.572663\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.633125\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.665893\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.429541\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.42379\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.442807\n",
      "[07/26/2023-05:58:59] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0452418 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.42379\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:58:59] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:58:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:58:59] [V] [TRT] =============== Computing costs for /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu\n",
      "[07/26/2023-05:58:59] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:58:59] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.254418\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.281161\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.283648\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.245175\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.312027\n",
      "[07/26/2023-05:58:59] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.749678\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.344357\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.518144\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.239323\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.254245\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.742546\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.243273\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.4352\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.351963\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.308517\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.503077\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.427301\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.288329\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.355621\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.299447\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.280137\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.272677\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.43637\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.239323\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.291109\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.427739\n",
      "[07/26/2023-05:59:00] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.263514 seconds. Fastest Tactic: 0x5aa723e0481da855 Time: 0.239323\n",
      "[07/26/2023-05:59:00] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:00] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:00] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CudnnConvolution[0x80000000])\n",
      "[07/26/2023-05:59:00] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5aa723e0481da855\n",
      "[07/26/2023-05:59:00] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:00] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.241079\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.299593\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.228791\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.44149\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.439054\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.392777\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.225134\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.553545\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.468846\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.438565\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.225719\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.4864\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.27659\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.360119\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.272238\n",
      "[07/26/2023-05:59:00] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.130355 seconds. Fastest Tactic: 0x1da91d865428f237 Time: 0.225134\n",
      "[07/26/2023-05:59:00] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:00] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1da91d865428f237\n",
      "[07/26/2023-05:59:00] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:00] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.170862\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.162085\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.158866\n",
      "[07/26/2023-05:59:00] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0159307 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.158866\n",
      "[07/26/2023-05:59:00] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:00] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:59:00] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:00] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.239909\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.239031\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.269751\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.285989\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.162523\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.159607\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.16896\n",
      "[07/26/2023-05:59:00] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0396387 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.159607\n",
      "[07/26/2023-05:59:00] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:00] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:59:00] [V] [TRT] =============== Computing costs for /layer4/layer4.0/conv2/Conv\n",
      "[07/26/2023-05:59:00] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1), Float(1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:00] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.555447\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.613083\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.555447\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.481134\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.604014\n",
      "[07/26/2023-05:59:00] [V] [TRT] Fast skip Tactic:0xa9366041633a5135 which exceed time limit during pre-run\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 1.25136\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.647314\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.893806\n",
      "[07/26/2023-05:59:00] [V] [TRT] Fast skip Tactic:0xcb8a43f748d8a338 which exceed time limit during pre-run\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 1.0281\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.468837\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.487717\n",
      "[07/26/2023-05:59:00] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.47251\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.473234\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.813637\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.691214\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.603525\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.973093\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.842459\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.562322\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.885783\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.712704\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.665746\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.653166\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.503223\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.530578\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.811008\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.452023\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.692809\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.565838\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.844503\n",
      "[07/26/2023-05:59:00] [V] [TRT] /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.562753 seconds. Fastest Tactic: 0x3712e3e595645874 Time: 0.452023\n",
      "[07/26/2023-05:59:00] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:00] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3712e3e595645874\n",
      "[07/26/2023-05:59:00] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512), Float(1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:00] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:00] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.468699\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.586313\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.441929\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.857234\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.860453\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.770926\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.437687\n",
      "[07/26/2023-05:59:01] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.08134\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.903017\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.874162\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.437833\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.967241\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.542135\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.697637\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.534089\n",
      "[07/26/2023-05:59:01] [V] [TRT] /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.201659 seconds. Fastest Tactic: 0x1da91d865428f237 Time: 0.437687\n",
      "[07/26/2023-05:59:01] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:01] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1da91d865428f237\n",
      "[07/26/2023-05:59:01] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:01] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.33397\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.31627\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.309687\n",
      "[07/26/2023-05:59:01] [V] [TRT] /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.040694 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.309687\n",
      "[07/26/2023-05:59:01] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:01] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:59:01] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:01] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.476571\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.467095\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.534711\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.561591\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.317147\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.311442\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.330752\n",
      "[07/26/2023-05:59:01] [V] [TRT] /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0826204 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.311442\n",
      "[07/26/2023-05:59:01] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:01] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:59:01] [V] [TRT] =============== Computing costs for /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu\n",
      "[07/26/2023-05:59:01] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(1), Float(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:01] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.046504\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.0439291\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.047264\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24 Time: 0.0467166\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.0936229\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675 Time: 0.0417406\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.0686095\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.0377326\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd Time: 0.04556\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.101888\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.0609524\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.0497371\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e Time: 0.0379623\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303 Time: 0.0405211\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.0908709\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338 Time: 0.040448\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.0583192\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.0448731\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.042016\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0411417\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.0446549\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96 Time: 0.0564587\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.0639269\n",
      "[07/26/2023-05:59:01] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0805869 seconds. Fastest Tactic: 0x5aa723e0481da855 Time: 0.0377326\n",
      "[07/26/2023-05:59:01] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:01] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5aa723e0481da855\n",
      "[07/26/2023-05:59:01] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(1), Float(25088,1,3584,512) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:01] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.0416869\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.0577829\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484 Time: 0.0422366\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86 Time: 0.0382766\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.0577341\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb Time: 0.0371931\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.0379337\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.0712411\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.0547596\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d Time: 0.0388286\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.0610499\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.0393109\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.0495939\n",
      "[07/26/2023-05:59:01] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0372435 seconds. Fastest Tactic: 0x1022069e6f8d9aeb Time: 0.0371931\n",
      "[07/26/2023-05:59:01] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:01] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1022069e6f8d9aeb\n",
      "[07/26/2023-05:59:01] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1), Float(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:01] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.0343899\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_alignc4 Tactic: 0xc8ad2c0ce0af5623 Time: 0.0324379\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.0335497\n",
      "[07/26/2023-05:59:01] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0104119 seconds. Fastest Tactic: 0xc8ad2c0ce0af5623 Time: 0.0324379\n",
      "[07/26/2023-05:59:01] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:01] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc8ad2c0ce0af5623\n",
      "[07/26/2023-05:59:01] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1), Float(6272,1:4,896,128) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:01] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.0398994\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.0403749\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.0445189\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4 Time: 0.0396434\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7 Time: 0.0437074\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3 Time: 0.0372389\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b Time: 0.0297134\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.0302482\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.0304814\n",
      "[07/26/2023-05:59:01] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0257231 seconds. Fastest Tactic: 0x130df49cb195156b Time: 0.0297134\n",
      "[07/26/2023-05:59:01] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:01] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x130df49cb195156b\n",
      "[07/26/2023-05:59:01] [V] [TRT] =============== Computing costs for /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu\n",
      "[07/26/2023-05:59:01] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1), Float(1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:01] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.555301\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.613559\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.555447\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.481106\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.603136\n",
      "[07/26/2023-05:59:01] [V] [TRT] Fast skip Tactic:0xa9366041633a5135 which exceed time limit during pre-run\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 1.25133\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.646258\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.893952\n",
      "[07/26/2023-05:59:01] [V] [TRT] Fast skip Tactic:0xcb8a43f748d8a338 which exceed time limit during pre-run\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 1.02502\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.469285\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.487424\n",
      "[07/26/2023-05:59:01] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.47558\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.473641\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.813349\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.690907\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.604745\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.974848\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.842295\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.563493\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.881518\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.713582\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.665454\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.653605\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.503223\n",
      "[07/26/2023-05:59:01] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.53131\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.811886\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.452201\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.692626\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.566272\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.844654\n",
      "[07/26/2023-05:59:02] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.554838 seconds. Fastest Tactic: 0x3712e3e595645874 Time: 0.452201\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:02] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3712e3e595645874\n",
      "[07/26/2023-05:59:02] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512), Float(1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.469138\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.586478\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.441783\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.857673\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.859721\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.770926\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.437394\n",
      "[07/26/2023-05:59:02] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.08378\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.902729\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.87285\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.437394\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.965193\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.542021\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.697637\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.534089\n",
      "[07/26/2023-05:59:02] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.189527 seconds. Fastest Tactic: 0x1da91d865428f237 Time: 0.437394\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:02] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1da91d865428f237\n",
      "[07/26/2023-05:59:02] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.333824\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.316393\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.309746\n",
      "[07/26/2023-05:59:02] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0280443 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.309746\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:02] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:59:02] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.476306\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.466944\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.534821\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.561737\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.317312\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.311589\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.331045\n",
      "[07/26/2023-05:59:02] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0740337 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.311589\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:02] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:59:02] [V] [TRT] =============== Computing costs for /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu\n",
      "[07/26/2023-05:59:02] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1), Float(4608,9,3,1), Float(1), Float(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.556727\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.613522\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.558226\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.483529\n",
      "[07/26/2023-05:59:02] [V] [TRT] Fast skip Tactic:0xa9366041633a5135 which exceed time limit during pre-run\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 1.46166\n",
      "[07/26/2023-05:59:02] [V] [TRT] Fast skip Tactic:0xcb8a43f748d8a338 which exceed time limit during pre-run\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 1.02605\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.470747\n",
      "[07/26/2023-05:59:02] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.47149\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.474697\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.814976\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.70773\n",
      "[07/26/2023-05:59:02] [V] [TRT] Fast skip Tactic:0x9d9fdb5fd9945f64 which exceed time limit during pre-run\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 1.00557\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.872594\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.571831\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.811154\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.677449\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.660745\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.537893\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.814519\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.566953\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.854309\n",
      "[07/26/2023-05:59:02] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.142946 seconds. Fastest Tactic: 0x5aa723e0481da855 Time: 0.470747\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:02] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5aa723e0481da855\n",
      "[07/26/2023-05:59:02] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512), Float(4608,9,3,1), Float(1), Float(25088,1,3584,512) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.901705\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.990354\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.844215\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 1.25776\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 1.23407\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 1.17146\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.837339\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.47471\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 1.29595\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 1.24826\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.836462\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 1.33398\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.943689\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 1.09903\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.937691\n",
      "[07/26/2023-05:59:02] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.200828 seconds. Fastest Tactic: 0xd15dd11d64344e83 Time: 0.836462\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:02] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd15dd11d64344e83\n",
      "[07/26/2023-05:59:02] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(4608,9,3,1), Float(1), Float(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.737865\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.722798\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.712997\n",
      "[07/26/2023-05:59:02] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0244313 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.712997\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:02] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:59:02] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(4608,9,3,1), Float(1), Float(6272,1:4,896,128) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.878446\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.868846\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.933888\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.962414\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.717582\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.709925\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.731483\n",
      "[07/26/2023-05:59:02] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0658011 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.709925\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:02] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:59:02] [V] [TRT] =============== Computing costs for /global_pool/pool/GlobalAveragePool\n",
      "[07/26/2023-05:59:02] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1) -> Float(512,1,1,1) ***************\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /global_pool/pool/GlobalAveragePool (CudnnPooling[0x80000005])\n",
      "[07/26/2023-05:59:02] [V] [TRT] CudnnPooling has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /global_pool/pool/GlobalAveragePool (CaskPooling[0x8000002f])\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll6_tThreads49 Tactic: 0xa4a96ea1892462c7 Time: 0.0189126\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll7_tThreads49 Tactic: 0x489ba15aaac78fba Time: 0.0193097\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll2_tThreads49 Tactic: 0x31d30f1a58b7ea39 Time: 0.0190286\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll3_tThreads49 Tactic: 0xdde1c0e17b540744 Time: 0.0188674\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll4_tThreads49 Tactic: 0xee145e7c61eda6b8 Time: 0.0191223\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll1_tThreads49 Tactic: 0x975cf03c939dc33b Time: 0.0192766\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm50_xmma_pooling_nd_NCDHW_kAVERAGE_kGENERIC_3D_POOLING_MODE_kFLOAT_0 Tactic: 0xba33c80addb15739 Time: 0.00618191\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll5_tThreads49 Tactic: 0x02269187420e4bc5 Time: 0.0190069\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll8_tThreads49 Tactic: 0xc342539bbc57213f Time: 0.0191366\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Average_FastDiv Tactic: 0x933eceba7b866d59 Time: 0.0050519\n",
      "[07/26/2023-05:59:02] [V] [TRT] /global_pool/pool/GlobalAveragePool (CaskPooling[0x8000002f]) profiling completed in 0.0216847 seconds. Fastest Tactic: 0x933eceba7b866d59 Time: 0.0050519\n",
      "[07/26/2023-05:59:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x933eceba7b866d59\n",
      "[07/26/2023-05:59:02] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128) -> Float(128,1:4,128,128) ***************\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /global_pool/pool/GlobalAveragePool (CaskPooling[0x8000002f])\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NHWC_Average_FastDiv_CAlign4 Tactic: 0xfab3e2ee1c085a9a Time: 0.00504607\n",
      "[07/26/2023-05:59:02] [V] [TRT] /global_pool/pool/GlobalAveragePool (CaskPooling[0x8000002f]) profiling completed in 0.0021392 seconds. Fastest Tactic: 0xfab3e2ee1c085a9a Time: 0.00504607\n",
      "[07/26/2023-05:59:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xfab3e2ee1c085a9a\n",
      "[07/26/2023-05:59:02] [V] [TRT] =============== Computing costs for /fc/Gemm\n",
      "[07/26/2023-05:59:02] [V] [TRT] *************** Autotuning format combination: Float(512,1,1,1) -> Float(1000,1,1,1) ***************\n",
      "[07/26/2023-05:59:02] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.059392\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.0474869\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.0599284\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24 Time: 0.0590507\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.0447234\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.0833691\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.0483368\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675 Time: 0.0614888\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.0279002\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.0399737\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd Time: 0.025405\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.084992\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1 Time: 0.114101\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.0230341\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.0302235\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.0457509\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505 Time: 0.0406766\n",
      "[07/26/2023-05:59:02] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7 Time: 0.0611962\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a Time: 0.0264053\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e Time: 0.03968\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303 Time: 0.0480198\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b Time: 0.0508099\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.075776\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x16_stage2_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe52b0ddb126aa135 Time: 0.0333486\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4 Time: 0.0613349\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338 Time: 0.0512488\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd Time: 0.0500267\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.0559299\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77 Time: 0.0237055\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.0494446\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x32x16_stage2_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xb2c8ebee321e63d6 Time: 0.0409234\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.047576\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0428606\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.0258514\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96 Time: 0.053248\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.119223\n",
      "[07/26/2023-05:59:03] [V] [TRT] /fc/Gemm (CaskConvolution[0x80000009]) profiling completed in 0.186633 seconds. Fastest Tactic: 0xb0bf940d5e0f9f45 Time: 0.0230341\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CublasConvolution[0x80000029])\n",
      "[07/26/2023-05:59:03] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskGemmConvolution[0x8000002e])\n",
      "[07/26/2023-05:59:03] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:03] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CudnnConvolution[0x80000000])\n",
      "[07/26/2023-05:59:03] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xb0bf940d5e0f9f45\n",
      "[07/26/2023-05:59:03] [V] [TRT] *************** Autotuning format combination: Float(512,1,512,512) -> Float(1000,1,1000,1000) ***************\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.0222472\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.0408937\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484 Time: 0.0576671\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86 Time: 0.0311835\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537 Time: 0.0209319\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0 Time: 0.0315685\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808 Time: 0.0309614\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.0544183\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb Time: 0.0317733\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.0324754\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.0495421\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939 Time: 0.0544183\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22 Time: 0.0358777\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a Time: 0.0166166\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.0162083\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d Time: 0.0207216\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.0192743\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.021169\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.0434103\n",
      "[07/26/2023-05:59:03] [V] [TRT] /fc/Gemm (CaskConvolution[0x80000009]) profiling completed in 0.0703038 seconds. Fastest Tactic: 0x7121ec1db3f80c67 Time: 0.0162083\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:03] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CublasConvolution[0x80000029])\n",
      "[07/26/2023-05:59:03] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7121ec1db3f80c67\n",
      "[07/26/2023-05:59:03] [V] [TRT] *************** Autotuning format combination: Float(128,1:4,128,128) -> Float(1000,1,1,1) ***************\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.041936\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_alignc4 Tactic: 0x440241d9c93d605d Time: 0.0392411\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_alignc4 Tactic: 0xc8ad2c0ce0af5623 Time: 0.0392777\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.0401143\n",
      "[07/26/2023-05:59:03] [V] [TRT] /fc/Gemm (CaskConvolution[0x80000009]) profiling completed in 0.0148388 seconds. Fastest Tactic: 0x440241d9c93d605d Time: 0.0392411\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:03] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x440241d9c93d605d\n",
      "[07/26/2023-05:59:03] [V] [TRT] *************** Autotuning format combination: Float(128,1:4,128,128) -> Float(250,1:4,250,250) ***************\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.0557349\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.0563718\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.0368274\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4 Time: 0.0556343\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7 Time: 0.036352\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3 Time: 0.0315977\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b Time: 0.0383977\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462 Time: 0.0382583\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.0391394\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.0404571\n",
      "[07/26/2023-05:59:03] [V] [TRT] /fc/Gemm (CaskConvolution[0x80000009]) profiling completed in 0.0359018 seconds. Fastest Tactic: 0xae0c89d047932ba3 Time: 0.0315977\n",
      "[07/26/2023-05:59:03] [V] [TRT] /fc/Gemm: 56 available tactics, 0 unparsable, 28 pruned, 28 remaining after tactic pruning.\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskGemmConvolution[0x8000002e])\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 1 numBuffers: 0 numKernels: 1 Tactic: 0x00000000000203be Time: 0.00869916\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 1 numBuffers: 0 numKernels: 1 Tactic: 0x00000000000202f3 Time: 0.00905943\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 1 numKernels: 1 Tactic: 0x00000000020403be Time: 0.0109642\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 1 numKernels: 1 Tactic: 0x00000000020402f3 Time: 0.0114124\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 1 numBuffers: 0 numKernels: 1 Tactic: 0x000000000002031a Time: 0.0142296\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 1 numBuffers: 0 numKernels: 1 Tactic: 0x00000000000202b8 Time: 0.0139188\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 2 numKernels: 2 Tactic: 0x00000002040403be Time: 0.0120678\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 2 numKernels: 2 Tactic: 0x00000002040402f3 Time: 0.0125154\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 2 numKernels: 1 Tactic: 0x00000000040602f3 Time: 0.0122122\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 2 numKernels: 1 Tactic: 0x00000000040603be Time: 0.011622\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 3 numKernels: 2 Tactic: 0x00000002060603be Time: 0.0136623\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 3 numKernels: 2 Tactic: 0x00000002060602f3 Time: 0.0138174\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 1 numKernels: 1 Tactic: 0x00000000020402b8 Time: 0.01696\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 1 numKernels: 1 Tactic: 0x000000000204031a Time: 0.0173862\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 3 numKernels: 1 Tactic: 0x00000000060802f3 Time: 0.0134766\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 3 numKernels: 1 Tactic: 0x00000000060803be Time: 0.0130215\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 2 numKernels: 1 Tactic: 0x00000000040602b8 Time: 0.0205107\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 2 numKernels: 1 Tactic: 0x000000000406031a Time: 0.0210612\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 2 numKernels: 2 Tactic: 0x000000020404031a Time: 0.0186623\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 2 numKernels: 2 Tactic: 0x00000002040402b8 Time: 0.0182491\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 4 numKernels: 2 Tactic: 0x00000002080803be Time: 0.0144082\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 4 numKernels: 2 Tactic: 0x00000002080802f3 Time: 0.0152\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 3 numKernels: 1 Tactic: 0x00000000060802b8 Time: 0.0226984\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 3 numKernels: 1 Tactic: 0x000000000608031a Time: 0.0234136\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 3 numKernels: 2 Tactic: 0x00000002060602b8 Time: 0.0207687\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 3 numKernels: 2 Tactic: 0x000000020606031a Time: 0.0213544\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 4 numKernels: 2 Tactic: 0x000000020808031a Time: 0.0231425\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 4 numKernels: 2 Tactic: 0x00000002080802b8 Time: 0.0224307\n",
      "[07/26/2023-05:59:03] [V] [TRT] /fc/Gemm (CaskGemmConvolution[0x8000002e]) profiling completed in 0.0805273 seconds. Fastest Tactic: 0x00000000000203be Time: 0.00869916\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:59:03] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CublasConvolution[0x80000029])\n",
      "[07/26/2023-05:59:03] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:59:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskGemmConvolution Tactic: 0x00000000000203be\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing costs for reshape_after_/fc/Gemm\n",
      "[07/26/2023-05:59:03] [V] [TRT] *************** Autotuning format combination: Float(1000,1,1,1) -> Float(1000,1) ***************\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: reshape_after_/fc/Gemm (Shuffle[0x8000000d])\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00818083\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0228173\n",
      "[07/26/2023-05:59:03] [V] [TRT] reshape_after_/fc/Gemm (Shuffle[0x8000000d]) profiling completed in 0.00350267 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00818083\n",
      "[07/26/2023-05:59:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
      "[07/26/2023-05:59:03] [V] [TRT] *************** Autotuning format combination: Float(250,1:4,250,250) -> Float(1000,1) ***************\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: reshape_after_/fc/Gemm (Shuffle[0x8000000d])\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00784505\n",
      "[07/26/2023-05:59:03] [V] [TRT] reshape_after_/fc/Gemm (Shuffle[0x8000000d]) profiling completed in 0.00172974 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00784505\n",
      "[07/26/2023-05:59:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:03] [V] [TRT] *************** Autotuning Reformat: Float(150528,50176,224,1) -> Float(150528,1,672,3) ***************\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(x -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0636526\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0703025\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.157111\n",
      "[07/26/2023-05:59:03] [V] [TRT] Optimizer Reformat(x -> <out>) (Reformat[0x80000006]) profiling completed in 0.00635022 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0636526\n",
      "[07/26/2023-05:59:03] [V] [TRT] *************** Autotuning Reformat: Float(150528,50176,224,1) -> Float(50176,1:4,224,1) ***************\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(x -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.117248\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0711436\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.158427\n",
      "[07/26/2023-05:59:03] [V] [TRT] Optimizer Reformat(x -> <out>) (Reformat[0x80000006]) profiling completed in 0.00667082 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0711436\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs: Identity_0\n",
      "[07/26/2023-05:59:03] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: Identity_0 (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00743269\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.014341\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00855395\n",
      "[07/26/2023-05:59:03] [V] [TRT] Identity_0 (Reformat[0x80000006]) profiling completed in 0.00499872 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00743269\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: Identity_0 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:59:03] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:59:03] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:03] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00670691\n",
      "[07/26/2023-05:59:03] [V] [TRT] Identity_0 (MyelinReformat[0x80000035]) profiling completed in 0.299785 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00670691\n",
      "[07/26/2023-05:59:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: MyelinReformat Tactic: 0x0000000000000000\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs: Identity_2\n",
      "[07/26/2023-05:59:03] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs: Identity_3\n",
      "[07/26/2023-05:59:03] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs: Identity_4\n",
      "[07/26/2023-05:59:03] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs: Identity_1\n",
      "[07/26/2023-05:59:03] [V] [TRT] *************** Autotuning Reformat: Float(4608,9,3,1) -> Float(4608,9,3,1) ***************\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: Identity_1 (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0589531\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0597379\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0593006\n",
      "[07/26/2023-05:59:03] [V] [TRT] Identity_1 (Reformat[0x80000006]) profiling completed in 0.00596324 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0589531\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: Identity_1 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:59:03] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:59:03] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:03] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0588175\n",
      "[07/26/2023-05:59:03] [V] [TRT] Identity_1 (MyelinReformat[0x80000035]) profiling completed in 0.288194 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0588175\n",
      "[07/26/2023-05:59:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: MyelinReformat Tactic: 0x0000000000000000\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:03] [V] [TRT] =============== Computing reformatting costs: Identity_5\n",
      "[07/26/2023-05:59:03] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: Identity_5 (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00529998\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0132343\n",
      "[07/26/2023-05:59:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00657621\n",
      "[07/26/2023-05:59:03] [V] [TRT] Identity_5 (Reformat[0x80000006]) profiling completed in 0.00584468 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00529998\n",
      "[07/26/2023-05:59:03] [V] [TRT] --------------- Timing Runner: Identity_5 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:59:04] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:59:04] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:04] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:04] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00680468\n",
      "[07/26/2023-05:59:04] [V] [TRT] Identity_5 (MyelinReformat[0x80000035]) profiling completed in 0.28934 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00680468\n",
      "[07/26/2023-05:59:04] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs: Identity_7\n",
      "[07/26/2023-05:59:04] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs: Identity_8\n",
      "[07/26/2023-05:59:04] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs: Identity_9\n",
      "[07/26/2023-05:59:04] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs: Identity_6\n",
      "[07/26/2023-05:59:04] [V] [TRT] *************** Autotuning Reformat: Float(2304,9,3,1) -> Float(2304,9,3,1) ***************\n",
      "[07/26/2023-05:59:04] [V] [TRT] --------------- Timing Runner: Identity_6 (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:04] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0163947\n",
      "[07/26/2023-05:59:04] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0169544\n",
      "[07/26/2023-05:59:04] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0168777\n",
      "[07/26/2023-05:59:04] [V] [TRT] Identity_6 (Reformat[0x80000006]) profiling completed in 0.00572285 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0163947\n",
      "[07/26/2023-05:59:04] [V] [TRT] --------------- Timing Runner: Identity_6 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:59:04] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:59:04] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:04] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:04] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0162702\n",
      "[07/26/2023-05:59:04] [V] [TRT] Identity_6 (MyelinReformat[0x80000035]) profiling completed in 0.288291 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0162702\n",
      "[07/26/2023-05:59:04] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: MyelinReformat Tactic: 0x0000000000000000\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs: Identity_10\n",
      "[07/26/2023-05:59:04] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:04] [V] [TRT] --------------- Timing Runner: Identity_10 (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:04] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00527984\n",
      "[07/26/2023-05:59:04] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0134683\n",
      "[07/26/2023-05:59:04] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00656229\n",
      "[07/26/2023-05:59:04] [V] [TRT] Identity_10 (Reformat[0x80000006]) profiling completed in 0.00645625 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00527984\n",
      "[07/26/2023-05:59:04] [V] [TRT] --------------- Timing Runner: Identity_10 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:59:04] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:59:04] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:04] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:04] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00674223\n",
      "[07/26/2023-05:59:04] [V] [TRT] Identity_10 (MyelinReformat[0x80000035]) profiling completed in 0.28658 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00674223\n",
      "[07/26/2023-05:59:04] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs: Identity_12\n",
      "[07/26/2023-05:59:04] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs: Identity_13\n",
      "[07/26/2023-05:59:04] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs: Identity_14\n",
      "[07/26/2023-05:59:04] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:04] [V] [TRT] =============== Computing reformatting costs: Identity_11\n",
      "[07/26/2023-05:59:04] [V] [TRT] *************** Autotuning Reformat: Float(1152,9,3,1) -> Float(1152,9,3,1) ***************\n",
      "[07/26/2023-05:59:04] [V] [TRT] --------------- Timing Runner: Identity_11 (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:04] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00553442\n",
      "[07/26/2023-05:59:04] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.013681\n",
      "[07/26/2023-05:59:04] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00674992\n",
      "[07/26/2023-05:59:04] [V] [TRT] Identity_11 (Reformat[0x80000006]) profiling completed in 0.00585023 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00553442\n",
      "[07/26/2023-05:59:04] [V] [TRT] --------------- Timing Runner: Identity_11 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:59:05] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:59:05] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:05] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00701823\n",
      "[07/26/2023-05:59:05] [V] [TRT] Identity_11 (MyelinReformat[0x80000035]) profiling completed in 0.289462 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00701823\n",
      "[07/26/2023-05:59:05] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: Identity_15\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Identity_15 (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00532554\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0132127\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00653964\n",
      "[07/26/2023-05:59:05] [V] [TRT] Identity_15 (Reformat[0x80000006]) profiling completed in 0.00625355 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00532554\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Identity_15 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:59:05] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:59:05] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:05] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00680533\n",
      "[07/26/2023-05:59:05] [V] [TRT] Identity_15 (MyelinReformat[0x80000035]) profiling completed in 0.299955 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00680533\n",
      "[07/26/2023-05:59:05] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: Identity_17\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: Identity_18\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: Identity_19\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: Identity_16\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(576,9,3,1) -> Float(576,9,3,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Identity_16 (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00541833\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.013878\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00665247\n",
      "[07/26/2023-05:59:05] [V] [TRT] Identity_16 (Reformat[0x80000006]) profiling completed in 0.00596272 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00541833\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Identity_16 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:59:05] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:59:05] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:05] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0069083\n",
      "[07/26/2023-05:59:05] [V] [TRT] Identity_16 (MyelinReformat[0x80000035]) profiling completed in 0.289781 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0069083\n",
      "[07/26/2023-05:59:05] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(802816,12544,112,1) -> Float(200704,1:4,1792,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.402286\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.313344\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.310994\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0126976 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.310994\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,7168,64) -> Float(802816,12544,112,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.494885\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.313051\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.311003\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.013978 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.311003\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,7168,64) -> Float(200704,1:4,1792,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.327241\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.31232\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.327095\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0126561 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.31232\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,1792,16) -> Float(802816,12544,112,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.500297\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.3128\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.310857\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0148807 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.310857\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.100352\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0825051\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0795086\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006]) profiling completed in 0.0060026 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0795086\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.101595\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0827977\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006]) profiling completed in 0.00592944 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.117102\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.08224\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006]) profiling completed in 0.00617137 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.083968\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0814811\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0838949\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006]) profiling completed in 0.00597784 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0814811\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.100425\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0826514\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0794377\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00627007 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0794377\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.101742\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0826469\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0794377\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00625645 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0794377\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.114907\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0820389\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0796526\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00649551 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0796526\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0838949\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0812617\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0839566\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00630531 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0812617\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.116955\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0821234\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00619929 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0838949\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0814811\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0838217\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00588053 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0814811\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer1/layer1.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.114981\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0821966\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(<in> -> /layer1/layer1.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00620906 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer1/layer1.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0843337\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.081408\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0841143\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(<in> -> /layer1/layer1.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00639299 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.081408\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0584655\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0423211\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.041024\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00579177 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.041024\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0600914\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0424263\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0410229\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00596086 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0410229\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0561844\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0426343\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0408423\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00577511 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0408423\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0435623\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0422069\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0435851\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00603315 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0422069\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.056445\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0427154\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0409234\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00555633 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0409234\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0435497\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0424206\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0434926\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0070911 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0424206\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0584168\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0423623\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0411314\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00585481 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0411314\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0602209\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.042376\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0410194\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.005814 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0410194\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0559299\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0426206\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0409634\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00776947 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0409634\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.04352\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0423371\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0435474\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00612655 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0423371\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0564663\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0425634\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0409394\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00998965 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0409394\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0435086\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0421909\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0434731\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00581003 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0421909\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0303634\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.023997\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0219389\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00534468 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0219389\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0309842\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0240884\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0219161\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00547581 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0219161\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0299255\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0243657\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0215197\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00546642 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0215197\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0233313\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0254118\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0233541\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00578163 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0233313\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0300434\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0231902\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0216261\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00580178 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0216261\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0233123\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0258248\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0232725\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00590797 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0232725\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:05] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0303954\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0240884\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0219148\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00543648 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0219148\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0311488\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0240335\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0218854\n",
      "[07/26/2023-05:59:05] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00552991 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0218854\n",
      "[07/26/2023-05:59:05] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0297545\n",
      "[07/26/2023-05:59:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0253714\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0215458\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00537753 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0215458\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0233731\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0267749\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0233496\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00602533 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0233496\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0288037\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0230211\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0214413\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00554491 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0214413\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0229473\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0231386\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0230093\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00603821 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0229473\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0156082\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.013477\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0104722\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0060018 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0104722\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0159949\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0135917\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0104989\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00585693 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0104989\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0140613\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0137529\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.01043\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00603452 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.01043\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0129684\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0137945\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0129539\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00597992 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0129539\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0142367\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0136274\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0104493\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00606297 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0104493\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0129707\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0137467\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.012917\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00592268 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.012917\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0156795\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0133876\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0105012\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00599166 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0105012\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0160269\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0134184\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0104438\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00583937 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0104438\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0140534\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0137542\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0104807\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00841207 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0104807\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0129135\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0137401\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0129707\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00602297 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0129135\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0142363\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0136864\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0104389\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00630391 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0104389\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0129387\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0141403\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0129543\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.0343872 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0129387\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(512,1,512,512) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00628214\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0140044\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00568703\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0124957 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00568703\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(128,1:4,128,128) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0060781\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0134059\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00598019\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00587575 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00598019\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(128,1:4,128,128) -> Float(512,1,1,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00650161\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0136578\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00557749\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0142851 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00557749\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(128,1:4,128,128) -> Float(512,1,512,512) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00819708\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0134674\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00611619\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00626048 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00611619\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(1000,1,1,1) -> Float(250,1:4,250,250) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00605352\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0132094\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00550312\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006]) profiling completed in 0.00588435 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00550312\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(1000,1,1000,1000) -> Float(1000,1,1,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00593975\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0136137\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00532944\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006]) profiling completed in 0.00599672 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00532944\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(1000,1,1000,1000) -> Float(250,1:4,250,250) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00603962\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0133677\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00616305\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006]) profiling completed in 0.00588569 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00603962\n",
      "[07/26/2023-05:59:06] [V] [TRT] *************** Autotuning Reformat: Float(250,1:4,250,250) -> Float(1000,1,1,1) ***************\n",
      "[07/26/2023-05:59:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00596762\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.013408\n",
      "[07/26/2023-05:59:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00546861\n",
      "[07/26/2023-05:59:06] [V] [TRT] Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006]) profiling completed in 0.00596956 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00546861\n",
      "[07/26/2023-05:59:06] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:59:06] [I] [TRT] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[07/26/2023-05:59:06] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to /conv1/Conv + /act1/Relu (x) from Float(150528,50176,224,1) to Float(50176,1:4,224,1)\n",
      "[07/26/2023-05:59:06] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (/layer4/layer4.0/act2/Relu_output_0) from Float(6272,1:4,896,128) to Float(25088,1,3584,512)\n",
      "[07/26/2023-05:59:06] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (/layer4/layer4.0/act2/Relu_output_0) from Float(25088,1,3584,512) to Float(6272,1:4,896,128)\n",
      "[07/26/2023-05:59:06] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 3 to /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (/layer4/layer4.0/act2/Relu_output_0) from Float(25088,1,3584,512) to Float(25088,49,7,1)\n",
      "[07/26/2023-05:59:06] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to /fc/Gemm (/global_pool/pool/GlobalAveragePool_output_0) from Float(512,1,1,1) to Float(128,1:4,128,128)\n",
      "[07/26/2023-05:59:06] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to reshape_after_/fc/Gemm (/fc/Gemm_out_tensor) from Float(250,1:4,250,250) to Float(1000,1,1,1)\n",
      "[07/26/2023-05:59:06] [V] [TRT] Formats and tactics selection completed in 12.8094 seconds.\n",
      "[07/26/2023-05:59:06] [V] [TRT] After reformat layers: 70 layers\n",
      "[07/26/2023-05:59:06] [V] [TRT] Total number of blocks in pre-optimized block assignment: 50\n",
      "[07/26/2023-05:59:06] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[07/26/2023-05:59:06] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:59:06] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:06] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:06] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:59:06] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:06] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:07] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:59:07] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:07] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:07] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:59:07] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:07] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:07] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:59:07] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:07] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:08] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:59:08] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:08] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /conv1/Conv + /act1/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: Identity_0 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: Identity_2 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: Identity_3 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: Identity_4 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: Identity_1 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: Identity_6 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /maxpool/MaxPool Host Persistent: 4048 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/act1/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 147456\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer2/layer2.0/conv2/Conv Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 589824\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer3/layer3.0/conv2/Conv Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 2359296\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer4/layer4.0/conv2/Conv Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu Host Persistent: 5488 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /global_pool/pool/GlobalAveragePool Host Persistent: 4112 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Layer: /fc/Gemm Host Persistent: 7200 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Skipped printing memory information for 41 layers with 0 memory size i.e. Host Persistent + Device Persistent + Scratch Memory == 0.\n",
      "[07/26/2023-05:59:08] [I] [TRT] Total Host Persistent Memory: 123200\n",
      "[07/26/2023-05:59:08] [I] [TRT] Total Device Persistent Memory: 0\n",
      "[07/26/2023-05:59:08] [I] [TRT] Total Scratch Memory: 2359296\n",
      "[07/26/2023-05:59:08] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 9 MiB, GPU 196 MiB\n",
      "[07/26/2023-05:59:08] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 52 steps to complete.\n",
      "[07/26/2023-05:59:08] [V] [TRT] STILL ALIVE: Started step 26 of 52\n",
      "[07/26/2023-05:59:08] [V] [TRT] STILL ALIVE: Started step 51 of 52\n",
      "[07/26/2023-05:59:08] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 5.58482ms to assign 22 blocks to 52 nodes requiring 169049088 bytes.\n",
      "[07/26/2023-05:59:08] [V] [TRT] Total number of blocks in optimized block assignment: 22\n",
      "[07/26/2023-05:59:08] [I] [TRT] Total Activation Memory: 169049088\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /conv1/Conv + /act1/Relu Set kernel index: 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /maxpool/MaxPool Set kernel index: 1\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu Set kernel index: 2\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu Set kernel index: 2\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/act1/Relu Set kernel index: 2\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu Set kernel index: 2\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu Set kernel index: 2\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer2/layer2.0/conv2/Conv Set kernel index: 3\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu Set kernel index: 4\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu Set kernel index: 3\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu Set kernel index: 3\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu Set kernel index: 3\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer3/layer3.0/conv2/Conv Set kernel index: 3\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu Set kernel index: 5\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu Set kernel index: 3\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu Set kernel index: 3\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu Set kernel index: 3\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer4/layer4.0/conv2/Conv Set kernel index: 3\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu Set kernel index: 5\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu Set kernel index: 6\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu Set kernel index: 7\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /global_pool/pool/GlobalAveragePool Set kernel index: 8\n",
      "[07/26/2023-05:59:08] [V] [TRT] Finalize: /fc/Gemm Set kernel index: 9\n",
      "[07/26/2023-05:59:08] [V] [TRT] Total number of generated kernels selected for the engine: 10\n",
      "[07/26/2023-05:59:08] [V] [TRT] Kernel: 0 CASK_STATIC\n",
      "[07/26/2023-05:59:08] [V] [TRT] Kernel: 1 CASK_STATIC\n",
      "[07/26/2023-05:59:08] [V] [TRT] Kernel: 2 CASK_STATIC\n",
      "[07/26/2023-05:59:08] [V] [TRT] Kernel: 3 CASK_STATIC\n",
      "[07/26/2023-05:59:08] [V] [TRT] Kernel: 4 CASK_STATIC\n",
      "[07/26/2023-05:59:08] [V] [TRT] Kernel: 5 CASK_STATIC\n",
      "[07/26/2023-05:59:08] [V] [TRT] Kernel: 6 CASK_STATIC\n",
      "[07/26/2023-05:59:08] [V] [TRT] Kernel: 7 CASK_STATIC\n",
      "[07/26/2023-05:59:08] [V] [TRT] Kernel: 8 CASK_STATIC\n",
      "[07/26/2023-05:59:08] [V] [TRT] Kernel: 9 CASK_STATIC\n",
      "[07/26/2023-05:59:08] [V] [TRT] Disabling unused tactic source: JIT_CONVOLUTIONS\n",
      "[07/26/2023-05:59:08] [V] [TRT] Engine generation completed in 14.741 seconds.\n",
      "[07/26/2023-05:59:08] [V] [TRT] Deleting timing cache: 156 entries, served 180 hits since creation.\n",
      "[07/26/2023-05:59:08] [V] [TRT] Engine Layer Information:\n",
      "Layer(Constant): onnx::Conv_239, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 0) [Constant]_output (Float[512])\n",
      "Layer(Constant): onnx::Conv_239_clone_1, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 0) [Constant]_output_clone_1 (Float[512])\n",
      "Layer(Constant): onnx::Conv_239_clone_2, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 0) [Constant]_output_clone_2 (Float[512])\n",
      "Layer(Constant): onnx::Conv_239_clone_3, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 0) [Constant]_output_clone_3 (Float[512])\n",
      "Layer(Constant): onnx::Conv_241, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 2) [Constant]_output (Float[512,512,3,3])\n",
      "Layer(Constant): onnx::Conv_224, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 7) [Constant]_output (Float[256])\n",
      "Layer(Constant): onnx::Conv_224_clone_1, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 7) [Constant]_output_clone_1 (Float[256])\n",
      "Layer(Constant): onnx::Conv_224_clone_2, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 7) [Constant]_output_clone_2 (Float[256])\n",
      "Layer(Constant): onnx::Conv_224_clone_3, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 7) [Constant]_output_clone_3 (Float[256])\n",
      "Layer(Constant): onnx::Conv_226, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 9) [Constant]_output (Float[256,256,3,3])\n",
      "Layer(Constant): onnx::Conv_209, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 14) [Constant]_output (Float[128])\n",
      "Layer(Constant): onnx::Conv_209_clone_1, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 14) [Constant]_output_clone_1 (Float[128])\n",
      "Layer(Constant): onnx::Conv_209_clone_2, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 14) [Constant]_output_clone_2 (Float[128])\n",
      "Layer(Constant): onnx::Conv_209_clone_3, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 14) [Constant]_output_clone_3 (Float[128])\n",
      "Layer(Constant): onnx::Conv_211, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 16) [Constant]_output (Float[128,128,3,3])\n",
      "Layer(Constant): onnx::Conv_194, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 21) [Constant]_output (Float[64])\n",
      "Layer(Constant): onnx::Conv_194_clone_1, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 21) [Constant]_output_clone_1 (Float[64])\n",
      "Layer(Constant): onnx::Conv_194_clone_2, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 21) [Constant]_output_clone_2 (Float[64])\n",
      "Layer(Constant): onnx::Conv_194_clone_3, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 21) [Constant]_output_clone_3 (Float[64])\n",
      "Layer(Constant): onnx::Conv_199, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 23) [Constant]_output (Float[64,64,3,3])\n",
      "Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to /conv1/Conv + /act1/Relu, Tactic: 0x00000000000003ea, x (Float[-1,3,224,224]) -> Reformatted Input Tensor 0 to /conv1/Conv + /act1/Relu (Float[-1,3:4,224,224])\n",
      "Layer(CaskConvolution): /conv1/Conv + /act1/Relu, Tactic: 0x9cb304e2edbc1221, Reformatted Input Tensor 0 to /conv1/Conv + /act1/Relu (Float[-1,3:4,224,224]) -> /act1/Relu_output_0 (Float[-1,64:4,112,112])\n",
      "Layer(MyelinReformat): Identity_0, Tactic: 0x0000000000000000, (Unnamed Layer* 0) [Constant]_output (Float[512]) -> onnx::Conv_251 (Float[512])\n",
      "Layer(MyelinReformat): Identity_2, Tactic: 0x0000000000000000, (Unnamed Layer* 0) [Constant]_output_clone_1 (Float[512]) -> onnx::Conv_248 (Float[512])\n",
      "Layer(MyelinReformat): Identity_3, Tactic: 0x0000000000000000, (Unnamed Layer* 0) [Constant]_output_clone_2 (Float[512]) -> onnx::Conv_245 (Float[512])\n",
      "Layer(MyelinReformat): Identity_4, Tactic: 0x0000000000000000, (Unnamed Layer* 0) [Constant]_output_clone_3 (Float[512]) -> onnx::Conv_242 (Float[512])\n",
      "Layer(MyelinReformat): Identity_1, Tactic: 0x0000000000000000, (Unnamed Layer* 2) [Constant]_output (Float[512,512,3,3]) -> onnx::Conv_250 (Float[512,512,3,3])\n",
      "Layer(Reformat): Identity_5, Tactic: 0x00000000000003e8, (Unnamed Layer* 7) [Constant]_output (Float[256]) -> onnx::Conv_236 (Float[256])\n",
      "Layer(Reformat): Identity_7, Tactic: 0x00000000000003e8, (Unnamed Layer* 7) [Constant]_output_clone_1 (Float[256]) -> onnx::Conv_233 (Float[256])\n",
      "Layer(Reformat): Identity_8, Tactic: 0x00000000000003e8, (Unnamed Layer* 7) [Constant]_output_clone_2 (Float[256]) -> onnx::Conv_230 (Float[256])\n",
      "Layer(Reformat): Identity_9, Tactic: 0x00000000000003e8, (Unnamed Layer* 7) [Constant]_output_clone_3 (Float[256]) -> onnx::Conv_227 (Float[256])\n",
      "Layer(MyelinReformat): Identity_6, Tactic: 0x0000000000000000, (Unnamed Layer* 9) [Constant]_output (Float[256,256,3,3]) -> onnx::Conv_235 (Float[256,256,3,3])\n",
      "Layer(Reformat): Identity_10, Tactic: 0x00000000000003e8, (Unnamed Layer* 14) [Constant]_output (Float[128]) -> onnx::Conv_221 (Float[128])\n",
      "Layer(Reformat): Identity_12, Tactic: 0x00000000000003e8, (Unnamed Layer* 14) [Constant]_output_clone_1 (Float[128]) -> onnx::Conv_218 (Float[128])\n",
      "Layer(Reformat): Identity_13, Tactic: 0x00000000000003e8, (Unnamed Layer* 14) [Constant]_output_clone_2 (Float[128]) -> onnx::Conv_215 (Float[128])\n",
      "Layer(Reformat): Identity_14, Tactic: 0x00000000000003e8, (Unnamed Layer* 14) [Constant]_output_clone_3 (Float[128]) -> onnx::Conv_212 (Float[128])\n",
      "Layer(Reformat): Identity_11, Tactic: 0x00000000000003e8, (Unnamed Layer* 16) [Constant]_output (Float[128,128,3,3]) -> onnx::Conv_220 (Float[128,128,3,3])\n",
      "Layer(Reformat): Identity_15, Tactic: 0x00000000000003e8, (Unnamed Layer* 21) [Constant]_output (Float[64]) -> onnx::Conv_206 (Float[64])\n",
      "Layer(Reformat): Identity_17, Tactic: 0x00000000000003e8, (Unnamed Layer* 21) [Constant]_output_clone_1 (Float[64]) -> onnx::Conv_203 (Float[64])\n",
      "Layer(Reformat): Identity_18, Tactic: 0x00000000000003e8, (Unnamed Layer* 21) [Constant]_output_clone_2 (Float[64]) -> onnx::Conv_200 (Float[64])\n",
      "Layer(Reformat): Identity_19, Tactic: 0x00000000000003e8, (Unnamed Layer* 21) [Constant]_output_clone_3 (Float[64]) -> onnx::Conv_197 (Float[64])\n",
      "Layer(Reformat): Identity_16, Tactic: 0x00000000000003e8, (Unnamed Layer* 23) [Constant]_output (Float[64,64,3,3]) -> onnx::Conv_205 (Float[64,64,3,3])\n",
      "Layer(CaskPooling): /maxpool/MaxPool, Tactic: 0x789b2859f2e03e79, /act1/Relu_output_0 (Float[-1,64:4,112,112]) -> /maxpool/MaxPool_output_0 (Float[-1,64:4,56,56])\n",
      "Layer(CaskConvolution): /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu, Tactic: 0x3a8712b17741b582, /maxpool/MaxPool_output_0 (Float[-1,64:4,56,56]), onnx::Conv_197 (Float[64]) -> /layer1/layer1.0/act1/Relu_output_0 (Float[-1,64:4,56,56])\n",
      "Layer(CaskConvolution): /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu, Tactic: 0x3a8712b17741b582, /layer1/layer1.0/act1/Relu_output_0 (Float[-1,64:4,56,56]), onnx::Conv_200 (Float[64]), /maxpool/MaxPool_output_0 (Float[-1,64:4,56,56]) -> /layer1/layer1.0/act2/Relu_output_0 (Float[-1,64:4,56,56])\n",
      "Layer(CaskConvolution): /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/act1/Relu, Tactic: 0x3a8712b17741b582, /layer1/layer1.0/act2/Relu_output_0 (Float[-1,64:4,56,56]), onnx::Conv_203 (Float[64]) -> /layer1/layer1.1/act1/Relu_output_0 (Float[-1,64:4,56,56])\n",
      "Layer(CaskConvolution): /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu, Tactic: 0x3a8712b17741b582, /layer1/layer1.1/act1/Relu_output_0 (Float[-1,64:4,56,56]), onnx::Conv_205 (Float[64,64,3,3]), onnx::Conv_206 (Float[64]), /layer1/layer1.0/act2/Relu_output_0 (Float[-1,64:4,56,56]) -> /layer1/layer1.1/act2/Relu_output_0 (Float[-1,64:4,56,56])\n",
      "Layer(CaskConvolution): /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu, Tactic: 0x3a8712b17741b582, /layer1/layer1.1/act2/Relu_output_0 (Float[-1,64:4,56,56]) -> /layer2/layer2.0/act1/Relu_output_0 (Float[-1,128:4,28,28])\n",
      "Layer(CaskConvolution): /layer2/layer2.0/conv2/Conv, Tactic: 0x999e005e3b016ea6, /layer2/layer2.0/act1/Relu_output_0 (Float[-1,128:4,28,28]), onnx::Conv_212 (Float[128]) -> /layer2/layer2.0/conv2/Conv_output_0 (Float[-1,128:4,28,28])\n",
      "Layer(CaskConvolution): /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu, Tactic: 0x72a5d05b1bb165ef, /layer1/layer1.1/act2/Relu_output_0 (Float[-1,64:4,56,56]), onnx::Conv_215 (Float[128]), /layer2/layer2.0/conv2/Conv_output_0 (Float[-1,128:4,28,28]) -> /layer2/layer2.0/act2/Relu_output_0 (Float[-1,128:4,28,28])\n",
      "Layer(CaskConvolution): /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu, Tactic: 0x999e005e3b016ea6, /layer2/layer2.0/act2/Relu_output_0 (Float[-1,128:4,28,28]), onnx::Conv_218 (Float[128]) -> /layer2/layer2.1/act1/Relu_output_0 (Float[-1,128:4,28,28])\n",
      "Layer(CaskConvolution): /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu, Tactic: 0x999e005e3b016ea6, /layer2/layer2.1/act1/Relu_output_0 (Float[-1,128:4,28,28]), onnx::Conv_220 (Float[128,128,3,3]), onnx::Conv_221 (Float[128]), /layer2/layer2.0/act2/Relu_output_0 (Float[-1,128:4,28,28]) -> /layer2/layer2.1/act2/Relu_output_0 (Float[-1,128:4,28,28])\n",
      "Layer(CaskConvolution): /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu, Tactic: 0x999e005e3b016ea6, /layer2/layer2.1/act2/Relu_output_0 (Float[-1,128:4,28,28]) -> /layer3/layer3.0/act1/Relu_output_0 (Float[-1,256:4,14,14])\n",
      "Layer(CaskConvolution): /layer3/layer3.0/conv2/Conv, Tactic: 0x999e005e3b016ea6, /layer3/layer3.0/act1/Relu_output_0 (Float[-1,256:4,14,14]), onnx::Conv_227 (Float[256]) -> /layer3/layer3.0/conv2/Conv_output_0 (Float[-1,256:4,14,14])\n",
      "Layer(CaskConvolution): /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu, Tactic: 0x130df49cb195156b, /layer2/layer2.1/act2/Relu_output_0 (Float[-1,128:4,28,28]), onnx::Conv_230 (Float[256]), /layer3/layer3.0/conv2/Conv_output_0 (Float[-1,256:4,14,14]) -> /layer3/layer3.0/act2/Relu_output_0 (Float[-1,256:4,14,14])\n",
      "Layer(CaskConvolution): /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu, Tactic: 0x999e005e3b016ea6, /layer3/layer3.0/act2/Relu_output_0 (Float[-1,256:4,14,14]), onnx::Conv_233 (Float[256]) -> /layer3/layer3.1/act1/Relu_output_0 (Float[-1,256:4,14,14])\n",
      "Layer(CaskConvolution): /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu, Tactic: 0x999e005e3b016ea6, /layer3/layer3.1/act1/Relu_output_0 (Float[-1,256:4,14,14]), onnx::Conv_235 (Float[256,256,3,3]), onnx::Conv_236 (Float[256]), /layer3/layer3.0/act2/Relu_output_0 (Float[-1,256:4,14,14]) -> /layer3/layer3.1/act2/Relu_output_0 (Float[-1,256:4,14,14])\n",
      "Layer(CaskConvolution): /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu, Tactic: 0x999e005e3b016ea6, /layer3/layer3.1/act2/Relu_output_0 (Float[-1,256:4,14,14]) -> /layer4/layer4.0/act1/Relu_output_0 (Float[-1,512:4,7,7])\n",
      "Layer(CaskConvolution): /layer4/layer4.0/conv2/Conv, Tactic: 0x999e005e3b016ea6, /layer4/layer4.0/act1/Relu_output_0 (Float[-1,512:4,7,7]), onnx::Conv_242 (Float[512]) -> /layer4/layer4.0/conv2/Conv_output_0 (Float[-1,512:4,7,7])\n",
      "Layer(CaskConvolution): /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu, Tactic: 0x130df49cb195156b, /layer3/layer3.1/act2/Relu_output_0 (Float[-1,256:4,14,14]), onnx::Conv_245 (Float[512]), /layer4/layer4.0/conv2/Conv_output_0 (Float[-1,512:4,7,7]) -> Reformatted Output Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (Float[-1,512:4,7,7])\n",
      "Layer(NoOp): Reformatting CopyNode for Output Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (Float[-1,512:4,7,7]) -> /layer4/layer4.0/act2/Relu_output_0 (Float[-1,512,7,7])\n",
      "Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu, Tactic: 0x0000000000000000, /layer4/layer4.0/act2/Relu_output_0 (Float[-1,512,7,7]) -> Reformatted Input Tensor 0 to /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (Float[-1,512:4,7,7])\n",
      "Layer(CaskConvolution): /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu, Tactic: 0x1323e48791e2f671, Reformatted Input Tensor 0 to /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (Float[-1,512:4,7,7]), onnx::Conv_248 (Float[512]) -> /layer4/layer4.1/act1/Relu_output_0 (Float[-1,512,7,7])\n",
      "Layer(Reformat): Reformatting CopyNode for Input Tensor 3 to /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu, Tactic: 0x0000000000000000, /layer4/layer4.0/act2/Relu_output_0 (Float[-1,512,7,7]) -> Reformatted Input Tensor 3 to /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (Float[-1,512,7,7])\n",
      "Layer(CaskConvolution): /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu, Tactic: 0x5aa723e0481da855, /layer4/layer4.1/act1/Relu_output_0 (Float[-1,512,7,7]), onnx::Conv_250 (Float[512,512,3,3]), onnx::Conv_251 (Float[512]), Reformatted Input Tensor 3 to /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (Float[-1,512,7,7]) -> /layer4/layer4.1/act2/Relu_output_0 (Float[-1,512,7,7])\n",
      "Layer(CaskPooling): /global_pool/pool/GlobalAveragePool, Tactic: 0x933eceba7b866d59, /layer4/layer4.1/act2/Relu_output_0 (Float[-1,512,7,7]) -> /global_pool/pool/GlobalAveragePool_output_0 (Float[-1,512,1,1])\n",
      "Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to /fc/Gemm, Tactic: 0x0000000000000000, /global_pool/pool/GlobalAveragePool_output_0 (Float[-1,512,1,1]) -> Reformatted Input Tensor 0 to /fc/Gemm (Float[-1,512:4,1,1])\n",
      "Layer(CaskGemmConvolution): /fc/Gemm, Tactic: 0x00000000000203be, Reformatted Input Tensor 0 to /fc/Gemm (Float[-1,512:4,1,1]) -> /fc/Gemm_out_tensor (Float[-1,1000:4,1,1])\n",
      "Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to reshape_after_/fc/Gemm, Tactic: 0x0000000000000000, /fc/Gemm_out_tensor (Float[-1,1000:4,1,1]) -> Reformatted Input Tensor 0 to reshape_after_/fc/Gemm (Float[-1,1000,1,1])\n",
      "Layer(NoOp): reshape_after_/fc/Gemm, Tactic: 0x0000000000000000, Reformatted Input Tensor 0 to reshape_after_/fc/Gemm (Float[-1,1000,1,1]) -> outputs (Float[-1,1000])\n",
      "[07/26/2023-05:59:08] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +45, now: CPU 0, GPU 45 (MiB)\n",
      "[07/26/2023-05:59:08] [V] [TRT] Adding 1 engine(s) to plan file.\n",
      "[07/26/2023-05:59:08] [I] Engine built in 25.786 sec.\n",
      "[07/26/2023-05:59:08] [I] [TRT] Loaded engine size: 46 MiB\n",
      "[07/26/2023-05:59:08] [V] [TRT] Deserialization required 35460 microseconds.\n",
      "[07/26/2023-05:59:08] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +44, now: CPU 0, GPU 44 (MiB)\n",
      "[07/26/2023-05:59:08] [I] Engine deserialized in 0.0370726 sec.\n",
      "[07/26/2023-05:59:08] [V] [TRT] Total per-runner device persistent memory is 0\n",
      "[07/26/2023-05:59:08] [V] [TRT] Total per-runner host persistent memory is 123200\n",
      "[07/26/2023-05:59:08] [V] [TRT] Allocated activation device memory of size 169049088\n",
      "[07/26/2023-05:59:08] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +161, now: CPU 0, GPU 205 (MiB)\n",
      "[07/26/2023-05:59:08] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "[07/26/2023-05:59:08] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[07/26/2023-05:59:08] [V] Using enqueueV3.\n",
      "[07/26/2023-05:59:08] [I] Using random values for input x\n",
      "[07/26/2023-05:59:08] [I] Input binding for x with dimensions 16x3x224x224 is created.\n",
      "[07/26/2023-05:59:08] [I] Output binding for outputs with dimensions 16x1000 is created.\n",
      "[07/26/2023-05:59:08] [I] Starting inference\n",
      "[07/26/2023-05:59:11] [I] Warmup completed 30 queries over 200 ms\n",
      "[07/26/2023-05:59:11] [I] Timing trace has 503 queries over 3.01592 s\n",
      "[07/26/2023-05:59:11] [I] \n",
      "[07/26/2023-05:59:11] [I] === Trace details ===\n",
      "[07/26/2023-05:59:11] [I] Trace averages of 10 runs:\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.46635 ms - Host latency: 7.26955 ms (enqueue 0.0163589 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00125 ms - Host latency: 6.80762 ms (enqueue 0.0301208 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00382 ms - Host latency: 6.80831 ms (enqueue 0.0209473 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00197 ms - Host latency: 6.80934 ms (enqueue 0.0157593 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00187 ms - Host latency: 6.80278 ms (enqueue 0.0121918 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00186 ms - Host latency: 6.80771 ms (enqueue 0.0189911 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00381 ms - Host latency: 6.80719 ms (enqueue 0.0233948 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.003 ms - Host latency: 6.80379 ms (enqueue 0.017334 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00157 ms - Host latency: 6.80721 ms (enqueue 0.0246948 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00259 ms - Host latency: 6.80272 ms (enqueue 0.0177673 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00258 ms - Host latency: 6.80624 ms (enqueue 0.0159851 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00227 ms - Host latency: 6.80438 ms (enqueue 0.0141541 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00278 ms - Host latency: 6.81026 ms (enqueue 0.0213867 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00311 ms - Host latency: 6.80585 ms (enqueue 0.0119751 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00328 ms - Host latency: 6.80548 ms (enqueue 0.01604 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00298 ms - Host latency: 6.80691 ms (enqueue 0.0171143 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00332 ms - Host latency: 6.81799 ms (enqueue 0.0216309 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.00297 ms - Host latency: 6.80831 ms (enqueue 0.0144897 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 6.02295 ms - Host latency: 6.82402 ms (enqueue 0.0219849 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.97186 ms - Host latency: 6.77584 ms (enqueue 0.02229 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95332 ms - Host latency: 6.75626 ms (enqueue 0.0148682 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95142 ms - Host latency: 6.75465 ms (enqueue 0.0175415 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95168 ms - Host latency: 6.74955 ms (enqueue 0.0196533 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95262 ms - Host latency: 6.75741 ms (enqueue 0.017749 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95117 ms - Host latency: 6.75709 ms (enqueue 0.0209961 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95181 ms - Host latency: 6.74939 ms (enqueue 0.0183838 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95237 ms - Host latency: 6.7584 ms (enqueue 0.0275269 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95374 ms - Host latency: 6.75468 ms (enqueue 0.014563 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.9532 ms - Host latency: 6.75251 ms (enqueue 0.0138428 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95189 ms - Host latency: 6.76383 ms (enqueue 0.0174072 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95271 ms - Host latency: 6.76075 ms (enqueue 0.0156738 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95256 ms - Host latency: 6.7595 ms (enqueue 0.0153564 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95085 ms - Host latency: 6.7573 ms (enqueue 0.020752 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95234 ms - Host latency: 6.75293 ms (enqueue 0.0153076 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95027 ms - Host latency: 6.7502 ms (enqueue 0.0112305 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95107 ms - Host latency: 6.76274 ms (enqueue 0.0210449 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95212 ms - Host latency: 6.7543 ms (enqueue 0.0140137 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95222 ms - Host latency: 6.75845 ms (enqueue 0.0181641 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95396 ms - Host latency: 6.75725 ms (enqueue 0.0147461 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95256 ms - Host latency: 6.75349 ms (enqueue 0.0151611 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95222 ms - Host latency: 6.75415 ms (enqueue 0.0147217 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95193 ms - Host latency: 6.75779 ms (enqueue 0.0306885 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95198 ms - Host latency: 6.75261 ms (enqueue 0.02229 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95249 ms - Host latency: 6.75374 ms (enqueue 0.0152832 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95022 ms - Host latency: 6.74949 ms (enqueue 0.0148926 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95129 ms - Host latency: 6.75452 ms (enqueue 0.017749 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95239 ms - Host latency: 6.76565 ms (enqueue 0.0184814 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95022 ms - Host latency: 6.76565 ms (enqueue 0.0256592 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95037 ms - Host latency: 6.75378 ms (enqueue 0.016626 ms)\n",
      "[07/26/2023-05:59:11] [I] Average on 10 runs - GPU latency: 5.95356 ms - Host latency: 6.75842 ms (enqueue 0.0150391 ms)\n",
      "[07/26/2023-05:59:11] [I] \n",
      "[07/26/2023-05:59:11] [I] === Performance summary ===\n",
      "[07/26/2023-05:59:11] [I] Throughput: 166.781 qps\n",
      "[07/26/2023-05:59:11] [I] Latency: min = 6.74072 ms, max = 7.28281 ms, mean = 6.7854 ms, median = 6.76392 ms, percentile(90%) = 6.81091 ms, percentile(95%) = 6.81958 ms, percentile(99%) = 7.26459 ms\n",
      "[07/26/2023-05:59:11] [I] Enqueue Time: min = 0.00732422 ms, max = 0.0952148 ms, mean = 0.0181866 ms, median = 0.0161743 ms, percentile(90%) = 0.029541 ms, percentile(95%) = 0.0351562 ms, percentile(99%) = 0.0529785 ms\n",
      "[07/26/2023-05:59:11] [I] H2D Latency: min = 0.78363 ms, max = 0.887451 ms, mean = 0.794184 ms, median = 0.791809 ms, percentile(90%) = 0.804688 ms, percentile(95%) = 0.808838 ms, percentile(99%) = 0.825195 ms\n",
      "[07/26/2023-05:59:11] [I] GPU Compute Time: min = 5.93799 ms, max = 6.47064 ms, mean = 5.98114 ms, median = 5.95459 ms, percentile(90%) = 6.00476 ms, percentile(95%) = 6.00671 ms, percentile(99%) = 6.46657 ms\n",
      "[07/26/2023-05:59:11] [I] D2H Latency: min = 0.0078125 ms, max = 0.0332642 ms, mean = 0.0100841 ms, median = 0.0101318 ms, percentile(90%) = 0.0113525 ms, percentile(95%) = 0.0117188 ms, percentile(99%) = 0.0123291 ms\n",
      "[07/26/2023-05:59:11] [I] Total Host Walltime: 3.01592 s\n",
      "[07/26/2023-05:59:11] [I] Total GPU Compute Time: 3.00851 s\n",
      "[07/26/2023-05:59:11] [W] * GPU compute time is unstable, with coefficient of variance = 1.23671%.\n",
      "[07/26/2023-05:59:11] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
      "[07/26/2023-05:59:11] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[07/26/2023-05:59:11] [V] \n",
      "[07/26/2023-05:59:11] [V] === Explanations of the performance metrics ===\n",
      "[07/26/2023-05:59:11] [V] Total Host Walltime: the host walltime from when the first query (after warmups) is enqueued to when the last query is completed.\n",
      "[07/26/2023-05:59:11] [V] GPU Compute Time: the GPU latency to execute the kernels for a query.\n",
      "[07/26/2023-05:59:11] [V] Total GPU Compute Time: the summation of the GPU Compute Time of all the queries. If this is significantly shorter than Total Host Walltime, the GPU may be under-utilized because of host-side overheads or data transfers.\n",
      "[07/26/2023-05:59:11] [V] Throughput: the observed throughput computed by dividing the number of queries by the Total Host Walltime. If this is significantly lower than the reciprocal of GPU Compute Time, the GPU may be under-utilized because of host-side overheads or data transfers.\n",
      "[07/26/2023-05:59:11] [V] Enqueue Time: the host latency to enqueue a query. If this is longer than GPU Compute Time, the GPU may be under-utilized.\n",
      "[07/26/2023-05:59:11] [V] H2D Latency: the latency for host-to-device data transfers for input tensors of a single query.\n",
      "[07/26/2023-05:59:11] [V] D2H Latency: the latency for device-to-host data transfers for output tensors of a single query.\n",
      "[07/26/2023-05:59:11] [V] Latency: the summation of H2D Latency, GPU Compute Time, and D2H Latency. This is the latency to infer a single query.\n",
      "[07/26/2023-05:59:11] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8601] # /opt/tensorrt/bin/trtexec --onnx=resnet18.onnx --minShapes=x:1x3x224x224 --optShapes=x:16x3x224x224 --maxShapes=x:32x3x224x224 --useCudaGraph --saveEngine=resnet18.plan --verbose=true\n"
     ]
    }
   ],
   "source": [
    "# change onnx model to tensorrt using trtexec\n",
    "\n",
    "! /opt/tensorrt/bin/trtexec --onnx=resnet18.onnx --minShapes=x:1x3x224x224 --optShapes=x:16x3x224x224 --maxShapes=x:32x3x224x224 --useCudaGraph --saveEngine=resnet18.plan --verbose=true \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_379905/3417225990.py:65: DeprecationWarning: Use set_input_shape instead.\n",
      "  context.set_binding_shape(0, in_shape) # Need to specify binding shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/26/2023-06:00:11] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[07/26/2023-06:00:11] [TRT] [I] Loaded engine size: 46 MiB\n",
      "[07/26/2023-06:00:11] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +45, now: CPU 0, GPU 250 (MiB)\n",
      "[07/26/2023-06:00:11] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +161, now: CPU 0, GPU 411 (MiB)\n",
      "[07/26/2023-06:00:11] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "{'x': {'index': 0, 'name': 'x', 'dtype': dtype('float32'), 'shape': [16, 3, 224, 224], 'allocation': 139688715223040}}\n",
      "{'outputs': {'index': 1, 'name': 'outputs', 'dtype': dtype('float32'), 'shape': [16, 1000], 'allocation': 139690907084800}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 21.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# type: ignore\n",
    "from tensorrt_handson_lab.tensorrt_utils import common\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "import tensorrt as trt\n",
    "from cuda import cudart\n",
    "import time\n",
    "\n",
    "def infer(input_bindings, output_bindings, context, batch: Dict[str, np.ndarray]):\n",
    "    # Copy given input to device memory (GPU memory)\n",
    "    \n",
    "    st = time.time()\n",
    "\n",
    "    allocations = []\n",
    "    for k, bindings in input_bindings.items():\n",
    "        allocations.append(bindings[\"allocation\"])\n",
    "\n",
    "    for k, bindings in output_bindings.items():\n",
    "        allocations.append(bindings[\"allocation\"])\n",
    "\n",
    "    for k, val in batch.items():\n",
    "        if input_bindings[k][\"shape\"][0] > val.shape[0]:\n",
    "            padded = np.zeros(dtype=input_bindings[k][\"dtype\"], shape=input_bindings[k][\"shape\"])\n",
    "            padded[: len(val)] = val\n",
    "        common.memcpy_host_to_device(\n",
    "            input_bindings[k][\"allocation\"],\n",
    "            np.ascontiguousarray(val.astype(input_bindings[k][\"dtype\"])),\n",
    "        )\n",
    "    \n",
    "    # execute model with tensorrt runtime\n",
    "    context.execute_v2(allocations)\n",
    "\n",
    "    # prepare host memory\n",
    "    output_dict = {}\n",
    "    for k, ob in output_bindings.items():\n",
    "        host_output = np.zeros(dtype=ob[\"dtype\"], shape=ob[\"shape\"])\n",
    "        common.memcpy_device_to_host(host_output, ob[\"allocation\"])\n",
    "        output_dict[k] = host_output\n",
    "        \n",
    "\n",
    "    cost = time.time() - st\n",
    "    return output_dict, cost\n",
    "\n",
    "torch_load_time = 0\n",
    "trt_load_time = 0\n",
    "\n",
    "with open(\"resnet18.plan\", \"rb\")as f, trt.Runtime(trt.Logger(trt.Logger.INFO)) as runtime:\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "in_name = engine.get_tensor_name(0)\n",
    "in_dtype = np.dtype(trt.nptype(engine.get_tensor_dtype(in_name)))\n",
    "in_shape = list((B,) + IN_SHAPE)\n",
    "in_size = in_dtype.itemsize * np.prod(in_shape)\n",
    "input_bindings = {\n",
    "    in_name : {\n",
    "        \"index\" : 0,\n",
    "        \"name\" : in_name,\n",
    "        \"dtype\" : in_dtype,\n",
    "        \"shape\" : in_shape,\n",
    "        \"allocation\" : common.cuda_call(cudart.cudaMalloc(in_size))\n",
    "    }\n",
    "}\n",
    "context.set_binding_shape(0, in_shape) # Need to specify binding shape\n",
    "\n",
    "out_name = engine.get_tensor_name(1)\n",
    "out_dtype = np.dtype(trt.nptype(engine.get_tensor_dtype(out_name)))\n",
    "out_shape = list((B,) + OUT_SHAPE)\n",
    "out_size = out_dtype.itemsize * np.prod(out_shape)\n",
    "output_bindings = {\n",
    "    out_name : {\n",
    "        \"index\" : 1,\n",
    "        \"name\" : out_name,\n",
    "        \"dtype\" : out_dtype,\n",
    "        \"shape\" : out_shape,\n",
    "        \"allocation\" : common.cuda_call(cudart.cudaMalloc(out_size))\n",
    "    }\n",
    "}\n",
    "\n",
    "print(input_bindings)\n",
    "print(output_bindings)\n",
    "\n",
    "model.cpu()\n",
    "st = time.time()\n",
    "model = model.cuda().eval()\n",
    "trt_load_time = time.time() - st\n",
    "\n",
    "mean_diff = 0\n",
    "torch_cost = 0\n",
    "trt_cost = 0\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(NUM_TEST)):\n",
    "        st = time.time()\n",
    "        input_dict = {\"x\" : torch.randn(*((B,) + IN_SHAPE)).cuda()}\n",
    "        torch_output = model(**input_dict)\n",
    "        torch_cost += time.time() - st\n",
    "        trt_output, trt_cost_batch = infer(\n",
    "            input_bindings=input_bindings,\n",
    "            output_bindings=output_bindings, \n",
    "            context=context,\n",
    "            batch={k:v.cpu().numpy() for k,v in input_dict.items()})\n",
    "        trt_cost += trt_cost_batch\n",
    "        mean_diff += (torch_output.cpu() - torch.from_numpy(trt_output[\"outputs\"])).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6373e-09)\n",
      "0.02593855857849121 0.01138007640838623\n"
     ]
    }
   ],
   "source": [
    "print(mean_diff / NUM_TEST)\n",
    "\n",
    "print(torch_cost / NUM_TEST, trt_cost /  NUM_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
