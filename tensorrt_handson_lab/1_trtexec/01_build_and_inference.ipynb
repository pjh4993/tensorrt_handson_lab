{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that you are running on tensorrt container\n",
    "! which trtexec\n",
    "# If you want to check trtexec running options, run trtexec -h \n",
    "# ! trtexec -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x)\n",
      "Exported graph: graph(%x : Float(*, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cpu),\n",
      "      %fc.weight : Float(1000, 512, strides=[512, 1], requires_grad=1, device=cpu),\n",
      "      %fc.bias : Float(1000, strides=[1], requires_grad=1, device=cpu),\n",
      "      %onnx::Conv_193 : Float(64, 3, 7, 7, strides=[147, 49, 7, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_194 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_196 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_199 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_202 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_208 : Float(128, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_209 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_211 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_214 : Float(128, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_217 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_223 : Float(256, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_224 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_226 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_229 : Float(256, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_232 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_238 : Float(512, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_239 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_241 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_244 : Float(512, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_247 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu)):\n",
      "  %onnx::Conv_251 : Float(512, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_239)\n",
      "  %onnx::Conv_250 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_241)\n",
      "  %onnx::Conv_248 : Float(512, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_239)\n",
      "  %onnx::Conv_245 : Float(512, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_239)\n",
      "  %onnx::Conv_242 : Float(512, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_239)\n",
      "  %onnx::Conv_236 : Float(256, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_224)\n",
      "  %onnx::Conv_235 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_226)\n",
      "  %onnx::Conv_233 : Float(256, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_224)\n",
      "  %onnx::Conv_230 : Float(256, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_224)\n",
      "  %onnx::Conv_227 : Float(256, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_224)\n",
      "  %onnx::Conv_221 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_209)\n",
      "  %onnx::Conv_220 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_211)\n",
      "  %onnx::Conv_218 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_209)\n",
      "  %onnx::Conv_215 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_209)\n",
      "  %onnx::Conv_212 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_209)\n",
      "  %onnx::Conv_206 : Float(64, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_194)\n",
      "  %onnx::Conv_205 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_199)\n",
      "  %onnx::Conv_203 : Float(64, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_194)\n",
      "  %onnx::Conv_200 : Float(64, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_194)\n",
      "  %onnx::Conv_197 : Float(64, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_194)\n",
      "  %/conv1/Conv_output_0 : Float(*, 64, 112, 112, strides=[802816, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[7, 7], pads=[3, 3, 3, 3], strides=[2, 2], onnx_name=\"/conv1/Conv\"](%x, %onnx::Conv_193, %onnx::Conv_194), scope: timm.models.resnet.ResNet::/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/act1/Relu_output_0 : Float(*, 64, 112, 112, strides=[802816, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/act1/Relu\"](%/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/maxpool/MaxPool_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/maxpool/MaxPool\"](%/act1/Relu_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.pooling.MaxPool2d::maxpool # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:782:0\n",
      "  %/layer1/layer1.0/conv1/Conv_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer1/layer1.0/conv1/Conv\"](%/maxpool/MaxPool_output_0, %onnx::Conv_196, %onnx::Conv_197), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.0/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer1/layer1.0/act1/Relu_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer1/layer1.0/act1/Relu\"](%/layer1/layer1.0/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.0/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer1/layer1.0/conv2/Conv_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer1/layer1.0/conv2/Conv\"](%/layer1/layer1.0/act1/Relu_output_0, %onnx::Conv_199, %onnx::Conv_200), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.0/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer1/layer1.0/Add_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer1/layer1.0/Add\"](%/layer1/layer1.0/conv2/Conv_output_0, %/maxpool/MaxPool_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.0 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer1/layer1.0/act2/Relu_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer1/layer1.0/act2/Relu\"](%/layer1/layer1.0/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.0/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer1/layer1.1/conv1/Conv_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer1/layer1.1/conv1/Conv\"](%/layer1/layer1.0/act2/Relu_output_0, %onnx::Conv_202, %onnx::Conv_203), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.1/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer1/layer1.1/act1/Relu_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer1/layer1.1/act1/Relu\"](%/layer1/layer1.1/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.1/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer1/layer1.1/conv2/Conv_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer1/layer1.1/conv2/Conv\"](%/layer1/layer1.1/act1/Relu_output_0, %onnx::Conv_205, %onnx::Conv_206), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.1/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer1/layer1.1/Add_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer1/layer1.1/Add\"](%/layer1/layer1.1/conv2/Conv_output_0, %/layer1/layer1.0/act2/Relu_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.1 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer1/layer1.1/act2/Relu_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer1/layer1.1/act2/Relu\"](%/layer1/layer1.1/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.1/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer2/layer2.0/conv1/Conv_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/layer2/layer2.0/conv1/Conv\"](%/layer1/layer1.1/act2/Relu_output_0, %onnx::Conv_208, %onnx::Conv_209), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer2/layer2.0/act1/Relu_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer2/layer2.0/act1/Relu\"](%/layer2/layer2.0/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer2/layer2.0/conv2/Conv_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer2/layer2.0/conv2/Conv\"](%/layer2/layer2.0/act1/Relu_output_0, %onnx::Conv_211, %onnx::Conv_212), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer2/layer2.0/downsample/downsample.0/Conv_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/layer2/layer2.0/downsample/downsample.0/Conv\"](%/layer1/layer1.1/act2/Relu_output_0, %onnx::Conv_214, %onnx::Conv_215), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0/torch.nn.modules.container.Sequential::downsample/torch.nn.modules.conv.Conv2d::downsample.0 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer2/layer2.0/Add_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer2/layer2.0/Add\"](%/layer2/layer2.0/conv2/Conv_output_0, %/layer2/layer2.0/downsample/downsample.0/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer2/layer2.0/act2/Relu_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer2/layer2.0/act2/Relu\"](%/layer2/layer2.0/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer2/layer2.1/conv1/Conv_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer2/layer2.1/conv1/Conv\"](%/layer2/layer2.0/act2/Relu_output_0, %onnx::Conv_217, %onnx::Conv_218), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.1/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer2/layer2.1/act1/Relu_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer2/layer2.1/act1/Relu\"](%/layer2/layer2.1/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.1/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer2/layer2.1/conv2/Conv_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer2/layer2.1/conv2/Conv\"](%/layer2/layer2.1/act1/Relu_output_0, %onnx::Conv_220, %onnx::Conv_221), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.1/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer2/layer2.1/Add_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer2/layer2.1/Add\"](%/layer2/layer2.1/conv2/Conv_output_0, %/layer2/layer2.0/act2/Relu_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.1 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer2/layer2.1/act2/Relu_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer2/layer2.1/act2/Relu\"](%/layer2/layer2.1/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.1/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer3/layer3.0/conv1/Conv_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/layer3/layer3.0/conv1/Conv\"](%/layer2/layer2.1/act2/Relu_output_0, %onnx::Conv_223, %onnx::Conv_224), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer3/layer3.0/act1/Relu_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer3/layer3.0/act1/Relu\"](%/layer3/layer3.0/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer3/layer3.0/conv2/Conv_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer3/layer3.0/conv2/Conv\"](%/layer3/layer3.0/act1/Relu_output_0, %onnx::Conv_226, %onnx::Conv_227), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer3/layer3.0/downsample/downsample.0/Conv_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/layer3/layer3.0/downsample/downsample.0/Conv\"](%/layer2/layer2.1/act2/Relu_output_0, %onnx::Conv_229, %onnx::Conv_230), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0/torch.nn.modules.container.Sequential::downsample/torch.nn.modules.conv.Conv2d::downsample.0 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer3/layer3.0/Add_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer3/layer3.0/Add\"](%/layer3/layer3.0/conv2/Conv_output_0, %/layer3/layer3.0/downsample/downsample.0/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer3/layer3.0/act2/Relu_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer3/layer3.0/act2/Relu\"](%/layer3/layer3.0/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer3/layer3.1/conv1/Conv_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer3/layer3.1/conv1/Conv\"](%/layer3/layer3.0/act2/Relu_output_0, %onnx::Conv_232, %onnx::Conv_233), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.1/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer3/layer3.1/act1/Relu_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer3/layer3.1/act1/Relu\"](%/layer3/layer3.1/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.1/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer3/layer3.1/conv2/Conv_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer3/layer3.1/conv2/Conv\"](%/layer3/layer3.1/act1/Relu_output_0, %onnx::Conv_235, %onnx::Conv_236), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.1/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer3/layer3.1/Add_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer3/layer3.1/Add\"](%/layer3/layer3.1/conv2/Conv_output_0, %/layer3/layer3.0/act2/Relu_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.1 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer3/layer3.1/act2/Relu_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer3/layer3.1/act2/Relu\"](%/layer3/layer3.1/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.1/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer4/layer4.0/conv1/Conv_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/layer4/layer4.0/conv1/Conv\"](%/layer3/layer3.1/act2/Relu_output_0, %onnx::Conv_238, %onnx::Conv_239), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer4/layer4.0/act1/Relu_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer4/layer4.0/act1/Relu\"](%/layer4/layer4.0/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer4/layer4.0/conv2/Conv_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer4/layer4.0/conv2/Conv\"](%/layer4/layer4.0/act1/Relu_output_0, %onnx::Conv_241, %onnx::Conv_242), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer4/layer4.0/downsample/downsample.0/Conv_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/layer4/layer4.0/downsample/downsample.0/Conv\"](%/layer3/layer3.1/act2/Relu_output_0, %onnx::Conv_244, %onnx::Conv_245), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0/torch.nn.modules.container.Sequential::downsample/torch.nn.modules.conv.Conv2d::downsample.0 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer4/layer4.0/Add_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer4/layer4.0/Add\"](%/layer4/layer4.0/conv2/Conv_output_0, %/layer4/layer4.0/downsample/downsample.0/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer4/layer4.0/act2/Relu_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer4/layer4.0/act2/Relu\"](%/layer4/layer4.0/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer4/layer4.1/conv1/Conv_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer4/layer4.1/conv1/Conv\"](%/layer4/layer4.0/act2/Relu_output_0, %onnx::Conv_247, %onnx::Conv_248), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.1/torch.nn.modules.conv.Conv2d::conv1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer4/layer4.1/act1/Relu_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer4/layer4.1/act1/Relu\"](%/layer4/layer4.1/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.1/torch.nn.modules.activation.ReLU::act1 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer4/layer4.1/conv2/Conv_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer4/layer4.1/conv2/Conv\"](%/layer4/layer4.1/act1/Relu_output_0, %onnx::Conv_250, %onnx::Conv_251), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.1/torch.nn.modules.conv.Conv2d::conv2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer4/layer4.1/Add_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer4/layer4.1/Add\"](%/layer4/layer4.1/conv2/Conv_output_0, %/layer4/layer4.0/act2/Relu_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.1 # /home/pyler/.local/lib/python3.8/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer4/layer4.1/act2/Relu_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer4/layer4.1/act2/Relu\"](%/layer4/layer4.1/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.1/torch.nn.modules.activation.ReLU::act2 # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/global_pool/pool/GlobalAveragePool_output_0 : Float(*, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cpu) = onnx::GlobalAveragePool[onnx_name=\"/global_pool/pool/GlobalAveragePool\"](%/layer4/layer4.1/act2/Relu_output_0), scope: timm.models.resnet.ResNet::/timm.layers.adaptive_avgmax_pool.SelectAdaptivePool2d::global_pool/torch.nn.modules.pooling.AdaptiveAvgPool2d::pool # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/functional.py:1214:0\n",
      "  %/global_pool/flatten/Flatten_output_0 : Float(*, 512, strides=[512, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1, onnx_name=\"/global_pool/flatten/Flatten\"](%/global_pool/pool/GlobalAveragePool_output_0), scope: timm.models.resnet.ResNet::/timm.layers.adaptive_avgmax_pool.SelectAdaptivePool2d::global_pool/torch.nn.modules.flatten.Flatten::flatten # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/flatten.py:46:0\n",
      "  %outputs : Float(*, 1000, strides=[1000, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc/Gemm\"](%/global_pool/flatten/Flatten_output_0, %fc.weight, %fc.bias), scope: timm.models.resnet.ResNet::/torch.nn.modules.linear.Linear::fc # /home/pyler/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  return (%outputs)\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare resnet-18 model\n",
    "import timm\n",
    "import torch\n",
    "import inspect\n",
    "\n",
    "model = timm.create_model(\"resnet18\").cpu()\n",
    "\n",
    "# check model forward's input parameter name\n",
    "IN_SHAPE = (3, 224, 224)\n",
    "OUT_SHAPE = (1000,)\n",
    "signature = inspect.signature(model.forward)\n",
    "print(signature)\n",
    "\n",
    "# export model to onnx format\n",
    "# For more details, please refer to https://pytorch.org/docs/stable/onnx.html\n",
    "dummy_input = (torch.randn(*((1,)+IN_SHAPE)).cpu(),)\n",
    "input_names = [\"x\"]\n",
    "output_names = [\"outputs\"]\n",
    "\n",
    "model = model.eval() # Need to set model to eval\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"resnet18.onnx\",\n",
    "    dynamic_axes={\n",
    "        \"x\" : {0: \"batch\"}, # To support dynamic shape on target axis\n",
    "    },\n",
    "    verbose=True,\n",
    "    input_names=input_names, # Need to be aligned with actual parameter name in forward function\n",
    "    output_names=output_names # Required to match with number of actual output \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize model graph with netron\n",
    "# ! netron resnet18.onnx -b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-26 05:36:56.142927328 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313729, index: 1, mask: {2, 26, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.142923523 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313728, index: 0, mask: {1, 25, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.145980903 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313731, index: 3, mask: {4, 28, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.163261299 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313732, index: 4, mask: {5, 29, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.168303259 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313733, index: 5, mask: {6, 30, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.168540219 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313734, index: 6, mask: {7, 31, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.172562323 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313730, index: 2, mask: {3, 27, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.172604049 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313740, index: 12, mask: {13, 37, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.176574289 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313742, index: 14, mask: {15, 39, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.180568217 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313741, index: 13, mask: {14, 38, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.183461798 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313735, index: 7, mask: {8, 32, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.192556611 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313743, index: 15, mask: {16, 40, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.198633841 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313744, index: 16, mask: {17, 41, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.203708467 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313746, index: 18, mask: {19, 43, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.204542544 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313747, index: 19, mask: {20, 44, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.208534745 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313748, index: 20, mask: {21, 45, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.212532356 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313749, index: 21, mask: {22, 46, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.216548228 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313745, index: 17, mask: {18, 42, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-26 05:36:56.216554129 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 313750, index: 22, mask: {23, 47, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "100%|██████████| 10/10 [00:21<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# check onnx integrity\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from tqdm import tqdm \n",
    "\n",
    "NUM_TEST = 10\n",
    "B = 16\n",
    "\n",
    "onnx_model = onnx.load(\"resnet18.onnx\")\n",
    "sess = ort.InferenceSession(\n",
    "    onnx_model.SerializeToString(), \n",
    "    providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "mean_diff = 0\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(NUM_TEST)):\n",
    "        input_dict = {\"x\" : torch.randn(*((B,) + SHAPE))}\n",
    "        torch_output = model(**input_dict)\n",
    "        onnx_output = sess.run(output_names, {k : v.numpy() for k, v in input_dict.items()})\n",
    "        mean_diff += (torch_output - torch.from_numpy(onnx_output[0])).square().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3039e-16)\n"
     ]
    }
   ],
   "source": [
    "print(mean_diff / NUM_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8601] # /opt/tensorrt/bin/trtexec --onnx=resnet18.onnx --minShapes=x:1x3x224x224 --optShapes=x:16x3x224x224 --maxShapes=x:32x3x224x224 --useCudaGraph --saveEngine=resnet18.plan --verbose=true\n",
      "[07/26/2023-05:37:37] [I] === Model Options ===\n",
      "[07/26/2023-05:37:37] [I] Format: ONNX\n",
      "[07/26/2023-05:37:37] [I] Model: resnet18.onnx\n",
      "[07/26/2023-05:37:37] [I] Output:\n",
      "[07/26/2023-05:37:37] [I] === Build Options ===\n",
      "[07/26/2023-05:37:37] [I] Max batch: explicit batch\n",
      "[07/26/2023-05:37:37] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[07/26/2023-05:37:37] [I] minTiming: 1\n",
      "[07/26/2023-05:37:37] [I] avgTiming: 8\n",
      "[07/26/2023-05:37:37] [I] Precision: FP32\n",
      "[07/26/2023-05:37:37] [I] LayerPrecisions: \n",
      "[07/26/2023-05:37:37] [I] Layer Device Types: \n",
      "[07/26/2023-05:37:37] [I] Calibration: \n",
      "[07/26/2023-05:37:37] [I] Refit: Disabled\n",
      "[07/26/2023-05:37:37] [I] Version Compatible: Disabled\n",
      "[07/26/2023-05:37:37] [I] TensorRT runtime: full\n",
      "[07/26/2023-05:37:37] [I] Lean DLL Path: \n",
      "[07/26/2023-05:37:37] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[07/26/2023-05:37:37] [I] Exclude Lean Runtime: Disabled\n",
      "[07/26/2023-05:37:37] [I] Sparsity: Disabled\n",
      "[07/26/2023-05:37:37] [I] Safe mode: Disabled\n",
      "[07/26/2023-05:37:37] [I] Build DLA standalone loadable: Disabled\n",
      "[07/26/2023-05:37:37] [I] Allow GPU fallback for DLA: Disabled\n",
      "[07/26/2023-05:37:37] [I] DirectIO mode: Disabled\n",
      "[07/26/2023-05:37:37] [I] Restricted mode: Disabled\n",
      "[07/26/2023-05:37:37] [I] Skip inference: Disabled\n",
      "[07/26/2023-05:37:37] [I] Save engine: resnet18.plan\n",
      "[07/26/2023-05:37:37] [I] Load engine: \n",
      "[07/26/2023-05:37:37] [I] Profiling verbosity: 0\n",
      "[07/26/2023-05:37:37] [I] Tactic sources: Using default tactic sources\n",
      "[07/26/2023-05:37:37] [I] timingCacheMode: local\n",
      "[07/26/2023-05:37:37] [I] timingCacheFile: \n",
      "[07/26/2023-05:37:37] [I] Heuristic: Disabled\n",
      "[07/26/2023-05:37:37] [I] Preview Features: Use default preview flags.\n",
      "[07/26/2023-05:37:37] [I] MaxAuxStreams: -1\n",
      "[07/26/2023-05:37:37] [I] BuilderOptimizationLevel: -1\n",
      "[07/26/2023-05:37:37] [I] Input(s)s format: fp32:CHW\n",
      "[07/26/2023-05:37:37] [I] Output(s)s format: fp32:CHW\n",
      "[07/26/2023-05:37:37] [I] Input build shape: x=1x3x224x224+16x3x224x224+32x3x224x224\n",
      "[07/26/2023-05:37:37] [I] Input calibration shapes: model\n",
      "[07/26/2023-05:37:37] [I] === System Options ===\n",
      "[07/26/2023-05:37:37] [I] Device: 0\n",
      "[07/26/2023-05:37:37] [I] DLACore: \n",
      "[07/26/2023-05:37:37] [I] Plugins:\n",
      "[07/26/2023-05:37:37] [I] setPluginsToSerialize:\n",
      "[07/26/2023-05:37:37] [I] dynamicPlugins:\n",
      "[07/26/2023-05:37:37] [I] ignoreParsedPluginLibs: 0\n",
      "[07/26/2023-05:37:37] [I] \n",
      "[07/26/2023-05:37:37] [I] === Inference Options ===\n",
      "[07/26/2023-05:37:37] [I] Batch: Explicit\n",
      "[07/26/2023-05:37:37] [I] Input inference shape: x=16x3x224x224\n",
      "[07/26/2023-05:37:37] [I] Iterations: 10\n",
      "[07/26/2023-05:37:37] [I] Duration: 3s (+ 200ms warm up)\n",
      "[07/26/2023-05:37:37] [I] Sleep time: 0ms\n",
      "[07/26/2023-05:37:37] [I] Idle time: 0ms\n",
      "[07/26/2023-05:37:37] [I] Inference Streams: 1\n",
      "[07/26/2023-05:37:37] [I] ExposeDMA: Disabled\n",
      "[07/26/2023-05:37:37] [I] Data transfers: Enabled\n",
      "[07/26/2023-05:37:37] [I] Spin-wait: Disabled\n",
      "[07/26/2023-05:37:37] [I] Multithreading: Disabled\n",
      "[07/26/2023-05:37:37] [I] CUDA Graph: Enabled\n",
      "[07/26/2023-05:37:37] [I] Separate profiling: Disabled\n",
      "[07/26/2023-05:37:37] [I] Time Deserialize: Disabled\n",
      "[07/26/2023-05:37:37] [I] Time Refit: Disabled\n",
      "[07/26/2023-05:37:37] [I] NVTX verbosity: 0\n",
      "[07/26/2023-05:37:37] [I] Persistent Cache Ratio: 0\n",
      "[07/26/2023-05:37:37] [I] Inputs:\n",
      "[07/26/2023-05:37:37] [I] === Reporting Options ===\n",
      "[07/26/2023-05:37:37] [I] Verbose: Enabled\n",
      "[07/26/2023-05:37:37] [I] Averages: 10 inferences\n",
      "[07/26/2023-05:37:37] [I] Percentiles: 90,95,99\n",
      "[07/26/2023-05:37:37] [I] Dump refittable layers:Disabled\n",
      "[07/26/2023-05:37:37] [I] Dump output: Disabled\n",
      "[07/26/2023-05:37:37] [I] Profile: Disabled\n",
      "[07/26/2023-05:37:37] [I] Export timing to JSON file: \n",
      "[07/26/2023-05:37:37] [I] Export output to JSON file: \n",
      "[07/26/2023-05:37:37] [I] Export profile to JSON file: \n",
      "[07/26/2023-05:37:37] [I] \n",
      "[07/26/2023-05:37:37] [I] === Device Information ===\n",
      "[07/26/2023-05:37:37] [I] Selected Device: NVIDIA GeForce RTX 3060\n",
      "[07/26/2023-05:37:37] [I] Compute Capability: 8.6\n",
      "[07/26/2023-05:37:37] [I] SMs: 28\n",
      "[07/26/2023-05:37:37] [I] Device Global Memory: 12044 MiB\n",
      "[07/26/2023-05:37:37] [I] Shared Memory per SM: 100 KiB\n",
      "[07/26/2023-05:37:37] [I] Memory Bus Width: 192 bits (ECC disabled)\n",
      "[07/26/2023-05:37:37] [I] Application Compute Clock Rate: 1.837 GHz\n",
      "[07/26/2023-05:37:37] [I] Application Memory Clock Rate: 7.501 GHz\n",
      "[07/26/2023-05:37:37] [I] \n",
      "[07/26/2023-05:37:37] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[07/26/2023-05:37:37] [I] \n",
      "[07/26/2023-05:37:37] [I] TensorRT version: 8.6.1\n",
      "[07/26/2023-05:37:37] [I] Loading standard plugins\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::BatchedNMS_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::BatchTilePlugin_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::Clip_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::CoordConvAC version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::CropAndResizeDynamic version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::CropAndResize version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::DecodeBbox3DPlugin version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::DetectionLayer_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::EfficientNMS_Explicit_TF_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::EfficientNMS_Implicit_TF_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::EfficientNMS_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::FlattenConcat_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::GenerateDetection_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::GridAnchor_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::GridAnchorRect_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 2\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::LReLU_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::ModulatedDeformConv2d version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::MultilevelCropAndResize_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::MultilevelProposeROI_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::NMSDynamic_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::NMS_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::Normalize_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::PillarScatterPlugin version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::PriorBox_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::ProposalDynamic version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::ProposalLayer_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::Proposal version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::PyramidROIAlign_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::Region_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::Reorg_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::ResizeNearest_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::ROIAlign_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::RPROI_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::ScatterND version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::SpecialSlice_TRT version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::Split version 1\n",
      "[07/26/2023-05:37:37] [V] [TRT] Registered plugin creator - ::VoxelGeneratorPlugin version 1\n",
      "[07/26/2023-05:37:38] [I] [TRT] [MemUsageChange] Init CUDA: CPU +352, GPU +0, now: CPU 367, GPU 3326 (MiB)\n",
      "[07/26/2023-05:37:38] [V] [TRT] Trying to load shared library libnvinfer_builder_resource.so.8.6.1\n",
      "[07/26/2023-05:37:38] [V] [TRT] Loaded shared library libnvinfer_builder_resource.so.8.6.1\n",
      "[07/26/2023-05:37:48] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +1217, GPU +266, now: CPU 1660, GPU 3592 (MiB)\n",
      "[07/26/2023-05:37:48] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "[07/26/2023-05:37:48] [I] Start parsing network model.\n",
      "[07/26/2023-05:37:48] [I] [TRT] ----------------------------------------------------------------\n",
      "[07/26/2023-05:37:48] [I] [TRT] Input filename:   resnet18.onnx\n",
      "[07/26/2023-05:37:48] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[07/26/2023-05:37:48] [I] [TRT] Opset version:    14\n",
      "[07/26/2023-05:37:48] [I] [TRT] Producer name:    pytorch\n",
      "[07/26/2023-05:37:48] [I] [TRT] Producer version: 2.0.0\n",
      "[07/26/2023-05:37:48] [I] [TRT] Domain:           \n",
      "[07/26/2023-05:37:48] [I] [TRT] Model version:    0\n",
      "[07/26/2023-05:37:48] [I] [TRT] Doc string:       \n",
      "[07/26/2023-05:37:48] [I] [TRT] ----------------------------------------------------------------\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::BatchedNMS_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::BatchTilePlugin_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::Clip_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::CoordConvAC version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::CropAndResizeDynamic version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::CropAndResize version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::DecodeBbox3DPlugin version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::DetectionLayer_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::EfficientNMS_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::FlattenConcat_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::GenerateDetection_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::GridAnchor_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::GridAnchorRect_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::InstanceNormalization_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::InstanceNormalization_TRT version 2\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::LReLU_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::ModulatedDeformConv2d version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::NMSDynamic_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::NMS_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::Normalize_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::PillarScatterPlugin version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::PriorBox_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::ProposalDynamic version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::ProposalLayer_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::Proposal version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::PyramidROIAlign_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::Region_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::Reorg_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::ResizeNearest_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::ROIAlign_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::RPROI_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::ScatterND version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::SpecialSlice_TRT version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::Split version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Plugin creator already registered - ::VoxelGeneratorPlugin version 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Adding network input: x with dtype: float32, dimensions: (-1, 3, 224, 224)\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: x for ONNX tensor: x\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: fc.weight\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: fc.bias\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_193\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_194\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_196\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_199\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_202\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_208\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_209\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_211\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_214\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_217\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_223\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_224\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_226\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_229\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_232\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_238\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_239\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_241\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_244\n",
      "[07/26/2023-05:37:48] [V] [TRT] Importing initializer: onnx::Conv_247\n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_0 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_239\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_0 [Identity] inputs: [onnx::Conv_239 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: onnx::Conv_239 for ONNX node: onnx::Conv_239\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_0 for ONNX node: Identity_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_251 for ONNX tensor: onnx::Conv_251\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_0 [Identity] outputs: [onnx::Conv_251 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_1 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_241\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_1 [Identity] inputs: [onnx::Conv_241 -> (512, 512, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: onnx::Conv_241 for ONNX node: onnx::Conv_241\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_1 for ONNX node: Identity_1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_250 for ONNX tensor: onnx::Conv_250\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_1 [Identity] outputs: [onnx::Conv_250 -> (512, 512, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_2 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_239\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_2 [Identity] inputs: [onnx::Conv_239 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_2 for ONNX node: Identity_2\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_248 for ONNX tensor: onnx::Conv_248\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_2 [Identity] outputs: [onnx::Conv_248 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_3 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_239\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_3 [Identity] inputs: [onnx::Conv_239 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_3 for ONNX node: Identity_3\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_245 for ONNX tensor: onnx::Conv_245\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_3 [Identity] outputs: [onnx::Conv_245 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_4 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_239\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_4 [Identity] inputs: [onnx::Conv_239 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_4 for ONNX node: Identity_4\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_242 for ONNX tensor: onnx::Conv_242\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_4 [Identity] outputs: [onnx::Conv_242 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_5 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_224\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_5 [Identity] inputs: [onnx::Conv_224 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: onnx::Conv_224 for ONNX node: onnx::Conv_224\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_5 for ONNX node: Identity_5\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_236 for ONNX tensor: onnx::Conv_236\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_5 [Identity] outputs: [onnx::Conv_236 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_6 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_226\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_6 [Identity] inputs: [onnx::Conv_226 -> (256, 256, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: onnx::Conv_226 for ONNX node: onnx::Conv_226\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_6 for ONNX node: Identity_6\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_235 for ONNX tensor: onnx::Conv_235\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_6 [Identity] outputs: [onnx::Conv_235 -> (256, 256, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_7 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_224\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_7 [Identity] inputs: [onnx::Conv_224 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_7 for ONNX node: Identity_7\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_233 for ONNX tensor: onnx::Conv_233\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_7 [Identity] outputs: [onnx::Conv_233 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_8 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_224\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_8 [Identity] inputs: [onnx::Conv_224 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_8 for ONNX node: Identity_8\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_230 for ONNX tensor: onnx::Conv_230\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_8 [Identity] outputs: [onnx::Conv_230 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_9 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_224\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_9 [Identity] inputs: [onnx::Conv_224 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_9 for ONNX node: Identity_9\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_227 for ONNX tensor: onnx::Conv_227\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_9 [Identity] outputs: [onnx::Conv_227 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_10 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_209\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_10 [Identity] inputs: [onnx::Conv_209 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: onnx::Conv_209 for ONNX node: onnx::Conv_209\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_10 for ONNX node: Identity_10\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_221 for ONNX tensor: onnx::Conv_221\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_10 [Identity] outputs: [onnx::Conv_221 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_11 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_211\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_11 [Identity] inputs: [onnx::Conv_211 -> (128, 128, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: onnx::Conv_211 for ONNX node: onnx::Conv_211\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_11 for ONNX node: Identity_11\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_220 for ONNX tensor: onnx::Conv_220\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_11 [Identity] outputs: [onnx::Conv_220 -> (128, 128, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_12 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_209\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_12 [Identity] inputs: [onnx::Conv_209 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_12 for ONNX node: Identity_12\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_218 for ONNX tensor: onnx::Conv_218\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_12 [Identity] outputs: [onnx::Conv_218 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_13 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_209\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_13 [Identity] inputs: [onnx::Conv_209 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_13 for ONNX node: Identity_13\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_215 for ONNX tensor: onnx::Conv_215\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_13 [Identity] outputs: [onnx::Conv_215 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_14 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_209\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_14 [Identity] inputs: [onnx::Conv_209 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_14 for ONNX node: Identity_14\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_212 for ONNX tensor: onnx::Conv_212\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_14 [Identity] outputs: [onnx::Conv_212 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_15 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_194\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_15 [Identity] inputs: [onnx::Conv_194 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: onnx::Conv_194 for ONNX node: onnx::Conv_194\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_15 for ONNX node: Identity_15\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_206 for ONNX tensor: onnx::Conv_206\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_15 [Identity] outputs: [onnx::Conv_206 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_16 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_199\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_16 [Identity] inputs: [onnx::Conv_199 -> (64, 64, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: onnx::Conv_199 for ONNX node: onnx::Conv_199\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_16 for ONNX node: Identity_16\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_205 for ONNX tensor: onnx::Conv_205\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_16 [Identity] outputs: [onnx::Conv_205 -> (64, 64, 3, 3)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_17 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_194\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_17 [Identity] inputs: [onnx::Conv_194 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_17 for ONNX node: Identity_17\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_203 for ONNX tensor: onnx::Conv_203\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_17 [Identity] outputs: [onnx::Conv_203 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_18 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_194\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_18 [Identity] inputs: [onnx::Conv_194 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_18 for ONNX node: Identity_18\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_200 for ONNX tensor: onnx::Conv_200\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_18 [Identity] outputs: [onnx::Conv_200 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: Identity_19 [Identity]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_194\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_19 [Identity] inputs: [onnx::Conv_194 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: Identity_19 for ONNX node: Identity_19\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: onnx::Conv_197 for ONNX tensor: onnx::Conv_197\n",
      "[07/26/2023-05:37:48] [V] [TRT] Identity_19 [Identity] outputs: [onnx::Conv_197 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /conv1/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: x\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_193\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_194\n",
      "[07/26/2023-05:37:48] [V] [TRT] /conv1/Conv [Conv] inputs: [x -> (-1, 3, 224, 224)[FLOAT]], [onnx::Conv_193 -> (64, 3, 7, 7)[FLOAT]], [onnx::Conv_194 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Convolution input dimensions: (-1, 3, 224, 224)\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /conv1/Conv for ONNX node: /conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Using kernel: (7, 7), strides: (2, 2), prepadding: (3, 3), postpadding: (3, 3), dilations: (1, 1), numOutputs: 64\n",
      "[07/26/2023-05:37:48] [V] [TRT] Convolution output dimensions: (-1, 64, 112, 112)\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /conv1/Conv_output_0 for ONNX tensor: /conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /conv1/Conv [Conv] outputs: [/conv1/Conv_output_0 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /act1/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /act1/Relu [Relu] inputs: [/conv1/Conv_output_0 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /act1/Relu for ONNX node: /act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /act1/Relu_output_0 for ONNX tensor: /act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /act1/Relu [Relu] outputs: [/act1/Relu_output_0 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /maxpool/MaxPool [MaxPool]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /maxpool/MaxPool [MaxPool] inputs: [/act1/Relu_output_0 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /maxpool/MaxPool for ONNX node: /maxpool/MaxPool\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /maxpool/MaxPool_output_0 for ONNX tensor: /maxpool/MaxPool_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /maxpool/MaxPool [MaxPool] outputs: [/maxpool/MaxPool_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer1/layer1.0/conv1/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /maxpool/MaxPool_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_196\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_197\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.0/conv1/Conv [Conv] inputs: [/maxpool/MaxPool_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_196 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_197 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer1/layer1.0/conv1/Conv for ONNX node: /layer1/layer1.0/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer1/layer1.0/conv1/Conv_output_0 for ONNX tensor: /layer1/layer1.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.0/conv1/Conv [Conv] outputs: [/layer1/layer1.0/conv1/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer1/layer1.0/act1/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer1/layer1.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.0/act1/Relu [Relu] inputs: [/layer1/layer1.0/conv1/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer1/layer1.0/act1/Relu for ONNX node: /layer1/layer1.0/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer1/layer1.0/act1/Relu_output_0 for ONNX tensor: /layer1/layer1.0/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.0/act1/Relu [Relu] outputs: [/layer1/layer1.0/act1/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer1/layer1.0/conv2/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer1/layer1.0/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_199\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_200\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.0/conv2/Conv [Conv] inputs: [/layer1/layer1.0/act1/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_199 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_200 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer1/layer1.0/conv2/Conv for ONNX node: /layer1/layer1.0/conv2/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer1/layer1.0/conv2/Conv_output_0 for ONNX tensor: /layer1/layer1.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.0/conv2/Conv [Conv] outputs: [/layer1/layer1.0/conv2/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer1/layer1.0/Add [Add]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer1/layer1.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /maxpool/MaxPool_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.0/Add [Add] inputs: [/layer1/layer1.0/conv2/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], [/maxpool/MaxPool_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer1/layer1.0/Add for ONNX node: /layer1/layer1.0/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer1/layer1.0/Add_output_0 for ONNX tensor: /layer1/layer1.0/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.0/Add [Add] outputs: [/layer1/layer1.0/Add_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer1/layer1.0/act2/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer1/layer1.0/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.0/act2/Relu [Relu] inputs: [/layer1/layer1.0/Add_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer1/layer1.0/act2/Relu for ONNX node: /layer1/layer1.0/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer1/layer1.0/act2/Relu_output_0 for ONNX tensor: /layer1/layer1.0/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.0/act2/Relu [Relu] outputs: [/layer1/layer1.0/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer1/layer1.1/conv1/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer1/layer1.0/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_202\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_203\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.1/conv1/Conv [Conv] inputs: [/layer1/layer1.0/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_202 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_203 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer1/layer1.1/conv1/Conv for ONNX node: /layer1/layer1.1/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer1/layer1.1/conv1/Conv_output_0 for ONNX tensor: /layer1/layer1.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.1/conv1/Conv [Conv] outputs: [/layer1/layer1.1/conv1/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer1/layer1.1/act1/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer1/layer1.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.1/act1/Relu [Relu] inputs: [/layer1/layer1.1/conv1/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer1/layer1.1/act1/Relu for ONNX node: /layer1/layer1.1/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer1/layer1.1/act1/Relu_output_0 for ONNX tensor: /layer1/layer1.1/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.1/act1/Relu [Relu] outputs: [/layer1/layer1.1/act1/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer1/layer1.1/conv2/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer1/layer1.1/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_205\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_206\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.1/conv2/Conv [Conv] inputs: [/layer1/layer1.1/act1/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_205 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_206 -> (64)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer1/layer1.1/conv2/Conv for ONNX node: /layer1/layer1.1/conv2/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer1/layer1.1/conv2/Conv_output_0 for ONNX tensor: /layer1/layer1.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.1/conv2/Conv [Conv] outputs: [/layer1/layer1.1/conv2/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer1/layer1.1/Add [Add]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer1/layer1.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer1/layer1.0/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.1/Add [Add] inputs: [/layer1/layer1.1/conv2/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], [/layer1/layer1.0/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer1/layer1.1/Add for ONNX node: /layer1/layer1.1/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer1/layer1.1/Add_output_0 for ONNX tensor: /layer1/layer1.1/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.1/Add [Add] outputs: [/layer1/layer1.1/Add_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer1/layer1.1/act2/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer1/layer1.1/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.1/act2/Relu [Relu] inputs: [/layer1/layer1.1/Add_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer1/layer1.1/act2/Relu for ONNX node: /layer1/layer1.1/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer1/layer1.1/act2/Relu_output_0 for ONNX tensor: /layer1/layer1.1/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer1/layer1.1/act2/Relu [Relu] outputs: [/layer1/layer1.1/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer2/layer2.0/conv1/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer1/layer1.1/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_208\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_209\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.0/conv1/Conv [Conv] inputs: [/layer1/layer1.1/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_208 -> (128, 64, 3, 3)[FLOAT]], [onnx::Conv_209 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Convolution input dimensions: (-1, 64, 56, 56)\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer2/layer2.0/conv1/Conv for ONNX node: /layer2/layer2.0/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128\n",
      "[07/26/2023-05:37:48] [V] [TRT] Convolution output dimensions: (-1, 128, 28, 28)\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer2/layer2.0/conv1/Conv_output_0 for ONNX tensor: /layer2/layer2.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.0/conv1/Conv [Conv] outputs: [/layer2/layer2.0/conv1/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer2/layer2.0/act1/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer2/layer2.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.0/act1/Relu [Relu] inputs: [/layer2/layer2.0/conv1/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer2/layer2.0/act1/Relu for ONNX node: /layer2/layer2.0/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer2/layer2.0/act1/Relu_output_0 for ONNX tensor: /layer2/layer2.0/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.0/act1/Relu [Relu] outputs: [/layer2/layer2.0/act1/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer2/layer2.0/conv2/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer2/layer2.0/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_211\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_212\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.0/conv2/Conv [Conv] inputs: [/layer2/layer2.0/act1/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_211 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_212 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer2/layer2.0/conv2/Conv for ONNX node: /layer2/layer2.0/conv2/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer2/layer2.0/conv2/Conv_output_0 for ONNX tensor: /layer2/layer2.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.0/conv2/Conv [Conv] outputs: [/layer2/layer2.0/conv2/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer2/layer2.0/downsample/downsample.0/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer1/layer1.1/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_214\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_215\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv [Conv] inputs: [/layer1/layer1.1/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_214 -> (128, 64, 1, 1)[FLOAT]], [onnx::Conv_215 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer2/layer2.0/downsample/downsample.0/Conv for ONNX node: /layer2/layer2.0/downsample/downsample.0/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer2/layer2.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /layer2/layer2.0/downsample/downsample.0/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv [Conv] outputs: [/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer2/layer2.0/Add [Add]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer2/layer2.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer2/layer2.0/downsample/downsample.0/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.0/Add [Add] inputs: [/layer2/layer2.0/conv2/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], [/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer2/layer2.0/Add for ONNX node: /layer2/layer2.0/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer2/layer2.0/Add_output_0 for ONNX tensor: /layer2/layer2.0/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.0/Add [Add] outputs: [/layer2/layer2.0/Add_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer2/layer2.0/act2/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer2/layer2.0/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.0/act2/Relu [Relu] inputs: [/layer2/layer2.0/Add_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer2/layer2.0/act2/Relu for ONNX node: /layer2/layer2.0/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer2/layer2.0/act2/Relu_output_0 for ONNX tensor: /layer2/layer2.0/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.0/act2/Relu [Relu] outputs: [/layer2/layer2.0/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer2/layer2.1/conv1/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer2/layer2.0/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_217\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_218\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.1/conv1/Conv [Conv] inputs: [/layer2/layer2.0/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_217 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_218 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer2/layer2.1/conv1/Conv for ONNX node: /layer2/layer2.1/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer2/layer2.1/conv1/Conv_output_0 for ONNX tensor: /layer2/layer2.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.1/conv1/Conv [Conv] outputs: [/layer2/layer2.1/conv1/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer2/layer2.1/act1/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer2/layer2.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.1/act1/Relu [Relu] inputs: [/layer2/layer2.1/conv1/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer2/layer2.1/act1/Relu for ONNX node: /layer2/layer2.1/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer2/layer2.1/act1/Relu_output_0 for ONNX tensor: /layer2/layer2.1/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.1/act1/Relu [Relu] outputs: [/layer2/layer2.1/act1/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer2/layer2.1/conv2/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer2/layer2.1/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_220\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_221\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.1/conv2/Conv [Conv] inputs: [/layer2/layer2.1/act1/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_220 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_221 -> (128)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer2/layer2.1/conv2/Conv for ONNX node: /layer2/layer2.1/conv2/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer2/layer2.1/conv2/Conv_output_0 for ONNX tensor: /layer2/layer2.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.1/conv2/Conv [Conv] outputs: [/layer2/layer2.1/conv2/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer2/layer2.1/Add [Add]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer2/layer2.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer2/layer2.0/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.1/Add [Add] inputs: [/layer2/layer2.1/conv2/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], [/layer2/layer2.0/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer2/layer2.1/Add for ONNX node: /layer2/layer2.1/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer2/layer2.1/Add_output_0 for ONNX tensor: /layer2/layer2.1/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.1/Add [Add] outputs: [/layer2/layer2.1/Add_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer2/layer2.1/act2/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer2/layer2.1/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.1/act2/Relu [Relu] inputs: [/layer2/layer2.1/Add_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer2/layer2.1/act2/Relu for ONNX node: /layer2/layer2.1/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer2/layer2.1/act2/Relu_output_0 for ONNX tensor: /layer2/layer2.1/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer2/layer2.1/act2/Relu [Relu] outputs: [/layer2/layer2.1/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer3/layer3.0/conv1/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer2/layer2.1/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_223\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_224\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.0/conv1/Conv [Conv] inputs: [/layer2/layer2.1/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_223 -> (256, 128, 3, 3)[FLOAT]], [onnx::Conv_224 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Convolution input dimensions: (-1, 128, 28, 28)\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer3/layer3.0/conv1/Conv for ONNX node: /layer3/layer3.0/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256\n",
      "[07/26/2023-05:37:48] [V] [TRT] Convolution output dimensions: (-1, 256, 14, 14)\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer3/layer3.0/conv1/Conv_output_0 for ONNX tensor: /layer3/layer3.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.0/conv1/Conv [Conv] outputs: [/layer3/layer3.0/conv1/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer3/layer3.0/act1/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer3/layer3.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.0/act1/Relu [Relu] inputs: [/layer3/layer3.0/conv1/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer3/layer3.0/act1/Relu for ONNX node: /layer3/layer3.0/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer3/layer3.0/act1/Relu_output_0 for ONNX tensor: /layer3/layer3.0/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.0/act1/Relu [Relu] outputs: [/layer3/layer3.0/act1/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer3/layer3.0/conv2/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer3/layer3.0/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_226\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_227\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.0/conv2/Conv [Conv] inputs: [/layer3/layer3.0/act1/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_226 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_227 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer3/layer3.0/conv2/Conv for ONNX node: /layer3/layer3.0/conv2/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer3/layer3.0/conv2/Conv_output_0 for ONNX tensor: /layer3/layer3.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.0/conv2/Conv [Conv] outputs: [/layer3/layer3.0/conv2/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer3/layer3.0/downsample/downsample.0/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer2/layer2.1/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_229\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_230\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv [Conv] inputs: [/layer2/layer2.1/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_229 -> (256, 128, 1, 1)[FLOAT]], [onnx::Conv_230 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer3/layer3.0/downsample/downsample.0/Conv for ONNX node: /layer3/layer3.0/downsample/downsample.0/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer3/layer3.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /layer3/layer3.0/downsample/downsample.0/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv [Conv] outputs: [/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer3/layer3.0/Add [Add]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer3/layer3.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer3/layer3.0/downsample/downsample.0/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.0/Add [Add] inputs: [/layer3/layer3.0/conv2/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], [/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer3/layer3.0/Add for ONNX node: /layer3/layer3.0/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer3/layer3.0/Add_output_0 for ONNX tensor: /layer3/layer3.0/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.0/Add [Add] outputs: [/layer3/layer3.0/Add_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer3/layer3.0/act2/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer3/layer3.0/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.0/act2/Relu [Relu] inputs: [/layer3/layer3.0/Add_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer3/layer3.0/act2/Relu for ONNX node: /layer3/layer3.0/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer3/layer3.0/act2/Relu_output_0 for ONNX tensor: /layer3/layer3.0/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.0/act2/Relu [Relu] outputs: [/layer3/layer3.0/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer3/layer3.1/conv1/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer3/layer3.0/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_232\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_233\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.1/conv1/Conv [Conv] inputs: [/layer3/layer3.0/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_232 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_233 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer3/layer3.1/conv1/Conv for ONNX node: /layer3/layer3.1/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer3/layer3.1/conv1/Conv_output_0 for ONNX tensor: /layer3/layer3.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.1/conv1/Conv [Conv] outputs: [/layer3/layer3.1/conv1/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer3/layer3.1/act1/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer3/layer3.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.1/act1/Relu [Relu] inputs: [/layer3/layer3.1/conv1/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer3/layer3.1/act1/Relu for ONNX node: /layer3/layer3.1/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer3/layer3.1/act1/Relu_output_0 for ONNX tensor: /layer3/layer3.1/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.1/act1/Relu [Relu] outputs: [/layer3/layer3.1/act1/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer3/layer3.1/conv2/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer3/layer3.1/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_235\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_236\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.1/conv2/Conv [Conv] inputs: [/layer3/layer3.1/act1/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_235 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_236 -> (256)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer3/layer3.1/conv2/Conv for ONNX node: /layer3/layer3.1/conv2/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer3/layer3.1/conv2/Conv_output_0 for ONNX tensor: /layer3/layer3.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.1/conv2/Conv [Conv] outputs: [/layer3/layer3.1/conv2/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer3/layer3.1/Add [Add]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer3/layer3.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer3/layer3.0/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.1/Add [Add] inputs: [/layer3/layer3.1/conv2/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], [/layer3/layer3.0/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer3/layer3.1/Add for ONNX node: /layer3/layer3.1/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer3/layer3.1/Add_output_0 for ONNX tensor: /layer3/layer3.1/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.1/Add [Add] outputs: [/layer3/layer3.1/Add_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer3/layer3.1/act2/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer3/layer3.1/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.1/act2/Relu [Relu] inputs: [/layer3/layer3.1/Add_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer3/layer3.1/act2/Relu for ONNX node: /layer3/layer3.1/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer3/layer3.1/act2/Relu_output_0 for ONNX tensor: /layer3/layer3.1/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer3/layer3.1/act2/Relu [Relu] outputs: [/layer3/layer3.1/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer4/layer4.0/conv1/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer3/layer3.1/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_238\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_239\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.0/conv1/Conv [Conv] inputs: [/layer3/layer3.1/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_238 -> (512, 256, 3, 3)[FLOAT]], [onnx::Conv_239 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Convolution input dimensions: (-1, 256, 14, 14)\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer4/layer4.0/conv1/Conv for ONNX node: /layer4/layer4.0/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 512\n",
      "[07/26/2023-05:37:48] [V] [TRT] Convolution output dimensions: (-1, 512, 7, 7)\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer4/layer4.0/conv1/Conv_output_0 for ONNX tensor: /layer4/layer4.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.0/conv1/Conv [Conv] outputs: [/layer4/layer4.0/conv1/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer4/layer4.0/act1/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer4/layer4.0/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.0/act1/Relu [Relu] inputs: [/layer4/layer4.0/conv1/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer4/layer4.0/act1/Relu for ONNX node: /layer4/layer4.0/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer4/layer4.0/act1/Relu_output_0 for ONNX tensor: /layer4/layer4.0/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.0/act1/Relu [Relu] outputs: [/layer4/layer4.0/act1/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer4/layer4.0/conv2/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer4/layer4.0/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_241\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_242\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.0/conv2/Conv [Conv] inputs: [/layer4/layer4.0/act1/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], [onnx::Conv_241 -> (512, 512, 3, 3)[FLOAT]], [onnx::Conv_242 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer4/layer4.0/conv2/Conv for ONNX node: /layer4/layer4.0/conv2/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer4/layer4.0/conv2/Conv_output_0 for ONNX tensor: /layer4/layer4.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.0/conv2/Conv [Conv] outputs: [/layer4/layer4.0/conv2/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer4/layer4.0/downsample/downsample.0/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer3/layer3.1/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_244\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_245\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv [Conv] inputs: [/layer3/layer3.1/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_244 -> (512, 256, 1, 1)[FLOAT]], [onnx::Conv_245 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer4/layer4.0/downsample/downsample.0/Conv for ONNX node: /layer4/layer4.0/downsample/downsample.0/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer4/layer4.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /layer4/layer4.0/downsample/downsample.0/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv [Conv] outputs: [/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer4/layer4.0/Add [Add]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer4/layer4.0/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer4/layer4.0/downsample/downsample.0/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.0/Add [Add] inputs: [/layer4/layer4.0/conv2/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], [/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer4/layer4.0/Add for ONNX node: /layer4/layer4.0/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer4/layer4.0/Add_output_0 for ONNX tensor: /layer4/layer4.0/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.0/Add [Add] outputs: [/layer4/layer4.0/Add_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer4/layer4.0/act2/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer4/layer4.0/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.0/act2/Relu [Relu] inputs: [/layer4/layer4.0/Add_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer4/layer4.0/act2/Relu for ONNX node: /layer4/layer4.0/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer4/layer4.0/act2/Relu_output_0 for ONNX tensor: /layer4/layer4.0/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.0/act2/Relu [Relu] outputs: [/layer4/layer4.0/act2/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer4/layer4.1/conv1/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer4/layer4.0/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_247\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_248\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.1/conv1/Conv [Conv] inputs: [/layer4/layer4.0/act2/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], [onnx::Conv_247 -> (512, 512, 3, 3)[FLOAT]], [onnx::Conv_248 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer4/layer4.1/conv1/Conv for ONNX node: /layer4/layer4.1/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer4/layer4.1/conv1/Conv_output_0 for ONNX tensor: /layer4/layer4.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.1/conv1/Conv [Conv] outputs: [/layer4/layer4.1/conv1/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer4/layer4.1/act1/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer4/layer4.1/conv1/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.1/act1/Relu [Relu] inputs: [/layer4/layer4.1/conv1/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer4/layer4.1/act1/Relu for ONNX node: /layer4/layer4.1/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer4/layer4.1/act1/Relu_output_0 for ONNX tensor: /layer4/layer4.1/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.1/act1/Relu [Relu] outputs: [/layer4/layer4.1/act1/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer4/layer4.1/conv2/Conv [Conv]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer4/layer4.1/act1/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_250\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: onnx::Conv_251\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.1/conv2/Conv [Conv] inputs: [/layer4/layer4.1/act1/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], [onnx::Conv_250 -> (512, 512, 3, 3)[FLOAT]], [onnx::Conv_251 -> (512)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer4/layer4.1/conv2/Conv for ONNX node: /layer4/layer4.1/conv2/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer4/layer4.1/conv2/Conv_output_0 for ONNX tensor: /layer4/layer4.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.1/conv2/Conv [Conv] outputs: [/layer4/layer4.1/conv2/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer4/layer4.1/Add [Add]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer4/layer4.1/conv2/Conv_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer4/layer4.0/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.1/Add [Add] inputs: [/layer4/layer4.1/conv2/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], [/layer4/layer4.0/act2/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer4/layer4.1/Add for ONNX node: /layer4/layer4.1/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer4/layer4.1/Add_output_0 for ONNX tensor: /layer4/layer4.1/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.1/Add [Add] outputs: [/layer4/layer4.1/Add_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /layer4/layer4.1/act2/Relu [Relu]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer4/layer4.1/Add_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.1/act2/Relu [Relu] inputs: [/layer4/layer4.1/Add_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /layer4/layer4.1/act2/Relu for ONNX node: /layer4/layer4.1/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /layer4/layer4.1/act2/Relu_output_0 for ONNX tensor: /layer4/layer4.1/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /layer4/layer4.1/act2/Relu [Relu] outputs: [/layer4/layer4.1/act2/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /global_pool/pool/GlobalAveragePool [GlobalAveragePool]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /layer4/layer4.1/act2/Relu_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /global_pool/pool/GlobalAveragePool [GlobalAveragePool] inputs: [/layer4/layer4.1/act2/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] GlobalAveragePool operators are implemented via Reduce layers rather than Pooling layers\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /global_pool/pool/GlobalAveragePool for ONNX node: /global_pool/pool/GlobalAveragePool\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /global_pool/pool/GlobalAveragePool_output_0 for ONNX tensor: /global_pool/pool/GlobalAveragePool_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /global_pool/pool/GlobalAveragePool [GlobalAveragePool] outputs: [/global_pool/pool/GlobalAveragePool_output_0 -> (-1, 512, 1, 1)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /global_pool/flatten/Flatten [Flatten]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /global_pool/pool/GlobalAveragePool_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /global_pool/flatten/Flatten [Flatten] inputs: [/global_pool/pool/GlobalAveragePool_output_0 -> (-1, 512, 1, 1)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [W] [TRT] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /global_pool/flatten/Flatten for ONNX node: /global_pool/flatten/Flatten\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: /global_pool/flatten/Flatten_output_0 for ONNX tensor: /global_pool/flatten/Flatten_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] /global_pool/flatten/Flatten [Flatten] outputs: [/global_pool/flatten/Flatten_output_0 -> (-1, 512)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Parsing node: /fc/Gemm [Gemm]\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: /global_pool/flatten/Flatten_output_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: fc.weight\n",
      "[07/26/2023-05:37:48] [V] [TRT] Searching for input: fc.bias\n",
      "[07/26/2023-05:37:48] [V] [TRT] /fc/Gemm [Gemm] inputs: [/global_pool/flatten/Flatten_output_0 -> (-1, 512)[FLOAT]], [fc.weight -> (1000, 512)[FLOAT]], [fc.bias -> (1000)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: fc.weight for ONNX node: fc.weight\n",
      "[07/26/2023-05:37:48] [V] [TRT] Using opA: 0 opB: 1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: /fc/Gemm for ONNX node: /fc/Gemm\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering layer: fc.bias for ONNX node: fc.bias\n",
      "[07/26/2023-05:37:48] [V] [TRT] Registering tensor: outputs_1 for ONNX tensor: outputs\n",
      "[07/26/2023-05:37:48] [V] [TRT] /fc/Gemm [Gemm] outputs: [outputs -> (-1, 1000)[FLOAT]], \n",
      "[07/26/2023-05:37:48] [V] [TRT] Marking outputs_1 as output: outputs\n",
      "[07/26/2023-05:37:48] [I] Finished parsing network model. Parse time: 0.104696\n",
      "[07/26/2023-05:37:48] [V] [TRT] Original: 81 layers\n",
      "[07/26/2023-05:37:48] [V] [TRT] After dead-layer removal: 81 layers\n",
      "[07/26/2023-05:37:48] [V] [TRT] Graph construction completed in 0.00200207 seconds.\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_0 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_2\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_2 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_3\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_3 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_4\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_4 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_1 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_5\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_5 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_7\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_7 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_8\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_8 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_9\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_9 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_6\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_6 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_10\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_10 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_12\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_12 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_13\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_13 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_14\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_14 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_11\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_11 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_15\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_15 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_17\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_17 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_18\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_18 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_19\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_19 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: IdentityToCastTransform on Identity_16\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_16 from IDENTITY to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConstShuffleFusion on fc.bias\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConstShuffleFusion: Fusing fc.bias with (Unnamed Layer* 84) [Shuffle]\n",
      "[07/26/2023-05:37:48] [V] [TRT] After Myelin optimization: 80 layers\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: MatMulToConvTransform on /fc/Gemm\n",
      "[07/26/2023-05:37:48] [V] [TRT] Convert layer type of /fc/Gemm from MATRIX_MULTIPLY to CONVOLUTION\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ShuffleShuffleFusion on /global_pool/flatten/Flatten\n",
      "[07/26/2023-05:37:48] [V] [TRT] ShuffleShuffleFusion: Fusing /global_pool/flatten/Flatten with reshape_before_/fc/Gemm\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ShuffleErasure on /global_pool/flatten/Flatten + reshape_before_/fc/Gemm\n",
      "[07/26/2023-05:37:48] [V] [TRT] Removing /global_pool/flatten/Flatten + reshape_before_/fc/Gemm\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReshapeBiasAddFusion on /fc/Gemm\n",
      "[07/26/2023-05:37:48] [V] [TRT] Applying ScaleNodes fusions.\n",
      "[07/26/2023-05:37:48] [V] [TRT] After scale fusion: 77 layers\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_0\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_0 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_2\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_2 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_3\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_3 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_4\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_4 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_1\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_1 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_5\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_5 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_7\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_7 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_8\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_8 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_9\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_9 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_6\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_6 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_10\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_10 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_12\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_12 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_13\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_13 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_14\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_14 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_11\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_11 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_15\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_15 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_17\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_17 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_18\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_18 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_19\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_19 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: CastToCopyTransform on Identity_16\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of Identity_16 from CAST to CAST\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConstantSplit on onnx::Conv_239\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConstantSplit on onnx::Conv_224\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConstantSplit on onnx::Conv_209\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConstantSplit on onnx::Conv_194\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /conv1/Conv with /act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.0/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.0/conv1/Conv with /layer1/layer1.0/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvEltwiseSumFusion on /layer1/layer1.0/conv2/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer1/layer1.0/conv2/Conv with /layer1/layer1.0/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add with /layer1/layer1.0/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.1/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.1/conv1/Conv with /layer1/layer1.1/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvEltwiseSumFusion on /layer1/layer1.1/conv2/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer1/layer1.1/conv2/Conv with /layer1/layer1.1/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add with /layer1/layer1.1/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.0/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.0/conv1/Conv with /layer2/layer2.0/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvEltwiseSumFusion on /layer2/layer2.0/downsample/downsample.0/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer2/layer2.0/downsample/downsample.0/Conv with /layer2/layer2.0/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add with /layer2/layer2.0/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.1/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.1/conv1/Conv with /layer2/layer2.1/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvEltwiseSumFusion on /layer2/layer2.1/conv2/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer2/layer2.1/conv2/Conv with /layer2/layer2.1/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add with /layer2/layer2.1/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.0/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.0/conv1/Conv with /layer3/layer3.0/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvEltwiseSumFusion on /layer3/layer3.0/downsample/downsample.0/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer3/layer3.0/downsample/downsample.0/Conv with /layer3/layer3.0/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add with /layer3/layer3.0/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.1/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.1/conv1/Conv with /layer3/layer3.1/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvEltwiseSumFusion on /layer3/layer3.1/conv2/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer3/layer3.1/conv2/Conv with /layer3/layer3.1/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add with /layer3/layer3.1/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.0/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.0/conv1/Conv with /layer4/layer4.0/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvEltwiseSumFusion on /layer4/layer4.0/downsample/downsample.0/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer4/layer4.0/downsample/downsample.0/Conv with /layer4/layer4.0/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add with /layer4/layer4.0/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.1/conv1/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.1/conv1/Conv with /layer4/layer4.1/act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvEltwiseSumFusion on /layer4/layer4.1/conv2/Conv\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer4/layer4.1/conv2/Conv with /layer4/layer4.1/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add\n",
      "[07/26/2023-05:37:48] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add with /layer4/layer4.1/act2/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] Running: ReduceToPoolingFusion on /global_pool/pool/GlobalAveragePool\n",
      "[07/26/2023-05:37:48] [V] [TRT] Swap the layer type of /global_pool/pool/GlobalAveragePool from REDUCE to POOLING\n",
      "[07/26/2023-05:37:48] [V] [TRT] After dupe layer removal: 64 layers\n",
      "[07/26/2023-05:37:48] [V] [TRT] After final dead-layer removal: 64 layers\n",
      "[07/26/2023-05:37:48] [V] [TRT] After tensor merging: 64 layers\n",
      "[07/26/2023-05:37:48] [V] [TRT] After vertical fusions: 64 layers\n",
      "[07/26/2023-05:37:48] [V] [TRT] After dupe layer removal: 64 layers\n",
      "[07/26/2023-05:37:48] [V] [TRT] After final dead-layer removal: 64 layers\n",
      "[07/26/2023-05:37:48] [V] [TRT] After tensor merging: 64 layers\n",
      "[07/26/2023-05:37:48] [V] [TRT] After slice removal: 64 layers\n",
      "[07/26/2023-05:37:48] [V] [TRT] After concat removal: 64 layers\n",
      "[07/26/2023-05:37:48] [V] [TRT] Trying to split Reshape and strided tensor\n",
      "[07/26/2023-05:37:48] [I] [TRT] Graph optimization time: 0.0177138 seconds.\n",
      "[07/26/2023-05:37:48] [V] [TRT] Building graph using backend strategy 2\n",
      "[07/26/2023-05:37:48] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[07/26/2023-05:37:48] [V] [TRT] Constructing optimization profile number 0 [1/1].\n",
      "[07/26/2023-05:37:48] [V] [TRT] Applying generic optimizations to the graph for inference.\n",
      "[07/26/2023-05:37:48] [V] [TRT] Reserving memory for host IO tensors. Host: 0 bytes\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_239\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_239_clone_1\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_239_clone_2\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_239_clone_3\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_241\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(4608,9,3,1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_224\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_224_clone_1\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_224_clone_2\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_224_clone_3\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_226\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(2304,9,3,1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_209\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_209_clone_1\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_209_clone_2\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_209_clone_3\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_211\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1152,9,3,1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_194\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_194_clone_1\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_194_clone_2\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_194_clone_3\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for onnx::Conv_199\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination:  -> Float(576,9,3,1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] =============== Computing costs for /conv1/Conv + /act1/Relu\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination: Float(150528,50176,224,1) -> Float(802816,12544,112,1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 1.07886\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.596114\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.785243\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 1.1226\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.799159\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 1.1434\n",
      "[07/26/2023-05:37:48] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.65581\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.789943\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.728832\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.597138\n",
      "[07/26/2023-05:37:48] [V] [TRT] /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0949502 seconds. Fastest Tactic: 0x5deb29b7a8e275f7 Time: 0.596114\n",
      "[07/26/2023-05:37:48] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:48] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:48] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CudnnConvolution[0x80000000])\n",
      "[07/26/2023-05:37:48] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:48] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5deb29b7a8e275f7\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination: Float(150528,1,672,3) -> Float(802816,1,7168,64) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 1.50455\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.804434\n",
      "[07/26/2023-05:37:48] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 2.61427\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.559397\n",
      "[07/26/2023-05:37:48] [V] [TRT] Fast skip Tactic:0x0a143be7a52f301a which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 2.23846\n",
      "[07/26/2023-05:37:48] [V] [TRT] Fast skip Tactic:0xa6448a1e79f1ca6f which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 1.77376\n",
      "[07/26/2023-05:37:48] [V] [TRT] /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.046467 seconds. Fastest Tactic: 0xf231cca3335919a4 Time: 0.559397\n",
      "[07/26/2023-05:37:48] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:48] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:48] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xf231cca3335919a4\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,224,1) -> Float(802816,12544,112,1) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 3.60872\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 3.49886\n",
      "[07/26/2023-05:37:48] [V] [TRT] /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0673279 seconds. Fastest Tactic: 0xe0a307ffe0ffb6a5 Time: 3.49886\n",
      "[07/26/2023-05:37:48] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:48] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:48] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xe0a307ffe0ffb6a5\n",
      "[07/26/2023-05:37:48] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,224,1) -> Float(200704,1:4,1792,16) ***************\n",
      "[07/26/2023-05:37:48] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 2.45263\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 2.77855\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.655799\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.713435\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.560151\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.848311\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.59584\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.677595\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.940818\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.683447\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.661856\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.538624\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.49035\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.483328\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.62976\n",
      "[07/26/2023-05:37:48] [V] [TRT] Fast skip Tactic:0x1acd4f006848c62b which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 1.0199\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.660334\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.586633\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.634441\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.669262\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.629614\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.590848\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.564078\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.598455\n",
      "[07/26/2023-05:37:48] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.655067\n",
      "[07/26/2023-05:37:49] [V] [TRT] Fast skip Tactic:0x65e41d81f093b482 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 3.26758\n",
      "[07/26/2023-05:37:49] [V] [TRT] Fast skip Tactic:0x22cadc265a3b2e32 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 1.16838\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.727099\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.582075\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.586752\n",
      "[07/26/2023-05:37:49] [V] [TRT] /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.257727 seconds. Fastest Tactic: 0x9cb304e2edbc1221 Time: 0.483328\n",
      "[07/26/2023-05:37:49] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:49] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:49] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9cb304e2edbc1221\n",
      "[07/26/2023-05:37:49] [V] [TRT] =============== Computing costs for /maxpool/MaxPool\n",
      "[07/26/2023-05:37:49] [V] [TRT] *************** Autotuning format combination: Float(802816,12544,112,1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:37:49] [V] [TRT] --------------- Timing Runner: /maxpool/MaxPool (CudnnPooling[0x80000005])\n",
      "[07/26/2023-05:37:49] [V] [TRT] CudnnPooling has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:49] [V] [TRT] --------------- Timing Runner: /maxpool/MaxPool (CaskPooling[0x8000002f])\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads855 Tactic: 0xf86a4e1f189f4821 Time: 0.515511\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads1017 Tactic: 0x7b9e5e445528b90a Time: 0.21621\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll2_tThreads841 Tactic: 0xdcecadaa3ad74a2c Time: 0.336018\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll5_tThreads841 Tactic: 0x7bd883ae684d33e0 Time: 0.272384\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads791 Tactic: 0xa23e43dae6aa4fa6 Time: 0.251282\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads855 Tactic: 0x76d52bcd240dc832 Time: 0.240201\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_nd_NCDHW_kMAX_kGENERIC_3D_POOLING_MODE_kFLOAT_0 Tactic: 0x5faf4a0a8a5670ed Time: 0.193563\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll6_tThreads225 Tactic: 0xdb90d0acdc9fc4e1 Time: 0.440174\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads1017 Tactic: 0x4cf88ed475f74f6e Time: 0.459721\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads791 Tactic: 0x8bb5080c88a2b679 Time: 0.226011\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads513 Tactic: 0xcb3875826530ea38 Time: 0.244297\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll4_tThreads225 Tactic: 0xf21b9b7ab2973d3e Time: 0.418144\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_custom_tP4_tQ32_tRS3_tUV2 Tactic: 0x2639d3932b27ac67 Time: 0.193829\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll4_tThreads255 Tactic: 0xfcb5fcaa68fff7ac Time: 0.350208\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads855 Tactic: 0x5f5e601b4a0531ed Time: 0.230693\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll1_tThreads255 Tactic: 0x720a9978546d77bf Time: 0.362642\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads1017 Tactic: 0x7647ea605d1f4493 Time: 0.222939\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads855 Tactic: 0xd1e105c97697b1fe Time: 0.265655\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll1_tThreads225 Tactic: 0x7ca4fea88e05bd2d Time: 0.432567\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll1_tThreads841 Tactic: 0x28ce1402b45cc05e Time: 0.525312\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll6_tThreads255 Tactic: 0xd53eb77c06f70e73 Time: 0.366592\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll3_tThreads225 Tactic: 0x552fb57ee00d44f2 Time: 0.414427\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll6_tThreads841 Tactic: 0x8ffa3a06e6c6b992 Time: 0.270446\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads1017 Tactic: 0x6df482284d70bfa1 Time: 0.215054\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads513 Tactic: 0x169187fc85b39995 Time: 0.270775\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll5_tThreads255 Tactic: 0x211c0ed4887c8401 Time: 0.356059\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads1017 Tactic: 0x5a9252b86daf49c5 Time: 0.283648\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll3_tThreads841 Tactic: 0x01455fd4da543981 Time: 0.290638\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Max Tactic: 0xb59f9cfb90407c92 Time: 0.193961\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads513 Tactic: 0xe2b33e540b3813e7 Time: 0.412768\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads513 Tactic: 0xb1a5a9f8d729e059 Time: 0.237275\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads791 Tactic: 0xd8a39fa054b345c7 Time: 0.346697\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll2_tThreads255 Tactic: 0x862820d0dae6fdcd Time: 0.346405\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads791 Tactic: 0x2c812608da38cfb5 Time: 0.580462\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads513 Tactic: 0x6c0c5b8637aa93f4 Time: 0.239031\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads791 Tactic: 0x7f97b1a406293c0b Time: 0.237422\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads855 Tactic: 0xab7cd9b3c48ebb9f Time: 0.233033\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads513 Tactic: 0x4587105059a26a2b Time: 0.236283\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll2_tThreads225 Tactic: 0x88864700008e375f Time: 0.413257\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads1017 Tactic: 0x574be69c6598b45c Time: 0.245614\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads791 Tactic: 0x050a6ddeb430366a Time: 0.28203\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads855 Tactic: 0x0c48f7b79614c253 Time: 0.316978\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll3_tThreads255 Tactic: 0x5b81d2ae3a658e60 Time: 0.347246\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll5_tThreads225 Tactic: 0x2fb2690452144e93 Time: 0.425691\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll4_tThreads841 Tactic: 0xa67171d088ce404d Time: 0.275689\n",
      "[07/26/2023-05:37:49] [V] [TRT] /maxpool/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.17892 seconds. Fastest Tactic: 0x5faf4a0a8a5670ed Time: 0.193563\n",
      "[07/26/2023-05:37:49] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x5faf4a0a8a5670ed\n",
      "[07/26/2023-05:37:49] [V] [TRT] *************** Autotuning format combination: Float(200704,1:4,1792,16) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:37:49] [V] [TRT] --------------- Timing Runner: /maxpool/MaxPool (CaskPooling[0x8000002f])\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_NOT_PROPAGATE_NAN_3D Tactic: 0xfa211b1cdd504de0 Time: 0.2048\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_PROPAGATE_NAN_3D Tactic: 0xe9d01a2a900075cb Time: 0.248247\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_NOT_PROPAGATE_NAN_2D Tactic: 0xaec8628e8180bced Time: 0.203776\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_PROPAGATE_NAN_3D Tactic: 0xd76bac5638836f8a Time: 0.220599\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_PROPAGATE_NAN_2D Tactic: 0x8382d5c464539e87 Time: 0.215278\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NHWC_Max_CAlign4 Tactic: 0x22fb1bb4a70e340d Time: 0.234203\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_NOT_PROPAGATE_NAN_3D Tactic: 0x2c7251cbae30cf74 Time: 0.233033\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_NOT_PROPAGATE_NAN_2D Tactic: 0x789b2859f2e03e79 Time: 0.19339\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_PROPAGATE_NAN_2D Tactic: 0xbd3963b8ccd084c6 Time: 0.193682\n",
      "[07/26/2023-05:37:49] [V] [TRT] /maxpool/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.0266715 seconds. Fastest Tactic: 0x789b2859f2e03e79 Time: 0.19339\n",
      "[07/26/2023-05:37:49] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x789b2859f2e03e79\n",
      "[07/26/2023-05:37:49] [V] [TRT] =============== Computing costs for /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu\n",
      "[07/26/2023-05:37:49] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:37:49] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.857381\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.464603\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.481317\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.847872\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.56715\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.885175\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.596553\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.423205\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.909166\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.864841\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.461678\n",
      "[07/26/2023-05:37:49] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.32506\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.868645\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.721774\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.618181\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.553303\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.912384\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.816274\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.513445\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.424667\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.379758\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.56715\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.511269\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.486839\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.495177\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.715922\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.417943\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.322734\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.500882\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.788187\n",
      "[07/26/2023-05:37:49] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.235472 seconds. Fastest Tactic: 0x0190806602534cfd Time: 0.322734\n",
      "[07/26/2023-05:37:49] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:49] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:49] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0190806602534cfd\n",
      "[07/26/2023-05:37:49] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:37:49] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 1.52795\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.525166\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.911653\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.440466\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.787895\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.782921\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.461385\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.439589\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.989477\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.8448\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.611913\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.845093\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.895561\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.88896\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.504101\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.66048\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.498542\n",
      "[07/26/2023-05:37:49] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.145089 seconds. Fastest Tactic: 0x1da91d865428f237 Time: 0.439589\n",
      "[07/26/2023-05:37:49] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:49] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:49] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1da91d865428f237\n",
      "[07/26/2023-05:37:49] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:37:49] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.643218\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.680814\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.651118\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.633417\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.609719\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.607232\n",
      "[07/26/2023-05:37:49] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0451059 seconds. Fastest Tactic: 0x3104d85fecdd547c Time: 0.607232\n",
      "[07/26/2023-05:37:49] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:49] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:49] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3104d85fecdd547c\n",
      "[07/26/2023-05:37:49] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:37:49] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.818313\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.815982\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.475136\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.520357\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.488741\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.445001\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.328411\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.868983\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.347136\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.507465\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.895593\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.624494\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.616256\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.360448\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.344795\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.342162\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.602441\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 0.716069\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.60533\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.40773\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.581632\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.484352\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.502789\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.400823\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.583973\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.338944\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.584411\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.391899\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.591433\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.621568\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 0.917189\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.521362\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.573294\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.488009\n",
      "[07/26/2023-05:37:49] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.230159 seconds. Fastest Tactic: 0x3a8712b17741b582 Time: 0.328411\n",
      "[07/26/2023-05:37:49] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:49] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:49] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3a8712b17741b582\n",
      "[07/26/2023-05:37:49] [V] [TRT] =============== Computing costs for /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu\n",
      "[07/26/2023-05:37:49] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(1), Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:37:49] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.864402\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.471918\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.487291\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.85504\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.913408\n",
      "[07/26/2023-05:37:49] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.908873\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.881079\n",
      "[07/26/2023-05:37:50] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.31379\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.915771\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.745179\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.646437\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.917504\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.854894\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.524727\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.426569\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.565152\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.513463\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.517559\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.719991\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.510098\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.836169\n",
      "[07/26/2023-05:37:50] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.17128 seconds. Fastest Tactic: 0x94119b4c514b211a Time: 0.426569\n",
      "[07/26/2023-05:37:50] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:50] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:50] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a\n",
      "[07/26/2023-05:37:50] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(1), Float(200704,1,3584,64) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:37:50] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 1.56555\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.53248\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.921746\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.458313\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.781897\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.777362\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.584119\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.468553\n",
      "[07/26/2023-05:37:50] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.04448\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.88971\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.636197\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.807643\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.867342\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.870107\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.49589\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.679936\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.511813\n",
      "[07/26/2023-05:37:50] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.142223 seconds. Fastest Tactic: 0xf48db81f02eca9ee Time: 0.458313\n",
      "[07/26/2023-05:37:50] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:50] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:50] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xf48db81f02eca9ee\n",
      "[07/26/2023-05:37:50] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1), Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:37:50] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.654629\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.746057\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.690176\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.643218\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.673646\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.616155\n",
      "[07/26/2023-05:37:50] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0480404 seconds. Fastest Tactic: 0x3104d85fecdd547c Time: 0.616155\n",
      "[07/26/2023-05:37:50] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:50] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:50] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3104d85fecdd547c\n",
      "[07/26/2023-05:37:50] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1), Float(50176,1:4,896,16) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:37:50] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.825499\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.82432\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.482597\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.529554\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.50293\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.457289\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.34816\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.874935\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.351067\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.517998\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.902875\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.659749\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.651849\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.367323\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.360544\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.357376\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.612352\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 0.726747\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.631703\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.408576\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.59392\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.494885\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.514633\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.386779\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.609573\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.356937\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.590848\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.401513\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.61824\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.652448\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 0.928475\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.528942\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.603579\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.499424\n",
      "[07/26/2023-05:37:50] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.245673 seconds. Fastest Tactic: 0x3a8712b17741b582 Time: 0.34816\n",
      "[07/26/2023-05:37:50] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:50] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:50] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3a8712b17741b582\n",
      "[07/26/2023-05:37:50] [V] [TRT] =============== Computing costs for /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/act1/Relu\n",
      "[07/26/2023-05:37:50] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:37:50] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:37:50] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:37:50] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:37:50] [V] [TRT] =============== Computing costs for /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu\n",
      "[07/26/2023-05:37:50] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(576,9,3,1), Float(1), Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:37:50] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.864402\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.471625\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.487433\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.85504\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.902875\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.907849\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.892782\n",
      "[07/26/2023-05:37:50] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.37933\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.918967\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.740791\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.643511\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.914432\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.872302\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.533504\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.43125\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.576233\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.524873\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.525166\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.735525\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.521947\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.836754\n",
      "[07/26/2023-05:37:50] [V] [TRT] /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.173829 seconds. Fastest Tactic: 0x94119b4c514b211a Time: 0.43125\n",
      "[07/26/2023-05:37:50] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:50] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:50] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a\n",
      "[07/26/2023-05:37:50] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(576,9,3,1), Float(1), Float(200704,1,3584,64) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:37:50] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 1.58661\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.547986\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.930688\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.46549\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.793454\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.785984\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.591287\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.469152\n",
      "[07/26/2023-05:37:50] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.01376\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.865719\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.623909\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.79243\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.862939\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.87669\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.503223\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.680384\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.513198\n",
      "[07/26/2023-05:37:50] [V] [TRT] /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.139652 seconds. Fastest Tactic: 0xf48db81f02eca9ee Time: 0.46549\n",
      "[07/26/2023-05:37:50] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:50] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:50] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xf48db81f02eca9ee\n",
      "[07/26/2023-05:37:50] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(576,9,3,1), Float(1), Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:37:50] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.670574\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.765915\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.70539\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.655945\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.681838\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.626103\n",
      "[07/26/2023-05:37:50] [V] [TRT] /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0493728 seconds. Fastest Tactic: 0x3104d85fecdd547c Time: 0.626103\n",
      "[07/26/2023-05:37:50] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:50] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:50] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3104d85fecdd547c\n",
      "[07/26/2023-05:37:50] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(576,9,3,1), Float(1), Float(50176,1:4,896,16) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:37:50] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.831927\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.832219\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.49035\n",
      "[07/26/2023-05:37:50] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.537198\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.503369\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.463726\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.354153\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.884736\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.356937\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.519022\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.910775\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.630199\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.606501\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.372443\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.364965\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.36539\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.614985\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 0.739328\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.641902\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.418085\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.60416\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.498981\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.515365\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.39424\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.610597\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.363813\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.597285\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.408722\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.626395\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.660919\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 0.935625\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.534967\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.608256\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.506002\n",
      "[07/26/2023-05:37:51] [V] [TRT] /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.224835 seconds. Fastest Tactic: 0x3a8712b17741b582 Time: 0.354153\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:51] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:51] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3a8712b17741b582\n",
      "[07/26/2023-05:37:51] [V] [TRT] =============== Computing costs for /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu\n",
      "[07/26/2023-05:37:51] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.251904\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.245321\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.247808\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.249856\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.358839\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.453659\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.345966\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.45685\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.24971\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.289819\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.655067\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.245595\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.374053\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.308809\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.311735\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.445015\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.231973\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.252343\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.289207\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.254537\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.279845\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.280869\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.373614\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.266679\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.25483\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.23157\n",
      "[07/26/2023-05:37:51] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.131705 seconds. Fastest Tactic: 0xbb8c3889c7eacd30 Time: 0.23157\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:51] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CudnnConvolution[0x80000000])\n",
      "[07/26/2023-05:37:51] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:51] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbb8c3889c7eacd30\n",
      "[07/26/2023-05:37:51] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.449097\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.272677\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.467822\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.256439\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.393509\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.389691\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.270043\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.254976\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.49664\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.419259\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.322853\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.388434\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.252782\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.430958\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.254245\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.321097\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.249563\n",
      "[07/26/2023-05:37:51] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0804585 seconds. Fastest Tactic: 0x94a7db94ba744c45 Time: 0.249563\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:51] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:51] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94a7db94ba744c45\n",
      "[07/26/2023-05:37:51] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.185198\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.193829\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.185198\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.189147\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.182418\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.18315\n",
      "[07/26/2023-05:37:51] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0186146 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.182418\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:51] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:51] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:37:51] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.281198\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.281614\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.242395\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.264338\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.284965\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.230299\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.175543\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.439589\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.184658\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.292425\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.462409\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.335328\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.329015\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.191291\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.180809\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.180174\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.344352\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 0.358254\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.180078\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.257902\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.312649\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.244005\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.292325\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.253175\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.331045\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.182277\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.30779\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.207579\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.177298\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.185929\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 0.476905\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.297984\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.324901\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.251182\n",
      "[07/26/2023-05:37:51] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.132518 seconds. Fastest Tactic: 0x3a8712b17741b582 Time: 0.175543\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:51] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:51] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3a8712b17741b582\n",
      "[07/26/2023-05:37:51] [V] [TRT] =============== Computing costs for /layer2/layer2.0/conv2/Conv\n",
      "[07/26/2023-05:37:51] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.488032\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.477801\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.482158\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.479054\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.626249\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.871717\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.592311\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.45056\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.890734\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.477504\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.502491\n",
      "[07/26/2023-05:37:51] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.30867\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.485083\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.711351\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.598587\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.542574\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.866743\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.447781\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.490203\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.43403\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.376686\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.557024\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.506149\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.465774\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.541842\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.701591\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.413403\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.358107\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.487717\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.437833\n",
      "[07/26/2023-05:37:51] [V] [TRT] /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.219317 seconds. Fastest Tactic: 0x0190806602534cfd Time: 0.358107\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:51] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:51] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0190806602534cfd\n",
      "[07/26/2023-05:37:51] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.842665\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.514505\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.458459\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.759515\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.761417\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.412526\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.45685\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.990501\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.838949\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.807223\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.485669\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.900681\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.498354\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.64629\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.490057\n",
      "[07/26/2023-05:37:51] [V] [TRT] /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.118853 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.412526\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:51] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:51] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/26/2023-05:37:51] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.355913\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.374473\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.343616\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.351963\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.336165\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.341138\n",
      "[07/26/2023-05:37:51] [V] [TRT] /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0294647 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.336165\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:51] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:51] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:37:51] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:37:51] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:51] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.472064\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.472795\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.468402\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.502053\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.332069\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.326546\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.344791\n",
      "[07/26/2023-05:37:52] [V] [TRT] /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0391929 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.326546\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:52] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:37:52] [V] [TRT] =============== Computing costs for /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu\n",
      "[07/26/2023-05:37:52] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(1), Float(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.0741669\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.0770926\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.0741371\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24 Time: 0.074032\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.0807497\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675 Time: 0.0928914\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.0790674\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.072432\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd Time: 0.073216\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.106\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.0755063\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.0738743\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e Time: 0.0725554\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303 Time: 0.0739474\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.0805303\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338 Time: 0.0857966\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.0751177\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.0769463\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.073728\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0753371\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.0721097\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96 Time: 0.0740937\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.0738011\n",
      "[07/26/2023-05:37:52] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0597612 seconds. Fastest Tactic: 0xa31d27de74b895ff Time: 0.0721097\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:52] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa31d27de74b895ff\n",
      "[07/26/2023-05:37:52] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(1), Float(100352,1,3584,128) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.0652922\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.079216\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.0726149\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484 Time: 0.0652922\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86 Time: 0.0711924\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.111559\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb Time: 0.070461\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.0716069\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.0795794\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.0659505\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.06144\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d Time: 0.064512\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.0664777\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.0652602\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.0726309\n",
      "[07/26/2023-05:37:52] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0430605 seconds. Fastest Tactic: 0x7121ec1db3f80c67 Time: 0.06144\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:52] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7121ec1db3f80c67\n",
      "[07/26/2023-05:37:52] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1), Float(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.0713676\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_alignc4 Tactic: 0xc8ad2c0ce0af5623 Time: 0.0672701\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.0690423\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.0609524\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x1d144cf9675b8d6f Time: 0.0610545\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.0611962\n",
      "[07/26/2023-05:37:52] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0190119 seconds. Fastest Tactic: 0xe0a307ffe0ffb6a5 Time: 0.0609524\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:52] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xe0a307ffe0ffb6a5\n",
      "[07/26/2023-05:37:52] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1), Float(25088,1:4,896,32) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.0648046\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.0643825\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.0795726\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4 Time: 0.0645166\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.0687543\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7 Time: 0.0783611\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.0574979\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.0545539\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.079152\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3 Time: 0.065597\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.0547398\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.0698758\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.0767269\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.0751886\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.0748983\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.0544183\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.0540282\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.0540282\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.0772389\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b Time: 0.0616838\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 0.0692907\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.0623543\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.0569539\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.0652922\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.0627002\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.0675352\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.0565638\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.0751337\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.0542232\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.0673402\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.0563048\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.0619764\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 0.0731428\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.0690956\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.076\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.0602209\n",
      "[07/26/2023-05:37:52] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0972314 seconds. Fastest Tactic: 0x72a5d05b1bb165ef Time: 0.0540282\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:52] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x72a5d05b1bb165ef\n",
      "[07/26/2023-05:37:52] [V] [TRT] =============== Computing costs for /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu\n",
      "[07/26/2023-05:37:52] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.488448\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.477769\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.481865\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.479232\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.627127\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.87157\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.592311\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.450757\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.891026\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.477477\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.509806\n",
      "[07/26/2023-05:37:52] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.30458\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.481746\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.703762\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.597431\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.541696\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.889563\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.452462\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.495323\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.436347\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.379634\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.559104\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.503369\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.466944\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.544329\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.701586\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.413989\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.357376\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.488302\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.438551\n",
      "[07/26/2023-05:37:52] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.212936 seconds. Fastest Tactic: 0x0190806602534cfd Time: 0.357376\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:52] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0190806602534cfd\n",
      "[07/26/2023-05:37:52] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.842606\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.514194\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.458313\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.759954\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.760686\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.412818\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.458021\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.974555\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.817774\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.763465\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.454363\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.895854\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.502345\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.651365\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.493275\n",
      "[07/26/2023-05:37:52] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.116509 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.412818\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:52] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/26/2023-05:37:52] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.356498\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.374345\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.355474\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.35211\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.336306\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.340992\n",
      "[07/26/2023-05:37:52] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.029176 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.336306\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:52] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:37:52] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.471918\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.472795\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.469723\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.502345\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.332215\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.326363\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.344795\n",
      "[07/26/2023-05:37:52] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0388655 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.326363\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:52] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:37:52] [V] [TRT] =============== Computing costs for /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu\n",
      "[07/26/2023-05:37:52] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(1152,9,3,1), Float(1), Float(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.491666\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.481573\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.486107\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.483474\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.877705\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.891904\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.477184\n",
      "[07/26/2023-05:37:52] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.31174\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.497664\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.714505\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.613669\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.87621\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.467378\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.515058\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.404928\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.586313\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.532919\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.577536\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.734939\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.501321\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.465774\n",
      "[07/26/2023-05:37:52] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.140028 seconds. Fastest Tactic: 0x94119b4c514b211a Time: 0.404928\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:52] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a\n",
      "[07/26/2023-05:37:52] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(1152,9,3,1), Float(1), Float(100352,1,3584,128) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.910194\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.556631\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.509074\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.79477\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.790528\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.492645\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.515369\n",
      "[07/26/2023-05:37:52] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.04243\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.87947\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.797719\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.496347\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.872448\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.503077\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.641463\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.492544\n",
      "[07/26/2023-05:37:52] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.120999 seconds. Fastest Tactic: 0x94a7db94ba744c45 Time: 0.492544\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:52] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94a7db94ba744c45\n",
      "[07/26/2023-05:37:52] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1152,9,3,1), Float(1), Float(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:37:52] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.387634\n",
      "[07/26/2023-05:37:52] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.418802\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.400238\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.391461\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.382647\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.374921\n",
      "[07/26/2023-05:37:53] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0302239 seconds. Fastest Tactic: 0x3104d85fecdd547c Time: 0.374921\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:53] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3104d85fecdd547c\n",
      "[07/26/2023-05:37:53] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1152,9,3,1), Float(1), Float(25088,1:4,896,32) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.507465\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.503461\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.498688\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.533065\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.372288\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.365568\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.385902\n",
      "[07/26/2023-05:37:53] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0397034 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.365568\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:53] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:37:53] [V] [TRT] =============== Computing costs for /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu\n",
      "[07/26/2023-05:37:53] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.247515\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.289646\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.28555\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.245467\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.334821\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.513463\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.368786\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.452462\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.243419\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.268869\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.691493\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.242688\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.376832\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.305179\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.33909\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.470309\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.220306\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.248096\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.345088\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.294478\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.290702\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.275017\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.378441\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.264777\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.251611\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.221056\n",
      "[07/26/2023-05:37:53] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.157869 seconds. Fastest Tactic: 0x4efce38acc876f5c Time: 0.220306\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:53] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CudnnConvolution[0x80000000])\n",
      "[07/26/2023-05:37:53] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x4efce38acc876f5c\n",
      "[07/26/2023-05:37:53] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.237861\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.262578\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.234057\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.414866\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.383415\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.205385\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.230546\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.52176\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.440905\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.382245\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.229815\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.424229\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.244869\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.360594\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.238738\n",
      "[07/26/2023-05:37:53] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0744764 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.205385\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:53] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/26/2023-05:37:53] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.178615\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.169984\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.166473\n",
      "[07/26/2023-05:37:53] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0103442 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.166473\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:53] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:37:53] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.252489\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.250638\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.269458\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.289353\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.167643\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.164914\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.174373\n",
      "[07/26/2023-05:37:53] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0448412 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.164914\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:53] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:37:53] [V] [TRT] =============== Computing costs for /layer3/layer3.0/conv2/Conv\n",
      "[07/26/2023-05:37:53] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.492544\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.580754\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.55925\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.476581\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.618789\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.991035\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.691493\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.467077\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.889417\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.469723\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.497079\n",
      "[07/26/2023-05:37:53] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.36886\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.470514\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.701001\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.595675\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.64629\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.91019\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.427301\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.480987\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.441929\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.367726\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.654021\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.590725\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.543305\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.533358\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.701294\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.493568\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.354889\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.48629\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.428617\n",
      "[07/26/2023-05:37:53] [V] [TRT] /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.288757 seconds. Fastest Tactic: 0x0190806602534cfd Time: 0.354889\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:53] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0190806602534cfd\n",
      "[07/26/2023-05:37:53] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.433883\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.51083\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.446171\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.80384\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.752329\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.393655\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.446903\n",
      "[07/26/2023-05:37:53] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.03834\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.864841\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.752018\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.458021\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.861518\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.482743\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.696873\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.467237\n",
      "[07/26/2023-05:37:53] [V] [TRT] /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.128678 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.393655\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:53] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/26/2023-05:37:53] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.345381\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.327095\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.315392\n",
      "[07/26/2023-05:37:53] [V] [TRT] /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0169304 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.315392\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:53] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:37:53] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.466066\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.464896\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.52853\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.561298\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.322034\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.316155\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.335579\n",
      "[07/26/2023-05:37:53] [V] [TRT] /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0452158 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.316155\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:53] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:37:53] [V] [TRT] =============== Computing costs for /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu\n",
      "[07/26/2023-05:37:53] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(1), Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.0577112\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.0674301\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.0576823\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24 Time: 0.0577051\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.0733714\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675 Time: 0.046592\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.0661943\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.0462057\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd Time: 0.0498347\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.0966217\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.0590446\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.0541257\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e Time: 0.0462697\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303 Time: 0.0485669\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.0764343\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338 Time: 0.0567589\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.0621531\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.0670583\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.0492983\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0627566\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.0489737\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96 Time: 0.0544244\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.048611\n",
      "[07/26/2023-05:37:53] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0657004 seconds. Fastest Tactic: 0x5aa723e0481da855 Time: 0.0462057\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:53] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5aa723e0481da855\n",
      "[07/26/2023-05:37:53] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(1), Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:37:53] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.0462263\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.0594956\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484 Time: 0.0445897\n",
      "[07/26/2023-05:37:53] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86 Time: 0.0484693\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.0571002\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb Time: 0.0483718\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.0488594\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.0718263\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.0530392\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d Time: 0.0427886\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.058173\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.0434103\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.0543573\n",
      "[07/26/2023-05:37:54] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.033766 seconds. Fastest Tactic: 0x55d80c17b1cd982d Time: 0.0427886\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x55d80c17b1cd982d\n",
      "[07/26/2023-05:37:54] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1), Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.0490377\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_alignc4 Tactic: 0xc8ad2c0ce0af5623 Time: 0.0447269\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.0456309\n",
      "[07/26/2023-05:37:54] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00863007 seconds. Fastest Tactic: 0xc8ad2c0ce0af5623 Time: 0.0447269\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc8ad2c0ce0af5623\n",
      "[07/26/2023-05:37:54] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1), Float(12544,1:4,896,64) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.0433371\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.0434091\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.0551985\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4 Time: 0.043008\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7 Time: 0.0541242\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3 Time: 0.0446903\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b Time: 0.0416229\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.0422469\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.0420571\n",
      "[07/26/2023-05:37:54] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0250379 seconds. Fastest Tactic: 0x130df49cb195156b Time: 0.0416229\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x130df49cb195156b\n",
      "[07/26/2023-05:37:54] [V] [TRT] =============== Computing costs for /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu\n",
      "[07/26/2023-05:37:54] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.492105\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.580901\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.559104\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.47616\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.619374\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.991086\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.691639\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.467086\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.889563\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.470455\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.497225\n",
      "[07/26/2023-05:37:54] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.37011\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.469559\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.700878\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.595968\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.646437\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.91136\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.427447\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.481422\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.442199\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.367762\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.654336\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.590848\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.543456\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.533358\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.701198\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.49269\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.355209\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.486546\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.428494\n",
      "[07/26/2023-05:37:54] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.278571 seconds. Fastest Tactic: 0x0190806602534cfd Time: 0.355209\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0190806602534cfd\n",
      "[07/26/2023-05:37:54] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.433298\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.510391\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.446464\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.803547\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.752201\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.393509\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.442368\n",
      "[07/26/2023-05:37:54] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.02707\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.855968\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.750885\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.439589\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.829262\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.470747\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.687982\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.463035\n",
      "[07/26/2023-05:37:54] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.116909 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.393509\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/26/2023-05:37:54] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.339538\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.328005\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.32139\n",
      "[07/26/2023-05:37:54] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0165538 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.32139\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:37:54] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.466651\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.465774\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.528677\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.561445\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.321975\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.315831\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.335433\n",
      "[07/26/2023-05:37:54] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0448935 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.315831\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:37:54] [V] [TRT] =============== Computing costs for /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu\n",
      "[07/26/2023-05:37:54] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(2304,9,3,1), Float(1), Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.494446\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.582949\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.560859\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.479232\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.993426\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.890149\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.473819\n",
      "[07/26/2023-05:37:54] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.36704\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.47323\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.726894\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.623177\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.968704\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.462555\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.511561\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.404334\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.683447\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.616763\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.547547\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.708901\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.494592\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.440905\n",
      "[07/26/2023-05:37:54] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.145418 seconds. Fastest Tactic: 0x94119b4c514b211a Time: 0.404334\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a\n",
      "[07/26/2023-05:37:54] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(2304,9,3,1), Float(1), Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.53131\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.614107\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.551497\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.905801\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.85387\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.502638\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.561883\n",
      "[07/26/2023-05:37:54] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.14483\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.974555\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.857056\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.547109\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.952613\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.594505\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.823881\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.586167\n",
      "[07/26/2023-05:37:54] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.116071 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.502638\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/26/2023-05:37:54] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(2304,9,3,1), Float(1), Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.473527\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.450414\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.442967\n",
      "[07/26/2023-05:37:54] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0235565 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.442967\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:37:54] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(2304,9,3,1), Float(1), Float(12544,1:4,896,64) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.575323\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.574939\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.635758\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.666039\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.429495\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.423205\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.443099\n",
      "[07/26/2023-05:37:54] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0444236 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.423205\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:37:54] [V] [TRT] =============== Computing costs for /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu\n",
      "[07/26/2023-05:37:54] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:37:54] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.254245\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.281454\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.283355\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.245029\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.312174\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.749568\n",
      "[07/26/2023-05:37:54] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.34421\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.518144\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.23947\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.253659\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.742693\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.242981\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.434615\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.351817\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.30837\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.502784\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.427003\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.28789\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.355474\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.299209\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.279698\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.272677\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.435319\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.23947\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.290866\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.427465\n",
      "[07/26/2023-05:37:55] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.261017 seconds. Fastest Tactic: 0x5aa723e0481da855 Time: 0.23947\n",
      "[07/26/2023-05:37:55] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:55] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:55] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CudnnConvolution[0x80000000])\n",
      "[07/26/2023-05:37:55] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5aa723e0481da855\n",
      "[07/26/2023-05:37:55] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:37:55] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.241371\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.299447\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.228498\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.440905\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.438254\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.392338\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.22528\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.553353\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.468453\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.438711\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.225134\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.485961\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.276773\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.360155\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.271653\n",
      "[07/26/2023-05:37:55] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.113013 seconds. Fastest Tactic: 0xd15dd11d64344e83 Time: 0.225134\n",
      "[07/26/2023-05:37:55] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:55] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd15dd11d64344e83\n",
      "[07/26/2023-05:37:55] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:37:55] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.170715\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.161938\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.158743\n",
      "[07/26/2023-05:37:55] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0155143 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.158743\n",
      "[07/26/2023-05:37:55] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:55] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:37:55] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:37:55] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.239762\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.239031\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.269897\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.286194\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.161938\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.158866\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.168521\n",
      "[07/26/2023-05:37:55] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0386344 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.158866\n",
      "[07/26/2023-05:37:55] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:55] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:37:55] [V] [TRT] =============== Computing costs for /layer4/layer4.0/conv2/Conv\n",
      "[07/26/2023-05:37:55] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1), Float(1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:37:55] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.554569\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.613815\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.555154\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.480695\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.602258\n",
      "[07/26/2023-05:37:55] [V] [TRT] Fast skip Tactic:0xa9366041633a5135 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 1.25222\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.646437\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.893806\n",
      "[07/26/2023-05:37:55] [V] [TRT] Fast skip Tactic:0xcb8a43f748d8a338 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 1.024\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.468553\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.487131\n",
      "[07/26/2023-05:37:55] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.47456\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.47221\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.814811\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.690322\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.603575\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.9728\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.841728\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.562322\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.885614\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.712265\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.665746\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.653166\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.503369\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.53131\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.809399\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.451584\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.691493\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.565394\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.844215\n",
      "[07/26/2023-05:37:55] [V] [TRT] /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.577675 seconds. Fastest Tactic: 0x3712e3e595645874 Time: 0.451584\n",
      "[07/26/2023-05:37:55] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:55] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3712e3e595645874\n",
      "[07/26/2023-05:37:55] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512), Float(1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:37:55] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.468096\n",
      "[07/26/2023-05:37:55] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.585874\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.441198\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.857381\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.862354\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.770752\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.437125\n",
      "[07/26/2023-05:37:56] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.08221\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.901998\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.859575\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.436521\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.97675\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.54155\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.69744\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.533797\n",
      "[07/26/2023-05:37:56] [V] [TRT] /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.224863 seconds. Fastest Tactic: 0xd15dd11d64344e83 Time: 0.436521\n",
      "[07/26/2023-05:37:56] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd15dd11d64344e83\n",
      "[07/26/2023-05:37:56] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:37:56] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.333531\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.315977\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.309541\n",
      "[07/26/2023-05:37:56] [V] [TRT] /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0355502 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.309541\n",
      "[07/26/2023-05:37:56] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:37:56] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:37:56] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.476599\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.466505\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.534967\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.561591\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.317294\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.311296\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.330606\n",
      "[07/26/2023-05:37:56] [V] [TRT] /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0796131 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.311296\n",
      "[07/26/2023-05:37:56] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:37:56] [V] [TRT] =============== Computing costs for /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu\n",
      "[07/26/2023-05:37:56] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(1), Float(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:37:56] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.0466286\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.0439954\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.0474697\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24 Time: 0.0467749\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.0964023\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675 Time: 0.0415554\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.0687238\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.037632\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd Time: 0.0453486\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.102181\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.0608076\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.0498286\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e Time: 0.037888\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303 Time: 0.0408114\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.0912823\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338 Time: 0.040184\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.0586118\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.045056\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.0421589\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0408389\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.0449109\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96 Time: 0.0566126\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.0628541\n",
      "[07/26/2023-05:37:56] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0786814 seconds. Fastest Tactic: 0x5aa723e0481da855 Time: 0.037632\n",
      "[07/26/2023-05:37:56] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5aa723e0481da855\n",
      "[07/26/2023-05:37:56] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(1), Float(25088,1,3584,512) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:37:56] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.0415451\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.0578804\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484 Time: 0.0420457\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86 Time: 0.038168\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.0579322\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb Time: 0.037144\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.03804\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.0713387\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.0548571\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d Time: 0.0386994\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.0609524\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.0392046\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.0495147\n",
      "[07/26/2023-05:37:56] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.037409 seconds. Fastest Tactic: 0x1022069e6f8d9aeb Time: 0.037144\n",
      "[07/26/2023-05:37:56] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1022069e6f8d9aeb\n",
      "[07/26/2023-05:37:56] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1), Float(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:37:56] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.0343269\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_alignc4 Tactic: 0xc8ad2c0ce0af5623 Time: 0.0322203\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.0330834\n",
      "[07/26/2023-05:37:56] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0206105 seconds. Fastest Tactic: 0xc8ad2c0ce0af5623 Time: 0.0322203\n",
      "[07/26/2023-05:37:56] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc8ad2c0ce0af5623\n",
      "[07/26/2023-05:37:56] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1), Float(6272,1:4,896,128) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:37:56] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.0398263\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.0402046\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.0443246\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4 Time: 0.0395463\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7 Time: 0.0434583\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3 Time: 0.0371657\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b Time: 0.0297289\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.0301687\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.0303918\n",
      "[07/26/2023-05:37:56] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0258053 seconds. Fastest Tactic: 0x130df49cb195156b Time: 0.0297289\n",
      "[07/26/2023-05:37:56] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:56] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x130df49cb195156b\n",
      "[07/26/2023-05:37:56] [V] [TRT] =============== Computing costs for /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu\n",
      "[07/26/2023-05:37:56] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1), Float(1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:37:56] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.555008\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.611328\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.555008\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.480695\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.602939\n",
      "[07/26/2023-05:37:56] [V] [TRT] Fast skip Tactic:0xa9366041633a5135 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 1.4592\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.647273\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.894537\n",
      "[07/26/2023-05:37:56] [V] [TRT] Fast skip Tactic:0xcb8a43f748d8a338 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 1.0281\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.468992\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.487278\n",
      "[07/26/2023-05:37:56] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.47354\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.472416\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.811447\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.690615\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.60416\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.974409\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.841728\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.562917\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.881225\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.713143\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.665454\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.653134\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.503369\n",
      "[07/26/2023-05:37:56] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.531456\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.810715\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.452023\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.691639\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.566126\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.844069\n",
      "[07/26/2023-05:37:57] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.548551 seconds. Fastest Tactic: 0x3712e3e595645874 Time: 0.452023\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3712e3e595645874\n",
      "[07/26/2023-05:37:57] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512), Float(1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.468407\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.586418\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.441093\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.857234\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.860306\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.770487\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.437394\n",
      "[07/26/2023-05:37:57] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.08134\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.902181\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.862802\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.46475\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.979529\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.542171\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.697298\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.53381\n",
      "[07/26/2023-05:37:57] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.1944 seconds. Fastest Tactic: 0x1da91d865428f237 Time: 0.437394\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1da91d865428f237\n",
      "[07/26/2023-05:37:57] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.333385\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.315685\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.309687\n",
      "[07/26/2023-05:37:57] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0287712 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.309687\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:37:57] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.476014\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.46709\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.534674\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.561445\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.316855\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.310821\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.330459\n",
      "[07/26/2023-05:37:57] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0741791 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.310821\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:37:57] [V] [TRT] =============== Computing costs for /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu\n",
      "[07/26/2023-05:37:57] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1), Float(4608,9,3,1), Float(1), Float(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.556763\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.612791\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.558226\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.483296\n",
      "[07/26/2023-05:37:57] [V] [TRT] Fast skip Tactic:0xa9366041633a5135 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 1.2544\n",
      "[07/26/2023-05:37:57] [V] [TRT] Fast skip Tactic:0xcb8a43f748d8a338 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 1.0216\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.470894\n",
      "[07/26/2023-05:37:57] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.47558\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.474258\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.813787\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.693541\n",
      "[07/26/2023-05:37:57] [V] [TRT] Fast skip Tactic:0x9d9fdb5fd9945f64 which exceed time limit during pre-run\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 1.01376\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.879177\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.581486\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.820078\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.685527\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.665161\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.542427\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.820809\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.567296\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.848165\n",
      "[07/26/2023-05:37:57] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.147606 seconds. Fastest Tactic: 0x5aa723e0481da855 Time: 0.470894\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5aa723e0481da855\n",
      "[07/26/2023-05:37:57] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512), Float(4608,9,3,1), Float(1), Float(25088,1,3584,512) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.907936\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.999717\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.844485\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 1.25904\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 1.24153\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 1.17029\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.837358\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.47488\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 1.29521\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 1.25762\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.844247\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 1.33822\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.942811\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 1.09188\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.93467\n",
      "[07/26/2023-05:37:57] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.190043 seconds. Fastest Tactic: 0x1da91d865428f237 Time: 0.837358\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1da91d865428f237\n",
      "[07/26/2023-05:37:57] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(4608,9,3,1), Float(1), Float(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.735963\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.721435\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.713582\n",
      "[07/26/2023-05:37:57] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0244588 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.713582\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/26/2023-05:37:57] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(4608,9,3,1), Float(1), Float(6272,1:4,896,128) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.874981\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.86645\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.932398\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.959049\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.716361\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.710789\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.730272\n",
      "[07/26/2023-05:37:57] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0651756 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.710789\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/26/2023-05:37:57] [V] [TRT] =============== Computing costs for /global_pool/pool/GlobalAveragePool\n",
      "[07/26/2023-05:37:57] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1) -> Float(512,1,1,1) ***************\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /global_pool/pool/GlobalAveragePool (CudnnPooling[0x80000005])\n",
      "[07/26/2023-05:37:57] [V] [TRT] CudnnPooling has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /global_pool/pool/GlobalAveragePool (CaskPooling[0x8000002f])\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll6_tThreads49 Tactic: 0xa4a96ea1892462c7 Time: 0.0187229\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll7_tThreads49 Tactic: 0x489ba15aaac78fba Time: 0.019004\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll2_tThreads49 Tactic: 0x31d30f1a58b7ea39 Time: 0.0188086\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll3_tThreads49 Tactic: 0xdde1c0e17b540744 Time: 0.0186754\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll4_tThreads49 Tactic: 0xee145e7c61eda6b8 Time: 0.0187703\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll1_tThreads49 Tactic: 0x975cf03c939dc33b Time: 0.0191983\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm50_xmma_pooling_nd_NCDHW_kAVERAGE_kGENERIC_3D_POOLING_MODE_kFLOAT_0 Tactic: 0xba33c80addb15739 Time: 0.00616286\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll5_tThreads49 Tactic: 0x02269187420e4bc5 Time: 0.0187223\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll8_tThreads49 Tactic: 0xc342539bbc57213f Time: 0.0188834\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Average_FastDiv Tactic: 0x933eceba7b866d59 Time: 0.00534129\n",
      "[07/26/2023-05:37:57] [V] [TRT] /global_pool/pool/GlobalAveragePool (CaskPooling[0x8000002f]) profiling completed in 0.0216965 seconds. Fastest Tactic: 0x933eceba7b866d59 Time: 0.00534129\n",
      "[07/26/2023-05:37:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x933eceba7b866d59\n",
      "[07/26/2023-05:37:57] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128) -> Float(128,1:4,128,128) ***************\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /global_pool/pool/GlobalAveragePool (CaskPooling[0x8000002f])\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NHWC_Average_FastDiv_CAlign4 Tactic: 0xfab3e2ee1c085a9a Time: 0.00494597\n",
      "[07/26/2023-05:37:57] [V] [TRT] /global_pool/pool/GlobalAveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00255832 seconds. Fastest Tactic: 0xfab3e2ee1c085a9a Time: 0.00494597\n",
      "[07/26/2023-05:37:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xfab3e2ee1c085a9a\n",
      "[07/26/2023-05:37:57] [V] [TRT] =============== Computing costs for /fc/Gemm\n",
      "[07/26/2023-05:37:57] [V] [TRT] *************** Autotuning format combination: Float(512,1,1,1) -> Float(1000,1,1,1) ***************\n",
      "[07/26/2023-05:37:57] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.0586423\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.0470674\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.0591969\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24 Time: 0.0582705\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.0441406\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.082432\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.0477775\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675 Time: 0.0607573\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.0279162\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.0394949\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd Time: 0.0251413\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.0838949\n",
      "[07/26/2023-05:37:57] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1 Time: 0.112713\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.0228624\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.0973029\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.0448731\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505 Time: 0.0401966\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7 Time: 0.0602133\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a Time: 0.0261592\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e Time: 0.039168\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303 Time: 0.0474697\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b Time: 0.050176\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.0751177\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x16_stage2_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe52b0ddb126aa135 Time: 0.032896\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4 Time: 0.0605897\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338 Time: 0.0506789\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd Time: 0.0494446\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.0553524\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77 Time: 0.0233848\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.0489569\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x32x16_stage2_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xb2c8ebee321e63d6 Time: 0.0404114\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.0471109\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0423577\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.0255512\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96 Time: 0.0524907\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.117979\n",
      "[07/26/2023-05:37:58] [V] [TRT] /fc/Gemm (CaskConvolution[0x80000009]) profiling completed in 0.220994 seconds. Fastest Tactic: 0xb0bf940d5e0f9f45 Time: 0.0228624\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CublasConvolution[0x80000029])\n",
      "[07/26/2023-05:37:58] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskGemmConvolution[0x8000002e])\n",
      "[07/26/2023-05:37:58] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CudnnConvolution[0x80000000])\n",
      "[07/26/2023-05:37:58] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xb0bf940d5e0f9f45\n",
      "[07/26/2023-05:37:58] [V] [TRT] *************** Autotuning format combination: Float(512,1,512,512) -> Float(1000,1,1000,1000) ***************\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.021935\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.0403874\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484 Time: 0.0570606\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86 Time: 0.0308105\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537 Time: 0.0207314\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0 Time: 0.0311689\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808 Time: 0.0305545\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.0537265\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb Time: 0.0314295\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.0321243\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.0488305\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939 Time: 0.0536869\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22 Time: 0.0354304\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a Time: 0.0158043\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.0158974\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d Time: 0.0205427\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.0194383\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.0209365\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.0428503\n",
      "[07/26/2023-05:37:58] [V] [TRT] /fc/Gemm (CaskConvolution[0x80000009]) profiling completed in 0.0667536 seconds. Fastest Tactic: 0x1fb90698107bb33a Time: 0.0158043\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CublasConvolution[0x80000029])\n",
      "[07/26/2023-05:37:58] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1fb90698107bb33a\n",
      "[07/26/2023-05:37:58] [V] [TRT] *************** Autotuning format combination: Float(128,1:4,128,128) -> Float(1000,1,1,1) ***************\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.0413989\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_alignc4 Tactic: 0x440241d9c93d605d Time: 0.038776\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_alignc4 Tactic: 0xc8ad2c0ce0af5623 Time: 0.0388754\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.0395703\n",
      "[07/26/2023-05:37:58] [V] [TRT] /fc/Gemm (CaskConvolution[0x80000009]) profiling completed in 0.0151022 seconds. Fastest Tactic: 0x440241d9c93d605d Time: 0.038776\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x440241d9c93d605d\n",
      "[07/26/2023-05:37:58] [V] [TRT] *************** Autotuning format combination: Float(128,1:4,128,128) -> Float(250,1:4,250,250) ***************\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskConvolution[0x80000009])\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.055101\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.0557364\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.0364617\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4 Time: 0.0550415\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7 Time: 0.0359394\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3 Time: 0.0312219\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b Time: 0.0380343\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462 Time: 0.0377554\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.0385829\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.0399589\n",
      "[07/26/2023-05:37:58] [V] [TRT] /fc/Gemm (CaskConvolution[0x80000009]) profiling completed in 0.0357274 seconds. Fastest Tactic: 0xae0c89d047932ba3 Time: 0.0312219\n",
      "[07/26/2023-05:37:58] [V] [TRT] /fc/Gemm: 56 available tactics, 0 unparsable, 28 pruned, 28 remaining after tactic pruning.\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskGemmConvolution[0x8000002e])\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 1 numBuffers: 0 numKernels: 1 Tactic: 0x00000000000203be Time: 0.00858111\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 1 numBuffers: 0 numKernels: 1 Tactic: 0x00000000000202f3 Time: 0.00896914\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 1 numKernels: 1 Tactic: 0x00000000020403be Time: 0.0104042\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 1 numKernels: 1 Tactic: 0x00000000020402f3 Time: 0.0109375\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 1 numBuffers: 0 numKernels: 1 Tactic: 0x000000000002031a Time: 0.014122\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 1 numBuffers: 0 numKernels: 1 Tactic: 0x00000000000202b8 Time: 0.0138934\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 2 numKernels: 2 Tactic: 0x00000002040403be Time: 0.0118699\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 2 numKernels: 2 Tactic: 0x00000002040402f3 Time: 0.0129025\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 2 numKernels: 1 Tactic: 0x00000000040602f3 Time: 0.013419\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 2 numKernels: 1 Tactic: 0x00000000040603be Time: 0.0114071\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 3 numKernels: 2 Tactic: 0x00000002060603be Time: 0.0134263\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 3 numKernels: 2 Tactic: 0x00000002060602f3 Time: 0.0136844\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 1 numKernels: 1 Tactic: 0x00000000020402b8 Time: 0.0172531\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 1 numKernels: 1 Tactic: 0x000000000204031a Time: 0.017665\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 3 numKernels: 1 Tactic: 0x00000000060802f3 Time: 0.013302\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 3 numKernels: 1 Tactic: 0x00000000060803be Time: 0.0144384\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 2 numKernels: 1 Tactic: 0x00000000040602b8 Time: 0.0204611\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 2 numKernels: 1 Tactic: 0x000000000406031a Time: 0.0211318\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 2 numKernels: 2 Tactic: 0x000000020404031a Time: 0.0189486\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 2 numKernels: 2 Tactic: 0x00000002040402b8 Time: 0.0185383\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 4 numKernels: 2 Tactic: 0x00000002080803be Time: 0.014523\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 4 numKernels: 2 Tactic: 0x00000002080802f3 Time: 0.016319\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 3 numKernels: 1 Tactic: 0x00000000060802b8 Time: 0.0223608\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 3 numKernels: 1 Tactic: 0x000000000608031a Time: 0.0230557\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 3 numKernels: 2 Tactic: 0x00000002060602b8 Time: 0.0205002\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 3 numKernels: 2 Tactic: 0x000000020606031a Time: 0.0212487\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 4 numKernels: 2 Tactic: 0x000000020808031a Time: 0.0228624\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 4 numKernels: 2 Tactic: 0x00000002080802b8 Time: 0.0221825\n",
      "[07/26/2023-05:37:58] [V] [TRT] /fc/Gemm (CaskGemmConvolution[0x8000002e]) profiling completed in 0.0782582 seconds. Fastest Tactic: 0x00000000000203be Time: 0.00858111\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskFlattenConvolution[0x80000036])\n",
      "[07/26/2023-05:37:58] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CublasConvolution[0x80000029])\n",
      "[07/26/2023-05:37:58] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[07/26/2023-05:37:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskGemmConvolution Tactic: 0x00000000000203be\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing costs for reshape_after_/fc/Gemm\n",
      "[07/26/2023-05:37:58] [V] [TRT] *************** Autotuning format combination: Float(1000,1,1,1) -> Float(1000,1) ***************\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: reshape_after_/fc/Gemm (Shuffle[0x8000000d])\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00800356\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0230106\n",
      "[07/26/2023-05:37:58] [V] [TRT] reshape_after_/fc/Gemm (Shuffle[0x8000000d]) profiling completed in 0.00347332 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00800356\n",
      "[07/26/2023-05:37:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
      "[07/26/2023-05:37:58] [V] [TRT] *************** Autotuning format combination: Float(250,1:4,250,250) -> Float(1000,1) ***************\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: reshape_after_/fc/Gemm (Shuffle[0x8000000d])\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00727337\n",
      "[07/26/2023-05:37:58] [V] [TRT] reshape_after_/fc/Gemm (Shuffle[0x8000000d]) profiling completed in 0.00167955 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00727337\n",
      "[07/26/2023-05:37:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:37:58] [V] [TRT] *************** Autotuning Reformat: Float(150528,50176,224,1) -> Float(150528,1,672,3) ***************\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(x -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0635368\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0693531\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.155648\n",
      "[07/26/2023-05:37:58] [V] [TRT] Optimizer Reformat(x -> <out>) (Reformat[0x80000006]) profiling completed in 0.00631476 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0635368\n",
      "[07/26/2023-05:37:58] [V] [TRT] *************** Autotuning Reformat: Float(150528,50176,224,1) -> Float(50176,1:4,224,1) ***************\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(x -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.117248\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0712899\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.156965\n",
      "[07/26/2023-05:37:58] [V] [TRT] Optimizer Reformat(x -> <out>) (Reformat[0x80000006]) profiling completed in 0.00662199 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0712899\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs: Identity_0\n",
      "[07/26/2023-05:37:58] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: Identity_0 (Reformat[0x80000006])\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00498144\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0129855\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00602952\n",
      "[07/26/2023-05:37:58] [V] [TRT] Identity_0 (Reformat[0x80000006]) profiling completed in 0.00614308 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00498144\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: Identity_0 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:37:58] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:37:58] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:37:58] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00648427\n",
      "[07/26/2023-05:37:58] [V] [TRT] Identity_0 (MyelinReformat[0x80000035]) profiling completed in 0.330631 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00648427\n",
      "[07/26/2023-05:37:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs: Identity_2\n",
      "[07/26/2023-05:37:58] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs: Identity_3\n",
      "[07/26/2023-05:37:58] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs: Identity_4\n",
      "[07/26/2023-05:37:58] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:58] [V] [TRT] =============== Computing reformatting costs: Identity_1\n",
      "[07/26/2023-05:37:58] [V] [TRT] *************** Autotuning Reformat: Float(4608,9,3,1) -> Float(4608,9,3,1) ***************\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: Identity_1 (Reformat[0x80000006])\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0588678\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0596495\n",
      "[07/26/2023-05:37:58] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0593432\n",
      "[07/26/2023-05:37:58] [V] [TRT] Identity_1 (Reformat[0x80000006]) profiling completed in 0.00603641 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0588678\n",
      "[07/26/2023-05:37:58] [V] [TRT] --------------- Timing Runner: Identity_1 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:37:59] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:37:59] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:37:59] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0587078\n",
      "[07/26/2023-05:37:59] [V] [TRT] Identity_1 (MyelinReformat[0x80000035]) profiling completed in 0.288043 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0587078\n",
      "[07/26/2023-05:37:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: MyelinReformat Tactic: 0x0000000000000000\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs: Identity_5\n",
      "[07/26/2023-05:37:59] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:37:59] [V] [TRT] --------------- Timing Runner: Identity_5 (Reformat[0x80000006])\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0050828\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0133752\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00601752\n",
      "[07/26/2023-05:37:59] [V] [TRT] Identity_5 (Reformat[0x80000006]) profiling completed in 0.00601613 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0050828\n",
      "[07/26/2023-05:37:59] [V] [TRT] --------------- Timing Runner: Identity_5 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:37:59] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:37:59] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:37:59] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00661278\n",
      "[07/26/2023-05:37:59] [V] [TRT] Identity_5 (MyelinReformat[0x80000035]) profiling completed in 0.288237 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00661278\n",
      "[07/26/2023-05:37:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs: Identity_7\n",
      "[07/26/2023-05:37:59] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs: Identity_8\n",
      "[07/26/2023-05:37:59] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs: Identity_9\n",
      "[07/26/2023-05:37:59] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs: Identity_6\n",
      "[07/26/2023-05:37:59] [V] [TRT] *************** Autotuning Reformat: Float(2304,9,3,1) -> Float(2304,9,3,1) ***************\n",
      "[07/26/2023-05:37:59] [V] [TRT] --------------- Timing Runner: Identity_6 (Reformat[0x80000006])\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0163764\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0171698\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0169697\n",
      "[07/26/2023-05:37:59] [V] [TRT] Identity_6 (Reformat[0x80000006]) profiling completed in 0.00589797 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0163764\n",
      "[07/26/2023-05:37:59] [V] [TRT] --------------- Timing Runner: Identity_6 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:37:59] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:37:59] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:37:59] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.016387\n",
      "[07/26/2023-05:37:59] [V] [TRT] Identity_6 (MyelinReformat[0x80000035]) profiling completed in 0.290311 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.016387\n",
      "[07/26/2023-05:37:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs: Identity_10\n",
      "[07/26/2023-05:37:59] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:37:59] [V] [TRT] --------------- Timing Runner: Identity_10 (Reformat[0x80000006])\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00505758\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0133831\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00598324\n",
      "[07/26/2023-05:37:59] [V] [TRT] Identity_10 (Reformat[0x80000006]) profiling completed in 0.00599456 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00505758\n",
      "[07/26/2023-05:37:59] [V] [TRT] --------------- Timing Runner: Identity_10 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:37:59] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:37:59] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:37:59] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00639821\n",
      "[07/26/2023-05:37:59] [V] [TRT] Identity_10 (MyelinReformat[0x80000035]) profiling completed in 0.290348 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00639821\n",
      "[07/26/2023-05:37:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs: Identity_12\n",
      "[07/26/2023-05:37:59] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs: Identity_13\n",
      "[07/26/2023-05:37:59] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs: Identity_14\n",
      "[07/26/2023-05:37:59] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:37:59] [V] [TRT] =============== Computing reformatting costs: Identity_11\n",
      "[07/26/2023-05:37:59] [V] [TRT] *************** Autotuning Reformat: Float(1152,9,3,1) -> Float(1152,9,3,1) ***************\n",
      "[07/26/2023-05:37:59] [V] [TRT] --------------- Timing Runner: Identity_11 (Reformat[0x80000006])\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00525192\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0138489\n",
      "[07/26/2023-05:37:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00622907\n",
      "[07/26/2023-05:37:59] [V] [TRT] Identity_11 (Reformat[0x80000006]) profiling completed in 0.00594396 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00525192\n",
      "[07/26/2023-05:37:59] [V] [TRT] --------------- Timing Runner: Identity_11 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:38:00] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:38:00] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:38:00] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00654732\n",
      "[07/26/2023-05:38:00] [V] [TRT] Identity_11 (MyelinReformat[0x80000035]) profiling completed in 0.288039 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00654732\n",
      "[07/26/2023-05:38:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: Identity_15\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Identity_15 (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00504875\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0134782\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00601886\n",
      "[07/26/2023-05:38:00] [V] [TRT] Identity_15 (Reformat[0x80000006]) profiling completed in 0.00591888 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00504875\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Identity_15 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:38:00] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:38:00] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:38:00] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0063521\n",
      "[07/26/2023-05:38:00] [V] [TRT] Identity_15 (MyelinReformat[0x80000035]) profiling completed in 0.288454 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0063521\n",
      "[07/26/2023-05:38:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: Identity_17\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: Identity_18\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: Identity_19\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: Identity_16\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(576,9,3,1) -> Float(576,9,3,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Identity_16 (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00516359\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0137342\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00633978\n",
      "[07/26/2023-05:38:00] [V] [TRT] Identity_16 (Reformat[0x80000006]) profiling completed in 0.00590971 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00516359\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Identity_16 (MyelinReformat[0x80000035])\n",
      "[07/26/2023-05:38:00] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:38:00] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:38:00] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00644353\n",
      "[07/26/2023-05:38:00] [V] [TRT] Identity_16 (MyelinReformat[0x80000035]) profiling completed in 0.286678 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00644353\n",
      "[07/26/2023-05:38:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(802816,12544,112,1) -> Float(200704,1:4,1792,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.402432\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.31349\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.311003\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0125757 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.311003\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,7168,64) -> Float(802816,12544,112,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.495323\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.312905\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.310857\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0132463 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.310857\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,7168,64) -> Float(200704,1:4,1792,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.327387\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.312027\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.327273\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0122041 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.312027\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,1792,16) -> Float(802816,12544,112,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.501175\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.313051\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.311003\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0135869 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.311003\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0999131\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0823589\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0794331\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006]) profiling completed in 0.00629241 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0794331\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.101449\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0826651\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0796046\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006]) profiling completed in 0.00631155 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0796046\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.117257\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.08192\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0794149\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006]) profiling completed in 0.00652251 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0794149\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.083968\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0812\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.083968\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006]) profiling completed in 0.0061282 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0812\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.09984\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0825051\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0796069\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0063426 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0796069\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.10123\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0825783\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0794331\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00617687 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0794331\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.115285\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0818423\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0795223\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00666961 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0795223\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.084112\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0814812\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0840411\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00614508 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0814812\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.117081\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.08192\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.07968\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00659902 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.07968\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0839086\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0813257\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0841143\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0329864 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0813257\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer1/layer1.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.115362\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0819931\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(<in> -> /layer1/layer1.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00644973 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer1/layer1.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0841143\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0811886\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0841143\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(<in> -> /layer1/layer1.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00602079 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0811886\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0584381\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0422766\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0411109\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00596448 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0411109\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0601463\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0423771\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0411097\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00599424 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0411097\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0562225\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0426457\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.04096\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00614725 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.04096\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0434857\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0423131\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0434469\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00616015 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0423131\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0564724\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0426766\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.040888\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0059796 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.040888\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.043456\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0424297\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0434103\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00610776 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0424297\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:00] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0586118\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0423131\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0411406\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00588437 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0411406\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0600594\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0423726\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0411063\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00591274 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0411063\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0560518\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.04268\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0409234\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00608847 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0409234\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.04352\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0425269\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0434491\n",
      "[07/26/2023-05:38:00] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.0065389 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0425269\n",
      "[07/26/2023-05:38:00] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0564952\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.042816\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0409886\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00591019 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0409886\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0434114\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0423497\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.043456\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00656701 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0423497\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0301879\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0240899\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0218162\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00528143 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0218162\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0311378\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0240983\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0217927\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00524846 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0217927\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0295497\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0232222\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0215667\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0053776 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0215667\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0232803\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0230648\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0229466\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00572514 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0229466\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0284769\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0230217\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0213793\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00578515 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0213793\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0229336\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0255764\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0229283\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00789165 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0229283\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0284526\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0235246\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0216248\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00593515 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0216248\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0288805\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0234952\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0216157\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00547181 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0216157\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0281288\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0230916\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0214126\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00596197 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0214126\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.022946\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.02301\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0229127\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00584113 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0229127\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.028576\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0255901\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0213525\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00576678 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0213525\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0228624\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0230498\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0229251\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00596092 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0228624\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0154194\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0154039\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.010608\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00606454 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.010608\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0163027\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0152398\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0105241\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00607031 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0105241\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.014075\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0153431\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0101937\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00616857 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0101937\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0128705\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0154702\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0128994\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00613379 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0128705\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0141486\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0150094\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0105136\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00607087 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0105136\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0128446\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0155854\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.012885\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00607575 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0128446\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0154542\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0150738\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0105773\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00602145 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0105773\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0157234\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0151031\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0105848\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00603935 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0105848\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0140405\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0154482\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0101522\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.0062288 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0101522\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0128857\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0155433\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0128602\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00608657 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0128602\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0141889\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0152946\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0101047\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00620636 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0101047\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0132214\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0155259\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0128358\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00596743 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0128358\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(512,1,512,512) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00766653\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0146679\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00771176\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00491507 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00766653\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(128,1:4,128,128) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00764752\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0151355\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00730469\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00502366 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00730469\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(128,1:4,128,128) -> Float(512,1,1,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00809067\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.014965\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00696468\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00500716 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00696468\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(128,1:4,128,128) -> Float(512,1,512,512) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00824838\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0151109\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00803556\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00497406 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00803556\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(1000,1,1,1) -> Float(250,1:4,250,250) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00822679\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0150107\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00758809\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006]) profiling completed in 0.00493527 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00758809\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(1000,1,1000,1000) -> Float(1000,1,1,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00798222\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0148439\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0071568\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006]) profiling completed in 0.00498925 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0071568\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(1000,1,1000,1000) -> Float(250,1:4,250,250) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00830756\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0150848\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00849183\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006]) profiling completed in 0.00493673 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00830756\n",
      "[07/26/2023-05:38:01] [V] [TRT] *************** Autotuning Reformat: Float(250,1:4,250,250) -> Float(1000,1,1,1) ***************\n",
      "[07/26/2023-05:38:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006])\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00824914\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.014571\n",
      "[07/26/2023-05:38:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0059739\n",
      "[07/26/2023-05:38:01] [V] [TRT] Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006]) profiling completed in 0.00508645 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0059739\n",
      "[07/26/2023-05:38:01] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/26/2023-05:38:01] [I] [TRT] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[07/26/2023-05:38:01] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to /conv1/Conv + /act1/Relu (x) from Float(150528,50176,224,1) to Float(50176,1:4,224,1)\n",
      "[07/26/2023-05:38:01] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (/layer4/layer4.0/act2/Relu_output_0) from Float(6272,1:4,896,128) to Float(25088,1,3584,512)\n",
      "[07/26/2023-05:38:01] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (/layer4/layer4.0/act2/Relu_output_0) from Float(25088,1,3584,512) to Float(6272,1:4,896,128)\n",
      "[07/26/2023-05:38:01] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 3 to /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (/layer4/layer4.0/act2/Relu_output_0) from Float(25088,1,3584,512) to Float(25088,49,7,1)\n",
      "[07/26/2023-05:38:01] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to /fc/Gemm (/global_pool/pool/GlobalAveragePool_output_0) from Float(512,1,1,1) to Float(128,1:4,128,128)\n",
      "[07/26/2023-05:38:01] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to reshape_after_/fc/Gemm (/fc/Gemm_out_tensor) from Float(250,1:4,250,250) to Float(1000,1,1,1)\n",
      "[07/26/2023-05:38:01] [V] [TRT] Formats and tactics selection completed in 12.6907 seconds.\n",
      "[07/26/2023-05:38:01] [V] [TRT] After reformat layers: 70 layers\n",
      "[07/26/2023-05:38:01] [V] [TRT] Total number of blocks in pre-optimized block assignment: 50\n",
      "[07/26/2023-05:38:01] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[07/26/2023-05:38:01] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/26/2023-05:38:01] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:38:01] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /conv1/Conv + /act1/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: Identity_1 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /maxpool/MaxPool Host Persistent: 4048 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/act1/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 147456\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer2/layer2.0/conv2/Conv Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 589824\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer3/layer3.0/conv2/Conv Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 2359296\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer4/layer4.0/conv2/Conv Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu Host Persistent: 5488 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /global_pool/pool/GlobalAveragePool Host Persistent: 4112 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Layer: /fc/Gemm Host Persistent: 7200 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Skipped printing memory information for 46 layers with 0 memory size i.e. Host Persistent + Device Persistent + Scratch Memory == 0.\n",
      "[07/26/2023-05:38:01] [I] [TRT] Total Host Persistent Memory: 123040\n",
      "[07/26/2023-05:38:01] [I] [TRT] Total Device Persistent Memory: 0\n",
      "[07/26/2023-05:38:01] [I] [TRT] Total Scratch Memory: 2359296\n",
      "[07/26/2023-05:38:01] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 9 MiB, GPU 196 MiB\n",
      "[07/26/2023-05:38:01] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 52 steps to complete.\n",
      "[07/26/2023-05:38:01] [V] [TRT] STILL ALIVE: Started step 26 of 52\n",
      "[07/26/2023-05:38:01] [V] [TRT] STILL ALIVE: Started step 51 of 52\n",
      "[07/26/2023-05:38:01] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 5.62184ms to assign 22 blocks to 52 nodes requiring 169049088 bytes.\n",
      "[07/26/2023-05:38:01] [V] [TRT] Total number of blocks in optimized block assignment: 22\n",
      "[07/26/2023-05:38:01] [I] [TRT] Total Activation Memory: 169049088\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /conv1/Conv + /act1/Relu Set kernel index: 0\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /maxpool/MaxPool Set kernel index: 1\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu Set kernel index: 2\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu Set kernel index: 2\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/act1/Relu Set kernel index: 2\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu Set kernel index: 2\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu Set kernel index: 2\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer2/layer2.0/conv2/Conv Set kernel index: 3\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu Set kernel index: 4\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu Set kernel index: 3\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu Set kernel index: 3\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu Set kernel index: 3\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer3/layer3.0/conv2/Conv Set kernel index: 3\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu Set kernel index: 5\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu Set kernel index: 3\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu Set kernel index: 3\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu Set kernel index: 3\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer4/layer4.0/conv2/Conv Set kernel index: 3\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu Set kernel index: 5\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu Set kernel index: 6\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu Set kernel index: 7\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /global_pool/pool/GlobalAveragePool Set kernel index: 8\n",
      "[07/26/2023-05:38:01] [V] [TRT] Finalize: /fc/Gemm Set kernel index: 9\n",
      "[07/26/2023-05:38:01] [V] [TRT] Total number of generated kernels selected for the engine: 10\n",
      "[07/26/2023-05:38:01] [V] [TRT] Kernel: 0 CASK_STATIC\n",
      "[07/26/2023-05:38:01] [V] [TRT] Kernel: 1 CASK_STATIC\n",
      "[07/26/2023-05:38:01] [V] [TRT] Kernel: 2 CASK_STATIC\n",
      "[07/26/2023-05:38:01] [V] [TRT] Kernel: 3 CASK_STATIC\n",
      "[07/26/2023-05:38:01] [V] [TRT] Kernel: 4 CASK_STATIC\n",
      "[07/26/2023-05:38:01] [V] [TRT] Kernel: 5 CASK_STATIC\n",
      "[07/26/2023-05:38:01] [V] [TRT] Kernel: 6 CASK_STATIC\n",
      "[07/26/2023-05:38:01] [V] [TRT] Kernel: 7 CASK_STATIC\n",
      "[07/26/2023-05:38:01] [V] [TRT] Kernel: 8 CASK_STATIC\n",
      "[07/26/2023-05:38:01] [V] [TRT] Kernel: 9 CASK_STATIC\n",
      "[07/26/2023-05:38:01] [V] [TRT] Disabling unused tactic source: JIT_CONVOLUTIONS\n",
      "[07/26/2023-05:38:01] [V] [TRT] Engine generation completed in 13.1999 seconds.\n",
      "[07/26/2023-05:38:01] [V] [TRT] Deleting timing cache: 156 entries, served 180 hits since creation.\n",
      "[07/26/2023-05:38:01] [V] [TRT] Engine Layer Information:\n",
      "Layer(Constant): onnx::Conv_239, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 0) [Constant]_output (Float[512])\n",
      "Layer(Constant): onnx::Conv_239_clone_1, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 0) [Constant]_output_clone_1 (Float[512])\n",
      "Layer(Constant): onnx::Conv_239_clone_2, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 0) [Constant]_output_clone_2 (Float[512])\n",
      "Layer(Constant): onnx::Conv_239_clone_3, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 0) [Constant]_output_clone_3 (Float[512])\n",
      "Layer(Constant): onnx::Conv_241, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 2) [Constant]_output (Float[512,512,3,3])\n",
      "Layer(Constant): onnx::Conv_224, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 7) [Constant]_output (Float[256])\n",
      "Layer(Constant): onnx::Conv_224_clone_1, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 7) [Constant]_output_clone_1 (Float[256])\n",
      "Layer(Constant): onnx::Conv_224_clone_2, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 7) [Constant]_output_clone_2 (Float[256])\n",
      "Layer(Constant): onnx::Conv_224_clone_3, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 7) [Constant]_output_clone_3 (Float[256])\n",
      "Layer(Constant): onnx::Conv_226, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 9) [Constant]_output (Float[256,256,3,3])\n",
      "Layer(Constant): onnx::Conv_209, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 14) [Constant]_output (Float[128])\n",
      "Layer(Constant): onnx::Conv_209_clone_1, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 14) [Constant]_output_clone_1 (Float[128])\n",
      "Layer(Constant): onnx::Conv_209_clone_2, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 14) [Constant]_output_clone_2 (Float[128])\n",
      "Layer(Constant): onnx::Conv_209_clone_3, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 14) [Constant]_output_clone_3 (Float[128])\n",
      "Layer(Constant): onnx::Conv_211, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 16) [Constant]_output (Float[128,128,3,3])\n",
      "Layer(Constant): onnx::Conv_194, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 21) [Constant]_output (Float[64])\n",
      "Layer(Constant): onnx::Conv_194_clone_1, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 21) [Constant]_output_clone_1 (Float[64])\n",
      "Layer(Constant): onnx::Conv_194_clone_2, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 21) [Constant]_output_clone_2 (Float[64])\n",
      "Layer(Constant): onnx::Conv_194_clone_3, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 21) [Constant]_output_clone_3 (Float[64])\n",
      "Layer(Constant): onnx::Conv_199, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 23) [Constant]_output (Float[64,64,3,3])\n",
      "Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to /conv1/Conv + /act1/Relu, Tactic: 0x00000000000003ea, x (Float[-1,3,224,224]) -> Reformatted Input Tensor 0 to /conv1/Conv + /act1/Relu (Float[-1,3:4,224,224])\n",
      "Layer(CaskConvolution): /conv1/Conv + /act1/Relu, Tactic: 0x9cb304e2edbc1221, Reformatted Input Tensor 0 to /conv1/Conv + /act1/Relu (Float[-1,3:4,224,224]) -> /act1/Relu_output_0 (Float[-1,64:4,112,112])\n",
      "Layer(Reformat): Identity_0, Tactic: 0x00000000000003e8, (Unnamed Layer* 0) [Constant]_output (Float[512]) -> onnx::Conv_251 (Float[512])\n",
      "Layer(Reformat): Identity_2, Tactic: 0x00000000000003e8, (Unnamed Layer* 0) [Constant]_output_clone_1 (Float[512]) -> onnx::Conv_248 (Float[512])\n",
      "Layer(Reformat): Identity_3, Tactic: 0x00000000000003e8, (Unnamed Layer* 0) [Constant]_output_clone_2 (Float[512]) -> onnx::Conv_245 (Float[512])\n",
      "Layer(Reformat): Identity_4, Tactic: 0x00000000000003e8, (Unnamed Layer* 0) [Constant]_output_clone_3 (Float[512]) -> onnx::Conv_242 (Float[512])\n",
      "Layer(MyelinReformat): Identity_1, Tactic: 0x0000000000000000, (Unnamed Layer* 2) [Constant]_output (Float[512,512,3,3]) -> onnx::Conv_250 (Float[512,512,3,3])\n",
      "Layer(Reformat): Identity_5, Tactic: 0x00000000000003e8, (Unnamed Layer* 7) [Constant]_output (Float[256]) -> onnx::Conv_236 (Float[256])\n",
      "Layer(Reformat): Identity_7, Tactic: 0x00000000000003e8, (Unnamed Layer* 7) [Constant]_output_clone_1 (Float[256]) -> onnx::Conv_233 (Float[256])\n",
      "Layer(Reformat): Identity_8, Tactic: 0x00000000000003e8, (Unnamed Layer* 7) [Constant]_output_clone_2 (Float[256]) -> onnx::Conv_230 (Float[256])\n",
      "Layer(Reformat): Identity_9, Tactic: 0x00000000000003e8, (Unnamed Layer* 7) [Constant]_output_clone_3 (Float[256]) -> onnx::Conv_227 (Float[256])\n",
      "Layer(Reformat): Identity_6, Tactic: 0x00000000000003e8, (Unnamed Layer* 9) [Constant]_output (Float[256,256,3,3]) -> onnx::Conv_235 (Float[256,256,3,3])\n",
      "Layer(Reformat): Identity_10, Tactic: 0x00000000000003e8, (Unnamed Layer* 14) [Constant]_output (Float[128]) -> onnx::Conv_221 (Float[128])\n",
      "Layer(Reformat): Identity_12, Tactic: 0x00000000000003e8, (Unnamed Layer* 14) [Constant]_output_clone_1 (Float[128]) -> onnx::Conv_218 (Float[128])\n",
      "Layer(Reformat): Identity_13, Tactic: 0x00000000000003e8, (Unnamed Layer* 14) [Constant]_output_clone_2 (Float[128]) -> onnx::Conv_215 (Float[128])\n",
      "Layer(Reformat): Identity_14, Tactic: 0x00000000000003e8, (Unnamed Layer* 14) [Constant]_output_clone_3 (Float[128]) -> onnx::Conv_212 (Float[128])\n",
      "Layer(Reformat): Identity_11, Tactic: 0x00000000000003e8, (Unnamed Layer* 16) [Constant]_output (Float[128,128,3,3]) -> onnx::Conv_220 (Float[128,128,3,3])\n",
      "Layer(Reformat): Identity_15, Tactic: 0x00000000000003e8, (Unnamed Layer* 21) [Constant]_output (Float[64]) -> onnx::Conv_206 (Float[64])\n",
      "Layer(Reformat): Identity_17, Tactic: 0x00000000000003e8, (Unnamed Layer* 21) [Constant]_output_clone_1 (Float[64]) -> onnx::Conv_203 (Float[64])\n",
      "Layer(Reformat): Identity_18, Tactic: 0x00000000000003e8, (Unnamed Layer* 21) [Constant]_output_clone_2 (Float[64]) -> onnx::Conv_200 (Float[64])\n",
      "Layer(Reformat): Identity_19, Tactic: 0x00000000000003e8, (Unnamed Layer* 21) [Constant]_output_clone_3 (Float[64]) -> onnx::Conv_197 (Float[64])\n",
      "Layer(Reformat): Identity_16, Tactic: 0x00000000000003e8, (Unnamed Layer* 23) [Constant]_output (Float[64,64,3,3]) -> onnx::Conv_205 (Float[64,64,3,3])\n",
      "Layer(CaskPooling): /maxpool/MaxPool, Tactic: 0x789b2859f2e03e79, /act1/Relu_output_0 (Float[-1,64:4,112,112]) -> /maxpool/MaxPool_output_0 (Float[-1,64:4,56,56])\n",
      "Layer(CaskConvolution): /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu, Tactic: 0x3a8712b17741b582, /maxpool/MaxPool_output_0 (Float[-1,64:4,56,56]), onnx::Conv_197 (Float[64]) -> /layer1/layer1.0/act1/Relu_output_0 (Float[-1,64:4,56,56])\n",
      "Layer(CaskConvolution): /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu, Tactic: 0x3a8712b17741b582, /layer1/layer1.0/act1/Relu_output_0 (Float[-1,64:4,56,56]), onnx::Conv_200 (Float[64]), /maxpool/MaxPool_output_0 (Float[-1,64:4,56,56]) -> /layer1/layer1.0/act2/Relu_output_0 (Float[-1,64:4,56,56])\n",
      "Layer(CaskConvolution): /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/act1/Relu, Tactic: 0x3a8712b17741b582, /layer1/layer1.0/act2/Relu_output_0 (Float[-1,64:4,56,56]), onnx::Conv_203 (Float[64]) -> /layer1/layer1.1/act1/Relu_output_0 (Float[-1,64:4,56,56])\n",
      "Layer(CaskConvolution): /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu, Tactic: 0x3a8712b17741b582, /layer1/layer1.1/act1/Relu_output_0 (Float[-1,64:4,56,56]), onnx::Conv_205 (Float[64,64,3,3]), onnx::Conv_206 (Float[64]), /layer1/layer1.0/act2/Relu_output_0 (Float[-1,64:4,56,56]) -> /layer1/layer1.1/act2/Relu_output_0 (Float[-1,64:4,56,56])\n",
      "Layer(CaskConvolution): /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu, Tactic: 0x3a8712b17741b582, /layer1/layer1.1/act2/Relu_output_0 (Float[-1,64:4,56,56]) -> /layer2/layer2.0/act1/Relu_output_0 (Float[-1,128:4,28,28])\n",
      "Layer(CaskConvolution): /layer2/layer2.0/conv2/Conv, Tactic: 0x999e005e3b016ea6, /layer2/layer2.0/act1/Relu_output_0 (Float[-1,128:4,28,28]), onnx::Conv_212 (Float[128]) -> /layer2/layer2.0/conv2/Conv_output_0 (Float[-1,128:4,28,28])\n",
      "Layer(CaskConvolution): /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu, Tactic: 0x72a5d05b1bb165ef, /layer1/layer1.1/act2/Relu_output_0 (Float[-1,64:4,56,56]), onnx::Conv_215 (Float[128]), /layer2/layer2.0/conv2/Conv_output_0 (Float[-1,128:4,28,28]) -> /layer2/layer2.0/act2/Relu_output_0 (Float[-1,128:4,28,28])\n",
      "Layer(CaskConvolution): /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu, Tactic: 0x999e005e3b016ea6, /layer2/layer2.0/act2/Relu_output_0 (Float[-1,128:4,28,28]), onnx::Conv_218 (Float[128]) -> /layer2/layer2.1/act1/Relu_output_0 (Float[-1,128:4,28,28])\n",
      "Layer(CaskConvolution): /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu, Tactic: 0x999e005e3b016ea6, /layer2/layer2.1/act1/Relu_output_0 (Float[-1,128:4,28,28]), onnx::Conv_220 (Float[128,128,3,3]), onnx::Conv_221 (Float[128]), /layer2/layer2.0/act2/Relu_output_0 (Float[-1,128:4,28,28]) -> /layer2/layer2.1/act2/Relu_output_0 (Float[-1,128:4,28,28])\n",
      "Layer(CaskConvolution): /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu, Tactic: 0x999e005e3b016ea6, /layer2/layer2.1/act2/Relu_output_0 (Float[-1,128:4,28,28]) -> /layer3/layer3.0/act1/Relu_output_0 (Float[-1,256:4,14,14])\n",
      "Layer(CaskConvolution): /layer3/layer3.0/conv2/Conv, Tactic: 0x999e005e3b016ea6, /layer3/layer3.0/act1/Relu_output_0 (Float[-1,256:4,14,14]), onnx::Conv_227 (Float[256]) -> /layer3/layer3.0/conv2/Conv_output_0 (Float[-1,256:4,14,14])\n",
      "Layer(CaskConvolution): /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu, Tactic: 0x130df49cb195156b, /layer2/layer2.1/act2/Relu_output_0 (Float[-1,128:4,28,28]), onnx::Conv_230 (Float[256]), /layer3/layer3.0/conv2/Conv_output_0 (Float[-1,256:4,14,14]) -> /layer3/layer3.0/act2/Relu_output_0 (Float[-1,256:4,14,14])\n",
      "Layer(CaskConvolution): /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu, Tactic: 0x999e005e3b016ea6, /layer3/layer3.0/act2/Relu_output_0 (Float[-1,256:4,14,14]), onnx::Conv_233 (Float[256]) -> /layer3/layer3.1/act1/Relu_output_0 (Float[-1,256:4,14,14])\n",
      "Layer(CaskConvolution): /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu, Tactic: 0x999e005e3b016ea6, /layer3/layer3.1/act1/Relu_output_0 (Float[-1,256:4,14,14]), onnx::Conv_235 (Float[256,256,3,3]), onnx::Conv_236 (Float[256]), /layer3/layer3.0/act2/Relu_output_0 (Float[-1,256:4,14,14]) -> /layer3/layer3.1/act2/Relu_output_0 (Float[-1,256:4,14,14])\n",
      "Layer(CaskConvolution): /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu, Tactic: 0x999e005e3b016ea6, /layer3/layer3.1/act2/Relu_output_0 (Float[-1,256:4,14,14]) -> /layer4/layer4.0/act1/Relu_output_0 (Float[-1,512:4,7,7])\n",
      "Layer(CaskConvolution): /layer4/layer4.0/conv2/Conv, Tactic: 0x999e005e3b016ea6, /layer4/layer4.0/act1/Relu_output_0 (Float[-1,512:4,7,7]), onnx::Conv_242 (Float[512]) -> /layer4/layer4.0/conv2/Conv_output_0 (Float[-1,512:4,7,7])\n",
      "Layer(CaskConvolution): /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu, Tactic: 0x130df49cb195156b, /layer3/layer3.1/act2/Relu_output_0 (Float[-1,256:4,14,14]), onnx::Conv_245 (Float[512]), /layer4/layer4.0/conv2/Conv_output_0 (Float[-1,512:4,7,7]) -> Reformatted Output Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (Float[-1,512:4,7,7])\n",
      "Layer(NoOp): Reformatting CopyNode for Output Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (Float[-1,512:4,7,7]) -> /layer4/layer4.0/act2/Relu_output_0 (Float[-1,512,7,7])\n",
      "Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu, Tactic: 0x0000000000000000, /layer4/layer4.0/act2/Relu_output_0 (Float[-1,512,7,7]) -> Reformatted Input Tensor 0 to /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (Float[-1,512:4,7,7])\n",
      "Layer(CaskConvolution): /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu, Tactic: 0x1323e48791e2f671, Reformatted Input Tensor 0 to /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (Float[-1,512:4,7,7]), onnx::Conv_248 (Float[512]) -> /layer4/layer4.1/act1/Relu_output_0 (Float[-1,512,7,7])\n",
      "Layer(Reformat): Reformatting CopyNode for Input Tensor 3 to /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu, Tactic: 0x0000000000000000, /layer4/layer4.0/act2/Relu_output_0 (Float[-1,512,7,7]) -> Reformatted Input Tensor 3 to /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (Float[-1,512,7,7])\n",
      "Layer(CaskConvolution): /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu, Tactic: 0x5aa723e0481da855, /layer4/layer4.1/act1/Relu_output_0 (Float[-1,512,7,7]), onnx::Conv_250 (Float[512,512,3,3]), onnx::Conv_251 (Float[512]), Reformatted Input Tensor 3 to /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (Float[-1,512,7,7]) -> /layer4/layer4.1/act2/Relu_output_0 (Float[-1,512,7,7])\n",
      "Layer(CaskPooling): /global_pool/pool/GlobalAveragePool, Tactic: 0x933eceba7b866d59, /layer4/layer4.1/act2/Relu_output_0 (Float[-1,512,7,7]) -> /global_pool/pool/GlobalAveragePool_output_0 (Float[-1,512,1,1])\n",
      "Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to /fc/Gemm, Tactic: 0x0000000000000000, /global_pool/pool/GlobalAveragePool_output_0 (Float[-1,512,1,1]) -> Reformatted Input Tensor 0 to /fc/Gemm (Float[-1,512:4,1,1])\n",
      "Layer(CaskGemmConvolution): /fc/Gemm, Tactic: 0x00000000000203be, Reformatted Input Tensor 0 to /fc/Gemm (Float[-1,512:4,1,1]) -> /fc/Gemm_out_tensor (Float[-1,1000:4,1,1])\n",
      "Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to reshape_after_/fc/Gemm, Tactic: 0x0000000000000000, /fc/Gemm_out_tensor (Float[-1,1000:4,1,1]) -> Reformatted Input Tensor 0 to reshape_after_/fc/Gemm (Float[-1,1000,1,1])\n",
      "Layer(NoOp): reshape_after_/fc/Gemm, Tactic: 0x0000000000000000, Reformatted Input Tensor 0 to reshape_after_/fc/Gemm (Float[-1,1000,1,1]) -> outputs (Float[-1,1000])\n",
      "[07/26/2023-05:38:01] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +45, now: CPU 0, GPU 45 (MiB)\n",
      "[07/26/2023-05:38:01] [V] [TRT] Adding 1 engine(s) to plan file.\n",
      "[07/26/2023-05:38:01] [I] Engine built in 24.7066 sec.\n",
      "[07/26/2023-05:38:02] [I] [TRT] Loaded engine size: 45 MiB\n",
      "[07/26/2023-05:38:02] [V] [TRT] Deserialization required 24208 microseconds.\n",
      "[07/26/2023-05:38:02] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +44, now: CPU 0, GPU 44 (MiB)\n",
      "[07/26/2023-05:38:02] [I] Engine deserialized in 0.0257067 sec.\n",
      "[07/26/2023-05:38:02] [V] [TRT] Total per-runner device persistent memory is 0\n",
      "[07/26/2023-05:38:02] [V] [TRT] Total per-runner host persistent memory is 123040\n",
      "[07/26/2023-05:38:02] [V] [TRT] Allocated activation device memory of size 169049088\n",
      "[07/26/2023-05:38:02] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +161, now: CPU 0, GPU 205 (MiB)\n",
      "[07/26/2023-05:38:02] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "[07/26/2023-05:38:02] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[07/26/2023-05:38:02] [V] Using enqueueV3.\n",
      "[07/26/2023-05:38:02] [I] Using random values for input x\n",
      "[07/26/2023-05:38:02] [I] Input binding for x with dimensions 16x3x224x224 is created.\n",
      "[07/26/2023-05:38:02] [I] Output binding for outputs with dimensions 16x1000 is created.\n",
      "[07/26/2023-05:38:02] [I] Starting inference\n",
      "[07/26/2023-05:38:05] [I] Warmup completed 33 queries over 200 ms\n",
      "[07/26/2023-05:38:05] [I] Timing trace has 504 queries over 3.01368 s\n",
      "[07/26/2023-05:38:05] [I] \n",
      "[07/26/2023-05:38:05] [I] === Trace details ===\n",
      "[07/26/2023-05:38:05] [I] Trace averages of 10 runs:\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95374 ms - Host latency: 6.7521 ms (enqueue 0.0123718 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95344 ms - Host latency: 6.75466 ms (enqueue 0.0123688 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95312 ms - Host latency: 6.75583 ms (enqueue 0.0116974 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95506 ms - Host latency: 6.75411 ms (enqueue 0.010614 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95404 ms - Host latency: 6.75999 ms (enqueue 0.0204254 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95558 ms - Host latency: 6.75854 ms (enqueue 0.0119812 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95332 ms - Host latency: 6.75596 ms (enqueue 0.0163391 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95457 ms - Host latency: 6.75546 ms (enqueue 0.0169189 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95477 ms - Host latency: 6.75622 ms (enqueue 0.0273071 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95527 ms - Host latency: 6.75654 ms (enqueue 0.0374207 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95365 ms - Host latency: 6.7522 ms (enqueue 0.0270935 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.9561 ms - Host latency: 6.75683 ms (enqueue 0.0229187 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.9558 ms - Host latency: 6.75811 ms (enqueue 0.0155457 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95497 ms - Host latency: 6.7613 ms (enqueue 0.0185974 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95283 ms - Host latency: 6.75653 ms (enqueue 0.0230225 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95446 ms - Host latency: 6.75765 ms (enqueue 0.0178467 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95413 ms - Host latency: 6.75302 ms (enqueue 0.0122559 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95435 ms - Host latency: 6.75555 ms (enqueue 0.0198608 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95536 ms - Host latency: 6.75458 ms (enqueue 0.0204468 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95773 ms - Host latency: 6.75574 ms (enqueue 0.0120728 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95459 ms - Host latency: 6.75083 ms (enqueue 0.0104736 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95525 ms - Host latency: 6.75802 ms (enqueue 0.017749 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.9554 ms - Host latency: 6.76177 ms (enqueue 0.0209595 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95476 ms - Host latency: 6.76003 ms (enqueue 0.0247314 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95486 ms - Host latency: 6.75774 ms (enqueue 0.0206177 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95323 ms - Host latency: 6.75514 ms (enqueue 0.0244751 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95375 ms - Host latency: 6.75592 ms (enqueue 0.0157959 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95365 ms - Host latency: 6.75099 ms (enqueue 0.0155762 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95569 ms - Host latency: 6.7547 ms (enqueue 0.0158569 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95569 ms - Host latency: 6.75754 ms (enqueue 0.0161743 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95448 ms - Host latency: 6.75979 ms (enqueue 0.0196655 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.97439 ms - Host latency: 6.77041 ms (enqueue 0.0179687 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95508 ms - Host latency: 6.75344 ms (enqueue 0.02229 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.9554 ms - Host latency: 6.75491 ms (enqueue 0.0249512 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95466 ms - Host latency: 6.75623 ms (enqueue 0.0313477 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95386 ms - Host latency: 6.75474 ms (enqueue 0.0258545 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95452 ms - Host latency: 6.75527 ms (enqueue 0.0352051 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95437 ms - Host latency: 6.75596 ms (enqueue 0.0297363 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95388 ms - Host latency: 6.74941 ms (enqueue 0.0187988 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95593 ms - Host latency: 6.75247 ms (enqueue 0.0173828 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95657 ms - Host latency: 6.75398 ms (enqueue 0.0183838 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95391 ms - Host latency: 6.75244 ms (enqueue 0.0206787 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95657 ms - Host latency: 6.75654 ms (enqueue 0.0294678 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95422 ms - Host latency: 6.75608 ms (enqueue 0.0250732 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.9552 ms - Host latency: 6.75381 ms (enqueue 0.0227051 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95415 ms - Host latency: 6.75857 ms (enqueue 0.0280762 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95393 ms - Host latency: 6.75344 ms (enqueue 0.0282227 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95273 ms - Host latency: 6.75415 ms (enqueue 0.0234863 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95405 ms - Host latency: 6.75664 ms (enqueue 0.0302979 ms)\n",
      "[07/26/2023-05:38:05] [I] Average on 10 runs - GPU latency: 5.95552 ms - Host latency: 6.75859 ms (enqueue 0.0285645 ms)\n",
      "[07/26/2023-05:38:05] [I] \n",
      "[07/26/2023-05:38:05] [I] === Performance summary ===\n",
      "[07/26/2023-05:38:05] [I] Throughput: 167.237 qps\n",
      "[07/26/2023-05:38:05] [I] Latency: min = 6.73999 ms, max = 6.94067 ms, mean = 6.75595 ms, median = 6.75427 ms, percentile(90%) = 6.76538 ms, percentile(95%) = 6.76965 ms, percentile(99%) = 6.77808 ms\n",
      "[07/26/2023-05:38:05] [I] Enqueue Time: min = 0.0083313 ms, max = 0.119873 ms, mean = 0.020884 ms, median = 0.0181427 ms, percentile(90%) = 0.0349121 ms, percentile(95%) = 0.0419922 ms, percentile(99%) = 0.0593262 ms\n",
      "[07/26/2023-05:38:05] [I] H2D Latency: min = 0.783203 ms, max = 0.828003 ms, mean = 0.790775 ms, median = 0.788818 ms, percentile(90%) = 0.800415 ms, percentile(95%) = 0.803467 ms, percentile(99%) = 0.809937 ms\n",
      "[07/26/2023-05:38:05] [I] GPU Compute Time: min = 5.94226 ms, max = 6.14722 ms, mean = 5.95503 ms, median = 5.95459 ms, percentile(90%) = 5.95862 ms, percentile(95%) = 5.95966 ms, percentile(99%) = 5.96274 ms\n",
      "[07/26/2023-05:38:05] [I] D2H Latency: min = 0.00805664 ms, max = 0.0158691 ms, mean = 0.0101365 ms, median = 0.0101471 ms, percentile(90%) = 0.0113525 ms, percentile(95%) = 0.0117188 ms, percentile(99%) = 0.0128174 ms\n",
      "[07/26/2023-05:38:05] [I] Total Host Walltime: 3.01368 s\n",
      "[07/26/2023-05:38:05] [I] Total GPU Compute Time: 3.00134 s\n",
      "[07/26/2023-05:38:05] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[07/26/2023-05:38:05] [V] \n",
      "[07/26/2023-05:38:05] [V] === Explanations of the performance metrics ===\n",
      "[07/26/2023-05:38:05] [V] Total Host Walltime: the host walltime from when the first query (after warmups) is enqueued to when the last query is completed.\n",
      "[07/26/2023-05:38:05] [V] GPU Compute Time: the GPU latency to execute the kernels for a query.\n",
      "[07/26/2023-05:38:05] [V] Total GPU Compute Time: the summation of the GPU Compute Time of all the queries. If this is significantly shorter than Total Host Walltime, the GPU may be under-utilized because of host-side overheads or data transfers.\n",
      "[07/26/2023-05:38:05] [V] Throughput: the observed throughput computed by dividing the number of queries by the Total Host Walltime. If this is significantly lower than the reciprocal of GPU Compute Time, the GPU may be under-utilized because of host-side overheads or data transfers.\n",
      "[07/26/2023-05:38:05] [V] Enqueue Time: the host latency to enqueue a query. If this is longer than GPU Compute Time, the GPU may be under-utilized.\n",
      "[07/26/2023-05:38:05] [V] H2D Latency: the latency for host-to-device data transfers for input tensors of a single query.\n",
      "[07/26/2023-05:38:05] [V] D2H Latency: the latency for device-to-host data transfers for output tensors of a single query.\n",
      "[07/26/2023-05:38:05] [V] Latency: the summation of H2D Latency, GPU Compute Time, and D2H Latency. This is the latency to infer a single query.\n",
      "[07/26/2023-05:38:05] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8601] # /opt/tensorrt/bin/trtexec --onnx=resnet18.onnx --minShapes=x:1x3x224x224 --optShapes=x:16x3x224x224 --maxShapes=x:32x3x224x224 --useCudaGraph --saveEngine=resnet18.plan --verbose=true\n"
     ]
    }
   ],
   "source": [
    "# change onnx model to tensorrt using trtexec\n",
    "\n",
    "! /opt/tensorrt/bin/trtexec --onnx=resnet18.onnx --minShapes=x:1x3x224x224 --optShapes=x:16x3x224x224 --maxShapes=x:32x3x224x224 --useCudaGraph --saveEngine=resnet18.plan --verbose=true \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_313578/2132844210.py:63: DeprecationWarning: Use set_input_shape instead.\n",
      "  context.set_binding_shape(0, in_shape) # Need to specify binding shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/26/2023-05:39:01] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[07/26/2023-05:39:01] [TRT] [I] Loaded engine size: 45 MiB\n",
      "[07/26/2023-05:39:01] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +45, now: CPU 0, GPU 250 (MiB)\n",
      "[07/26/2023-05:39:01] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +161, now: CPU 0, GPU 411 (MiB)\n",
      "[07/26/2023-05:39:01] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "{'x': {'index': 0, 'name': 'x', 'dtype': dtype('float32'), 'shape': [16, 3, 224, 224], 'allocation': 139919265628160}}\n",
      "{'outputs': {'index': 1, 'name': 'outputs', 'dtype': dtype('float32'), 'shape': [16, 1000], 'allocation': 139927063177216}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 21.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# type: ignore\n",
    "from tensorrt_handson_lab.tensorrt_utils import common\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "import tensorrt as trt\n",
    "from cuda import cudart\n",
    "import time\n",
    "\n",
    "def infer(input_bindings, output_bindings, context, batch: Dict[str, np.ndarray]):\n",
    "    # Copy given input to device memory (GPU memory)\n",
    "    \n",
    "    st = time.time()\n",
    "\n",
    "    allocations = []\n",
    "    for k, bindings in input_bindings.items():\n",
    "        allocations.append(bindings[\"allocation\"])\n",
    "\n",
    "    for k, bindings in output_bindings.items():\n",
    "        allocations.append(bindings[\"allocation\"])\n",
    "\n",
    "    for k, val in batch.items():\n",
    "        if input_bindings[k][\"shape\"][0] > val.shape[0]:\n",
    "            padded = np.zeros(dtype=input_bindings[k][\"dtype\"], shape=input_bindings[k][\"shape\"])\n",
    "            padded[: len(val)] = val\n",
    "        common.memcpy_host_to_device(\n",
    "            input_bindings[k][\"allocation\"],\n",
    "            np.ascontiguousarray(val.astype(input_bindings[k][\"dtype\"])),\n",
    "        )\n",
    "    \n",
    "    # execute model with tensorrt runtime\n",
    "    context.execute_v2(allocations)\n",
    "\n",
    "    # prepare host memory\n",
    "    output_dict = {}\n",
    "    for k, ob in output_bindings.items():\n",
    "        host_output = np.zeros(dtype=ob[\"dtype\"], shape=ob[\"shape\"])\n",
    "        common.memcpy_device_to_host(host_output, ob[\"allocation\"])\n",
    "        output_dict[k] = host_output\n",
    "        \n",
    "\n",
    "    cost = time.time() - st\n",
    "    return output_dict, cost\n",
    "\n",
    "torch_load_time = 0\n",
    "trt_load_time = 0\n",
    "\n",
    "with open(\"resnet18.plan\", \"rb\")as f, trt.Runtime(trt.Logger(trt.Logger.INFO)) as runtime:\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "in_name = engine.get_tensor_name(0)\n",
    "in_dtype = np.dtype(trt.nptype(engine.get_tensor_dtype(in_name)))\n",
    "in_shape = list((B,) + IN_SHAPE)\n",
    "in_size = in_dtype.itemsize * np.prod(in_shape)\n",
    "input_bindings = {\n",
    "    in_name : {\n",
    "        \"index\" : 0,\n",
    "        \"name\" : in_name,\n",
    "        \"dtype\" : in_dtype,\n",
    "        \"shape\" : in_shape,\n",
    "        \"allocation\" : common.cuda_call(cudart.cudaMalloc(in_size))\n",
    "    }\n",
    "}\n",
    "context.set_binding_shape(0, in_shape) # Need to specify binding shape\n",
    "\n",
    "out_name = engine.get_tensor_name(1)\n",
    "out_dtype = np.dtype(trt.nptype(engine.get_tensor_dtype(out_name)))\n",
    "out_shape = list((B,) + OUT_SHAPE)\n",
    "out_size = out_dtype.itemsize * np.prod(out_shape)\n",
    "output_bindings = {\n",
    "    out_name : {\n",
    "        \"index\" : 1,\n",
    "        \"name\" : out_name,\n",
    "        \"dtype\" : out_dtype,\n",
    "        \"shape\" : out_shape,\n",
    "        \"allocation\" : common.cuda_call(cudart.cudaMalloc(out_size))\n",
    "    }\n",
    "}\n",
    "\n",
    "print(input_bindings)\n",
    "print(output_bindings)\n",
    "\n",
    "model.cpu()\n",
    "st = time.time()\n",
    "model = model.cuda().eval()\n",
    "trt_load_time = time.time() - st\n",
    "\n",
    "mean_diff = 0\n",
    "torch_cost = 0\n",
    "trt_cost = 0\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(NUM_TEST)):\n",
    "        st = time.time()\n",
    "        input_dict = {\"x\" : torch.randn(*((B,) + IN_SHAPE)).cuda()}\n",
    "        torch_output = model(**input_dict)\n",
    "        torch_cost += time.time() - st\n",
    "        trt_output, trt_cost_batch = infer(\n",
    "            input_bindings=input_bindings,\n",
    "            output_bindings=output_bindings, \n",
    "            context=context,\n",
    "            batch={k:v.cpu().numpy() for k,v in input_dict.items()})\n",
    "        trt_cost += trt_cost_batch\n",
    "        mean_diff += (torch_output.cpu() - torch.from_numpy(trt_output[\"outputs\"])).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0190)\n",
      "0.025162768363952637 0.011265873908996582\n"
     ]
    }
   ],
   "source": [
    "print(mean_diff / NUM_TEST)\n",
    "\n",
    "print(torch_cost / NUM_TEST, trt_cost /  NUM_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
