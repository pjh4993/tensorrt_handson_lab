{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/tensorrt/bin/trtexec\n"
     ]
    }
   ],
   "source": [
    "# Assert that you are running on tensorrt container\n",
    "! which trtexec\n",
    "# If you want to check trtexec running options, run trtexec -h \n",
    "# ! trtexec -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x)\n",
      "Exported graph: graph(%x : Float(*, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cpu),\n",
      "      %fc.weight : Float(1000, 512, strides=[512, 1], requires_grad=1, device=cpu),\n",
      "      %fc.bias : Float(1000, strides=[1], requires_grad=1, device=cpu),\n",
      "      %onnx::Conv_193 : Float(64, 3, 7, 7, strides=[147, 49, 7, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_194 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_196 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_199 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_202 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_208 : Float(128, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_209 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_211 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_214 : Float(128, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_217 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_223 : Float(256, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_224 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_226 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_229 : Float(256, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_232 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_238 : Float(512, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_239 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_241 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_244 : Float(512, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_247 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu)):\n",
      "  %onnx::Conv_251 : Float(512, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_239)\n",
      "  %onnx::Conv_250 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_241)\n",
      "  %onnx::Conv_248 : Float(512, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_239)\n",
      "  %onnx::Conv_245 : Float(512, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_239)\n",
      "  %onnx::Conv_242 : Float(512, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_239)\n",
      "  %onnx::Conv_236 : Float(256, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_224)\n",
      "  %onnx::Conv_235 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_226)\n",
      "  %onnx::Conv_233 : Float(256, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_224)\n",
      "  %onnx::Conv_230 : Float(256, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_224)\n",
      "  %onnx::Conv_227 : Float(256, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_224)\n",
      "  %onnx::Conv_221 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_209)\n",
      "  %onnx::Conv_220 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_211)\n",
      "  %onnx::Conv_218 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_209)\n",
      "  %onnx::Conv_215 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_209)\n",
      "  %onnx::Conv_212 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_209)\n",
      "  %onnx::Conv_206 : Float(64, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_194)\n",
      "  %onnx::Conv_205 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_199)\n",
      "  %onnx::Conv_203 : Float(64, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_194)\n",
      "  %onnx::Conv_200 : Float(64, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_194)\n",
      "  %onnx::Conv_197 : Float(64, strides=[1], requires_grad=0, device=cpu) = onnx::Identity(%onnx::Conv_194)\n",
      "  %/conv1/Conv_output_0 : Float(*, 64, 112, 112, strides=[802816, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[7, 7], pads=[3, 3, 3, 3], strides=[2, 2], onnx_name=\"/conv1/Conv\"](%x, %onnx::Conv_193, %onnx::Conv_194), scope: timm.models.resnet.ResNet::/torch.nn.modules.conv.Conv2d::conv1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/act1/Relu_output_0 : Float(*, 64, 112, 112, strides=[802816, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/act1/Relu\"](%/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.activation.ReLU::act1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/maxpool/MaxPool_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/maxpool/MaxPool\"](%/act1/Relu_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.pooling.MaxPool2d::maxpool # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:782:0\n",
      "  %/layer1/layer1.0/conv1/Conv_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer1/layer1.0/conv1/Conv\"](%/maxpool/MaxPool_output_0, %onnx::Conv_196, %onnx::Conv_197), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.0/torch.nn.modules.conv.Conv2d::conv1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer1/layer1.0/act1/Relu_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer1/layer1.0/act1/Relu\"](%/layer1/layer1.0/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.0/torch.nn.modules.activation.ReLU::act1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer1/layer1.0/conv2/Conv_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer1/layer1.0/conv2/Conv\"](%/layer1/layer1.0/act1/Relu_output_0, %onnx::Conv_199, %onnx::Conv_200), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.0/torch.nn.modules.conv.Conv2d::conv2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer1/layer1.0/Add_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer1/layer1.0/Add\"](%/layer1/layer1.0/conv2/Conv_output_0, %/maxpool/MaxPool_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.0 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer1/layer1.0/act2/Relu_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer1/layer1.0/act2/Relu\"](%/layer1/layer1.0/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.0/torch.nn.modules.activation.ReLU::act2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer1/layer1.1/conv1/Conv_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer1/layer1.1/conv1/Conv\"](%/layer1/layer1.0/act2/Relu_output_0, %onnx::Conv_202, %onnx::Conv_203), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.1/torch.nn.modules.conv.Conv2d::conv1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer1/layer1.1/act1/Relu_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer1/layer1.1/act1/Relu\"](%/layer1/layer1.1/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.1/torch.nn.modules.activation.ReLU::act1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer1/layer1.1/conv2/Conv_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer1/layer1.1/conv2/Conv\"](%/layer1/layer1.1/act1/Relu_output_0, %onnx::Conv_205, %onnx::Conv_206), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.1/torch.nn.modules.conv.Conv2d::conv2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer1/layer1.1/Add_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer1/layer1.1/Add\"](%/layer1/layer1.1/conv2/Conv_output_0, %/layer1/layer1.0/act2/Relu_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer1/layer1.1/act2/Relu_output_0 : Float(*, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer1/layer1.1/act2/Relu\"](%/layer1/layer1.1/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer1/timm.models.resnet.BasicBlock::layer1.1/torch.nn.modules.activation.ReLU::act2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer2/layer2.0/conv1/Conv_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/layer2/layer2.0/conv1/Conv\"](%/layer1/layer1.1/act2/Relu_output_0, %onnx::Conv_208, %onnx::Conv_209), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0/torch.nn.modules.conv.Conv2d::conv1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer2/layer2.0/act1/Relu_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer2/layer2.0/act1/Relu\"](%/layer2/layer2.0/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0/torch.nn.modules.activation.ReLU::act1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer2/layer2.0/conv2/Conv_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer2/layer2.0/conv2/Conv\"](%/layer2/layer2.0/act1/Relu_output_0, %onnx::Conv_211, %onnx::Conv_212), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0/torch.nn.modules.conv.Conv2d::conv2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer2/layer2.0/downsample/downsample.0/Conv_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/layer2/layer2.0/downsample/downsample.0/Conv\"](%/layer1/layer1.1/act2/Relu_output_0, %onnx::Conv_214, %onnx::Conv_215), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0/torch.nn.modules.container.Sequential::downsample/torch.nn.modules.conv.Conv2d::downsample.0 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer2/layer2.0/Add_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer2/layer2.0/Add\"](%/layer2/layer2.0/conv2/Conv_output_0, %/layer2/layer2.0/downsample/downsample.0/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer2/layer2.0/act2/Relu_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer2/layer2.0/act2/Relu\"](%/layer2/layer2.0/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.0/torch.nn.modules.activation.ReLU::act2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer2/layer2.1/conv1/Conv_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer2/layer2.1/conv1/Conv\"](%/layer2/layer2.0/act2/Relu_output_0, %onnx::Conv_217, %onnx::Conv_218), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.1/torch.nn.modules.conv.Conv2d::conv1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer2/layer2.1/act1/Relu_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer2/layer2.1/act1/Relu\"](%/layer2/layer2.1/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.1/torch.nn.modules.activation.ReLU::act1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer2/layer2.1/conv2/Conv_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer2/layer2.1/conv2/Conv\"](%/layer2/layer2.1/act1/Relu_output_0, %onnx::Conv_220, %onnx::Conv_221), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.1/torch.nn.modules.conv.Conv2d::conv2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer2/layer2.1/Add_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer2/layer2.1/Add\"](%/layer2/layer2.1/conv2/Conv_output_0, %/layer2/layer2.0/act2/Relu_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer2/layer2.1/act2/Relu_output_0 : Float(*, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer2/layer2.1/act2/Relu\"](%/layer2/layer2.1/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer2/timm.models.resnet.BasicBlock::layer2.1/torch.nn.modules.activation.ReLU::act2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer3/layer3.0/conv1/Conv_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/layer3/layer3.0/conv1/Conv\"](%/layer2/layer2.1/act2/Relu_output_0, %onnx::Conv_223, %onnx::Conv_224), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0/torch.nn.modules.conv.Conv2d::conv1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer3/layer3.0/act1/Relu_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer3/layer3.0/act1/Relu\"](%/layer3/layer3.0/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0/torch.nn.modules.activation.ReLU::act1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer3/layer3.0/conv2/Conv_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer3/layer3.0/conv2/Conv\"](%/layer3/layer3.0/act1/Relu_output_0, %onnx::Conv_226, %onnx::Conv_227), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0/torch.nn.modules.conv.Conv2d::conv2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer3/layer3.0/downsample/downsample.0/Conv_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/layer3/layer3.0/downsample/downsample.0/Conv\"](%/layer2/layer2.1/act2/Relu_output_0, %onnx::Conv_229, %onnx::Conv_230), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0/torch.nn.modules.container.Sequential::downsample/torch.nn.modules.conv.Conv2d::downsample.0 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer3/layer3.0/Add_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer3/layer3.0/Add\"](%/layer3/layer3.0/conv2/Conv_output_0, %/layer3/layer3.0/downsample/downsample.0/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer3/layer3.0/act2/Relu_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer3/layer3.0/act2/Relu\"](%/layer3/layer3.0/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.0/torch.nn.modules.activation.ReLU::act2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer3/layer3.1/conv1/Conv_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer3/layer3.1/conv1/Conv\"](%/layer3/layer3.0/act2/Relu_output_0, %onnx::Conv_232, %onnx::Conv_233), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.1/torch.nn.modules.conv.Conv2d::conv1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer3/layer3.1/act1/Relu_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer3/layer3.1/act1/Relu\"](%/layer3/layer3.1/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.1/torch.nn.modules.activation.ReLU::act1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer3/layer3.1/conv2/Conv_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer3/layer3.1/conv2/Conv\"](%/layer3/layer3.1/act1/Relu_output_0, %onnx::Conv_235, %onnx::Conv_236), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.1/torch.nn.modules.conv.Conv2d::conv2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer3/layer3.1/Add_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer3/layer3.1/Add\"](%/layer3/layer3.1/conv2/Conv_output_0, %/layer3/layer3.0/act2/Relu_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer3/layer3.1/act2/Relu_output_0 : Float(*, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer3/layer3.1/act2/Relu\"](%/layer3/layer3.1/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer3/timm.models.resnet.BasicBlock::layer3.1/torch.nn.modules.activation.ReLU::act2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer4/layer4.0/conv1/Conv_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/layer4/layer4.0/conv1/Conv\"](%/layer3/layer3.1/act2/Relu_output_0, %onnx::Conv_238, %onnx::Conv_239), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0/torch.nn.modules.conv.Conv2d::conv1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer4/layer4.0/act1/Relu_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer4/layer4.0/act1/Relu\"](%/layer4/layer4.0/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0/torch.nn.modules.activation.ReLU::act1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer4/layer4.0/conv2/Conv_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer4/layer4.0/conv2/Conv\"](%/layer4/layer4.0/act1/Relu_output_0, %onnx::Conv_241, %onnx::Conv_242), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0/torch.nn.modules.conv.Conv2d::conv2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer4/layer4.0/downsample/downsample.0/Conv_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/layer4/layer4.0/downsample/downsample.0/Conv\"](%/layer3/layer3.1/act2/Relu_output_0, %onnx::Conv_244, %onnx::Conv_245), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0/torch.nn.modules.container.Sequential::downsample/torch.nn.modules.conv.Conv2d::downsample.0 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer4/layer4.0/Add_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer4/layer4.0/Add\"](%/layer4/layer4.0/conv2/Conv_output_0, %/layer4/layer4.0/downsample/downsample.0/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer4/layer4.0/act2/Relu_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer4/layer4.0/act2/Relu\"](%/layer4/layer4.0/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.0/torch.nn.modules.activation.ReLU::act2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer4/layer4.1/conv1/Conv_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer4/layer4.1/conv1/Conv\"](%/layer4/layer4.0/act2/Relu_output_0, %onnx::Conv_247, %onnx::Conv_248), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.1/torch.nn.modules.conv.Conv2d::conv1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer4/layer4.1/act1/Relu_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer4/layer4.1/act1/Relu\"](%/layer4/layer4.1/conv1/Conv_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.1/torch.nn.modules.activation.ReLU::act1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/layer4/layer4.1/conv2/Conv_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/layer4/layer4.1/conv2/Conv\"](%/layer4/layer4.1/act1/Relu_output_0, %onnx::Conv_250, %onnx::Conv_251), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.1/torch.nn.modules.conv.Conv2d::conv2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/layer4/layer4.1/Add_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer4/layer4.1/Add\"](%/layer4/layer4.1/conv2/Conv_output_0, %/layer4/layer4.0/act2/Relu_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.1 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/timm/models/resnet.py:115:0\n",
      "  %/layer4/layer4.1/act2/Relu_output_0 : Float(*, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/layer4/layer4.1/act2/Relu\"](%/layer4/layer4.1/Add_output_0), scope: timm.models.resnet.ResNet::/torch.nn.modules.container.Sequential::layer4/timm.models.resnet.BasicBlock::layer4.1/torch.nn.modules.activation.ReLU::act2 # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/global_pool/pool/GlobalAveragePool_output_0 : Float(*, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cpu) = onnx::GlobalAveragePool[onnx_name=\"/global_pool/pool/GlobalAveragePool\"](%/layer4/layer4.1/act2/Relu_output_0), scope: timm.models.resnet.ResNet::/timm.layers.adaptive_avgmax_pool.SelectAdaptivePool2d::global_pool/torch.nn.modules.pooling.AdaptiveAvgPool2d::pool # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1214:0\n",
      "  %/global_pool/flatten/Flatten_output_0 : Float(*, 512, strides=[512, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1, onnx_name=\"/global_pool/flatten/Flatten\"](%/global_pool/pool/GlobalAveragePool_output_0), scope: timm.models.resnet.ResNet::/timm.layers.adaptive_avgmax_pool.SelectAdaptivePool2d::global_pool/torch.nn.modules.flatten.Flatten::flatten # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/flatten.py:46:0\n",
      "  %outputs : Float(*, 1000, strides=[1000, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc/Gemm\"](%/global_pool/flatten/Flatten_output_0, %fc.weight, %fc.bias), scope: timm.models.resnet.ResNet::/torch.nn.modules.linear.Linear::fc # /workspace/tensorrt_handson_lab/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  return (%outputs)\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare resnet-18 model\n",
    "import timm\n",
    "import torch\n",
    "import inspect\n",
    "\n",
    "model = timm.create_model(\"resnet18\").cpu()\n",
    "\n",
    "# check model forward's input parameter name\n",
    "SHAPE = (3, 224, 224)\n",
    "signature = inspect.signature(model.forward)\n",
    "print(signature)\n",
    "\n",
    "# export model to onnx format\n",
    "# For more details, please refer to https://pytorch.org/docs/stable/onnx.html\n",
    "dummy_input = (torch.randn(*((1,)+SHAPE)).cpu(),)\n",
    "input_names = [\"x\"]\n",
    "output_names = [\"outputs\"]\n",
    "\n",
    "model = model.eval() # Need to set model to eval\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"resnet18.onnx\",\n",
    "    dynamic_axes={\n",
    "        \"x\" : {0: \"batch\"}, # To support dynamic shape on target axis\n",
    "    },\n",
    "    verbose=True,\n",
    "    input_names=input_names, # Need to be aligned with actual parameter name in forward function\n",
    "    output_names=output_names # Required to match with number of actual output \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'resnet18.onnx' at http://localhost:8080\n",
      "^C\n",
      "\n",
      "Stopping http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "# visualize model graph with netron\n",
    "! netron resnet18.onnx -b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 05:31:11.263182515 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 1525182, index: 8, mask: {9, 33, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-24 05:31:11.268547193 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 1525191, index: 17, mask: {18, 42, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-24 05:31:11.268570122 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 1525189, index: 15, mask: {16, 40, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-24 05:31:11.272535812 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 1525184, index: 10, mask: {11, 35, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-24 05:31:11.276533257 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 1525187, index: 13, mask: {14, 38, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-24 05:31:11.276541706 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 1525195, index: 21, mask: {22, 46, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-24 05:31:11.276545101 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 1525185, index: 11, mask: {12, 36, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-24 05:31:11.276560501 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 1525193, index: 19, mask: {20, 44, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-24 05:31:11.276564210 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 1525186, index: 12, mask: {13, 37, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-24 05:31:11.268550922 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 1525188, index: 14, mask: {15, 39, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-24 05:31:11.280533285 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 1525190, index: 16, mask: {17, 41, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-24 05:31:11.283453457 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 1525183, index: 9, mask: {10, 34, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-24 05:31:11.276546877 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 1525196, index: 22, mask: {23, 47, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-24 05:31:11.276541629 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 1525192, index: 18, mask: {19, 43, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "2023-07-24 05:31:11.288534290 [E:onnxruntime:Default, env.cc:251 ThreadMain] pthread_setaffinity_np failed for thread: 1525194, index: 20, mask: {21, 45, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# check onnx integrity\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from tqdm import tqdm \n",
    "\n",
    "NUM_TEST = 10\n",
    "B = 16\n",
    "\n",
    "onnx_model = onnx.load(\"resnet18.onnx\")\n",
    "sess = ort.InferenceSession(\n",
    "    onnx_model.SerializeToString(), \n",
    "    providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "mean_diff = 0\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(NUM_TEST)):\n",
    "        input_dict = {\"x\" : torch.randn(*((B,) + SHAPE))}\n",
    "        torch_output = model(**input_dict)\n",
    "        onnx_output = sess.run(output_names, {k : v.numpy() for k, v in input_dict.items()})\n",
    "        mean_diff += (torch_output - torch.from_numpy(onnx_output[0])).square().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.0126e-17)\n"
     ]
    }
   ],
   "source": [
    "print(mean_diff / NUM_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=resnet18.onnx --minShapes=x:1x3x224x224 --optShapes=x:16x3x224x224 --maxShapes=x:32x3x224x224 --useCudaGraph --saveEngine=resnet18.plan --verbose=true\n",
      "[07/24/2023-05:44:16] [I] === Model Options ===\n",
      "[07/24/2023-05:44:16] [I] Format: ONNX\n",
      "[07/24/2023-05:44:16] [I] Model: resnet18.onnx\n",
      "[07/24/2023-05:44:16] [I] Output:\n",
      "[07/24/2023-05:44:16] [I] === Build Options ===\n",
      "[07/24/2023-05:44:16] [I] Max batch: explicit batch\n",
      "[07/24/2023-05:44:16] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[07/24/2023-05:44:16] [I] minTiming: 1\n",
      "[07/24/2023-05:44:16] [I] avgTiming: 8\n",
      "[07/24/2023-05:44:16] [I] Precision: FP32\n",
      "[07/24/2023-05:44:16] [I] LayerPrecisions: \n",
      "[07/24/2023-05:44:16] [I] Layer Device Types: \n",
      "[07/24/2023-05:44:16] [I] Calibration: \n",
      "[07/24/2023-05:44:16] [I] Refit: Disabled\n",
      "[07/24/2023-05:44:16] [I] Version Compatible: Disabled\n",
      "[07/24/2023-05:44:16] [I] TensorRT runtime: full\n",
      "[07/24/2023-05:44:16] [I] Lean DLL Path: \n",
      "[07/24/2023-05:44:16] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[07/24/2023-05:44:16] [I] Exclude Lean Runtime: Disabled\n",
      "[07/24/2023-05:44:16] [I] Sparsity: Disabled\n",
      "[07/24/2023-05:44:16] [I] Safe mode: Disabled\n",
      "[07/24/2023-05:44:16] [I] Build DLA standalone loadable: Disabled\n",
      "[07/24/2023-05:44:16] [I] Allow GPU fallback for DLA: Disabled\n",
      "[07/24/2023-05:44:16] [I] DirectIO mode: Disabled\n",
      "[07/24/2023-05:44:16] [I] Restricted mode: Disabled\n",
      "[07/24/2023-05:44:16] [I] Skip inference: Disabled\n",
      "[07/24/2023-05:44:16] [I] Save engine: resnet18.plan\n",
      "[07/24/2023-05:44:16] [I] Load engine: \n",
      "[07/24/2023-05:44:16] [I] Profiling verbosity: 0\n",
      "[07/24/2023-05:44:16] [I] Tactic sources: Using default tactic sources\n",
      "[07/24/2023-05:44:16] [I] timingCacheMode: local\n",
      "[07/24/2023-05:44:16] [I] timingCacheFile: \n",
      "[07/24/2023-05:44:16] [I] Heuristic: Disabled\n",
      "[07/24/2023-05:44:16] [I] Preview Features: Use default preview flags.\n",
      "[07/24/2023-05:44:16] [I] MaxAuxStreams: -1\n",
      "[07/24/2023-05:44:16] [I] BuilderOptimizationLevel: -1\n",
      "[07/24/2023-05:44:16] [I] Input(s)s format: fp32:CHW\n",
      "[07/24/2023-05:44:16] [I] Output(s)s format: fp32:CHW\n",
      "[07/24/2023-05:44:16] [I] Input build shape: x=1x3x224x224+16x3x224x224+32x3x224x224\n",
      "[07/24/2023-05:44:16] [I] Input calibration shapes: model\n",
      "[07/24/2023-05:44:16] [I] === System Options ===\n",
      "[07/24/2023-05:44:16] [I] Device: 0\n",
      "[07/24/2023-05:44:16] [I] DLACore: \n",
      "[07/24/2023-05:44:16] [I] Plugins:\n",
      "[07/24/2023-05:44:16] [I] setPluginsToSerialize:\n",
      "[07/24/2023-05:44:16] [I] dynamicPlugins:\n",
      "[07/24/2023-05:44:16] [I] ignoreParsedPluginLibs: 0\n",
      "[07/24/2023-05:44:16] [I] \n",
      "[07/24/2023-05:44:16] [I] === Inference Options ===\n",
      "[07/24/2023-05:44:16] [I] Batch: Explicit\n",
      "[07/24/2023-05:44:16] [I] Input inference shape: x=16x3x224x224\n",
      "[07/24/2023-05:44:16] [I] Iterations: 10\n",
      "[07/24/2023-05:44:16] [I] Duration: 3s (+ 200ms warm up)\n",
      "[07/24/2023-05:44:16] [I] Sleep time: 0ms\n",
      "[07/24/2023-05:44:16] [I] Idle time: 0ms\n",
      "[07/24/2023-05:44:16] [I] Inference Streams: 1\n",
      "[07/24/2023-05:44:16] [I] ExposeDMA: Disabled\n",
      "[07/24/2023-05:44:16] [I] Data transfers: Enabled\n",
      "[07/24/2023-05:44:16] [I] Spin-wait: Disabled\n",
      "[07/24/2023-05:44:16] [I] Multithreading: Disabled\n",
      "[07/24/2023-05:44:16] [I] CUDA Graph: Enabled\n",
      "[07/24/2023-05:44:16] [I] Separate profiling: Disabled\n",
      "[07/24/2023-05:44:16] [I] Time Deserialize: Disabled\n",
      "[07/24/2023-05:44:16] [I] Time Refit: Disabled\n",
      "[07/24/2023-05:44:16] [I] NVTX verbosity: 0\n",
      "[07/24/2023-05:44:16] [I] Persistent Cache Ratio: 0\n",
      "[07/24/2023-05:44:16] [I] Inputs:\n",
      "[07/24/2023-05:44:16] [I] === Reporting Options ===\n",
      "[07/24/2023-05:44:16] [I] Verbose: Enabled\n",
      "[07/24/2023-05:44:16] [I] Averages: 10 inferences\n",
      "[07/24/2023-05:44:16] [I] Percentiles: 90,95,99\n",
      "[07/24/2023-05:44:16] [I] Dump refittable layers:Disabled\n",
      "[07/24/2023-05:44:16] [I] Dump output: Disabled\n",
      "[07/24/2023-05:44:16] [I] Profile: Disabled\n",
      "[07/24/2023-05:44:16] [I] Export timing to JSON file: \n",
      "[07/24/2023-05:44:16] [I] Export output to JSON file: \n",
      "[07/24/2023-05:44:16] [I] Export profile to JSON file: \n",
      "[07/24/2023-05:44:16] [I] \n",
      "[07/24/2023-05:44:17] [I] === Device Information ===\n",
      "[07/24/2023-05:44:17] [I] Selected Device: NVIDIA GeForce RTX 3060\n",
      "[07/24/2023-05:44:17] [I] Compute Capability: 8.6\n",
      "[07/24/2023-05:44:17] [I] SMs: 28\n",
      "[07/24/2023-05:44:17] [I] Device Global Memory: 12044 MiB\n",
      "[07/24/2023-05:44:17] [I] Shared Memory per SM: 100 KiB\n",
      "[07/24/2023-05:44:17] [I] Memory Bus Width: 192 bits (ECC disabled)\n",
      "[07/24/2023-05:44:17] [I] Application Compute Clock Rate: 1.837 GHz\n",
      "[07/24/2023-05:44:17] [I] Application Memory Clock Rate: 7.501 GHz\n",
      "[07/24/2023-05:44:17] [I] \n",
      "[07/24/2023-05:44:17] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[07/24/2023-05:44:17] [I] \n",
      "[07/24/2023-05:44:17] [I] TensorRT version: 8.6.1\n",
      "[07/24/2023-05:44:17] [I] Loading standard plugins\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::BatchedNMS_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::BatchTilePlugin_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::Clip_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::CoordConvAC version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::CropAndResizeDynamic version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::CropAndResize version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::DecodeBbox3DPlugin version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::DetectionLayer_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::EfficientNMS_Explicit_TF_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::EfficientNMS_Implicit_TF_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::EfficientNMS_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::FlattenConcat_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::GenerateDetection_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::GridAnchor_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::GridAnchorRect_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 2\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::LReLU_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::ModulatedDeformConv2d version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::MultilevelCropAndResize_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::MultilevelProposeROI_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::NMSDynamic_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::NMS_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::Normalize_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::PillarScatterPlugin version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::PriorBox_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::ProposalDynamic version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::ProposalLayer_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::Proposal version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::PyramidROIAlign_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::Region_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::Reorg_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::ResizeNearest_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::ROIAlign_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::RPROI_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::ScatterND version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::SpecialSlice_TRT version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::Split version 1\n",
      "[07/24/2023-05:44:17] [V] [TRT] Registered plugin creator - ::VoxelGeneratorPlugin version 1\n",
      "[07/24/2023-05:44:17] [I] [TRT] [MemUsageChange] Init CUDA: CPU +352, GPU +0, now: CPU 367, GPU 178 (MiB)\n",
      "[07/24/2023-05:44:18] [V] [TRT] Trying to load shared library libnvinfer_builder_resource.so.8.6.1\n",
      "[07/24/2023-05:44:18] [V] [TRT] Loaded shared library libnvinfer_builder_resource.so.8.6.1\n",
      "[07/24/2023-05:44:27] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +1217, GPU +266, now: CPU 1660, GPU 444 (MiB)\n",
      "[07/24/2023-05:44:27] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "[07/24/2023-05:44:27] [I] Start parsing network model.\n",
      "[07/24/2023-05:44:27] [I] [TRT] ----------------------------------------------------------------\n",
      "[07/24/2023-05:44:27] [I] [TRT] Input filename:   resnet18.onnx\n",
      "[07/24/2023-05:44:27] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[07/24/2023-05:44:27] [I] [TRT] Opset version:    14\n",
      "[07/24/2023-05:44:27] [I] [TRT] Producer name:    pytorch\n",
      "[07/24/2023-05:44:27] [I] [TRT] Producer version: 2.0.0\n",
      "[07/24/2023-05:44:27] [I] [TRT] Domain:           \n",
      "[07/24/2023-05:44:27] [I] [TRT] Model version:    0\n",
      "[07/24/2023-05:44:27] [I] [TRT] Doc string:       \n",
      "[07/24/2023-05:44:27] [I] [TRT] ----------------------------------------------------------------\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::BatchedNMS_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::BatchTilePlugin_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::Clip_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::CoordConvAC version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::CropAndResizeDynamic version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::CropAndResize version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::DecodeBbox3DPlugin version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::DetectionLayer_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::EfficientNMS_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::FlattenConcat_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::GenerateDetection_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::GridAnchor_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::GridAnchorRect_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::InstanceNormalization_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::InstanceNormalization_TRT version 2\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::LReLU_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::ModulatedDeformConv2d version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::NMSDynamic_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::NMS_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::Normalize_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::PillarScatterPlugin version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::PriorBox_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::ProposalDynamic version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::ProposalLayer_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::Proposal version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::PyramidROIAlign_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::Region_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::Reorg_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::ResizeNearest_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::ROIAlign_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::RPROI_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::ScatterND version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::SpecialSlice_TRT version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::Split version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Plugin creator already registered - ::VoxelGeneratorPlugin version 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Adding network input: x with dtype: float32, dimensions: (-1, 3, 224, 224)\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: x for ONNX tensor: x\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: fc.weight\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: fc.bias\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_193\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_194\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_196\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_199\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_202\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_208\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_209\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_211\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_214\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_217\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_223\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_224\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_226\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_229\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_232\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_238\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_239\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_241\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_244\n",
      "[07/24/2023-05:44:27] [V] [TRT] Importing initializer: onnx::Conv_247\n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_0 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_239\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_0 [Identity] inputs: [onnx::Conv_239 -> (512)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: onnx::Conv_239 for ONNX node: onnx::Conv_239\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_0 for ONNX node: Identity_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_251 for ONNX tensor: onnx::Conv_251\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_0 [Identity] outputs: [onnx::Conv_251 -> (512)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_1 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_241\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_1 [Identity] inputs: [onnx::Conv_241 -> (512, 512, 3, 3)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: onnx::Conv_241 for ONNX node: onnx::Conv_241\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_1 for ONNX node: Identity_1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_250 for ONNX tensor: onnx::Conv_250\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_1 [Identity] outputs: [onnx::Conv_250 -> (512, 512, 3, 3)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_2 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_239\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_2 [Identity] inputs: [onnx::Conv_239 -> (512)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_2 for ONNX node: Identity_2\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_248 for ONNX tensor: onnx::Conv_248\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_2 [Identity] outputs: [onnx::Conv_248 -> (512)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_3 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_239\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_3 [Identity] inputs: [onnx::Conv_239 -> (512)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_3 for ONNX node: Identity_3\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_245 for ONNX tensor: onnx::Conv_245\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_3 [Identity] outputs: [onnx::Conv_245 -> (512)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_4 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_239\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_4 [Identity] inputs: [onnx::Conv_239 -> (512)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_4 for ONNX node: Identity_4\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_242 for ONNX tensor: onnx::Conv_242\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_4 [Identity] outputs: [onnx::Conv_242 -> (512)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_5 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_224\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_5 [Identity] inputs: [onnx::Conv_224 -> (256)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: onnx::Conv_224 for ONNX node: onnx::Conv_224\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_5 for ONNX node: Identity_5\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_236 for ONNX tensor: onnx::Conv_236\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_5 [Identity] outputs: [onnx::Conv_236 -> (256)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_6 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_226\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_6 [Identity] inputs: [onnx::Conv_226 -> (256, 256, 3, 3)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: onnx::Conv_226 for ONNX node: onnx::Conv_226\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_6 for ONNX node: Identity_6\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_235 for ONNX tensor: onnx::Conv_235\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_6 [Identity] outputs: [onnx::Conv_235 -> (256, 256, 3, 3)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_7 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_224\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_7 [Identity] inputs: [onnx::Conv_224 -> (256)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_7 for ONNX node: Identity_7\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_233 for ONNX tensor: onnx::Conv_233\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_7 [Identity] outputs: [onnx::Conv_233 -> (256)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_8 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_224\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_8 [Identity] inputs: [onnx::Conv_224 -> (256)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_8 for ONNX node: Identity_8\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_230 for ONNX tensor: onnx::Conv_230\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_8 [Identity] outputs: [onnx::Conv_230 -> (256)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_9 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_224\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_9 [Identity] inputs: [onnx::Conv_224 -> (256)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_9 for ONNX node: Identity_9\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_227 for ONNX tensor: onnx::Conv_227\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_9 [Identity] outputs: [onnx::Conv_227 -> (256)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_10 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_209\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_10 [Identity] inputs: [onnx::Conv_209 -> (128)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: onnx::Conv_209 for ONNX node: onnx::Conv_209\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_10 for ONNX node: Identity_10\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_221 for ONNX tensor: onnx::Conv_221\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_10 [Identity] outputs: [onnx::Conv_221 -> (128)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_11 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_211\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_11 [Identity] inputs: [onnx::Conv_211 -> (128, 128, 3, 3)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: onnx::Conv_211 for ONNX node: onnx::Conv_211\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_11 for ONNX node: Identity_11\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_220 for ONNX tensor: onnx::Conv_220\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_11 [Identity] outputs: [onnx::Conv_220 -> (128, 128, 3, 3)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_12 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_209\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_12 [Identity] inputs: [onnx::Conv_209 -> (128)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_12 for ONNX node: Identity_12\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_218 for ONNX tensor: onnx::Conv_218\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_12 [Identity] outputs: [onnx::Conv_218 -> (128)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_13 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_209\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_13 [Identity] inputs: [onnx::Conv_209 -> (128)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_13 for ONNX node: Identity_13\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_215 for ONNX tensor: onnx::Conv_215\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_13 [Identity] outputs: [onnx::Conv_215 -> (128)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_14 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_209\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_14 [Identity] inputs: [onnx::Conv_209 -> (128)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_14 for ONNX node: Identity_14\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_212 for ONNX tensor: onnx::Conv_212\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_14 [Identity] outputs: [onnx::Conv_212 -> (128)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_15 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_194\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_15 [Identity] inputs: [onnx::Conv_194 -> (64)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: onnx::Conv_194 for ONNX node: onnx::Conv_194\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_15 for ONNX node: Identity_15\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_206 for ONNX tensor: onnx::Conv_206\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_15 [Identity] outputs: [onnx::Conv_206 -> (64)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_16 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_199\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_16 [Identity] inputs: [onnx::Conv_199 -> (64, 64, 3, 3)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: onnx::Conv_199 for ONNX node: onnx::Conv_199\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_16 for ONNX node: Identity_16\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_205 for ONNX tensor: onnx::Conv_205\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_16 [Identity] outputs: [onnx::Conv_205 -> (64, 64, 3, 3)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_17 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_194\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_17 [Identity] inputs: [onnx::Conv_194 -> (64)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_17 for ONNX node: Identity_17\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_203 for ONNX tensor: onnx::Conv_203\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_17 [Identity] outputs: [onnx::Conv_203 -> (64)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_18 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_194\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_18 [Identity] inputs: [onnx::Conv_194 -> (64)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_18 for ONNX node: Identity_18\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_200 for ONNX tensor: onnx::Conv_200\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_18 [Identity] outputs: [onnx::Conv_200 -> (64)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: Identity_19 [Identity]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_194\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_19 [Identity] inputs: [onnx::Conv_194 -> (64)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: Identity_19 for ONNX node: Identity_19\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: onnx::Conv_197 for ONNX tensor: onnx::Conv_197\n",
      "[07/24/2023-05:44:27] [V] [TRT] Identity_19 [Identity] outputs: [onnx::Conv_197 -> (64)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /conv1/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: x\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_193\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_194\n",
      "[07/24/2023-05:44:27] [V] [TRT] /conv1/Conv [Conv] inputs: [x -> (-1, 3, 224, 224)[FLOAT]], [onnx::Conv_193 -> (64, 3, 7, 7)[FLOAT]], [onnx::Conv_194 -> (64)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Convolution input dimensions: (-1, 3, 224, 224)\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /conv1/Conv for ONNX node: /conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Using kernel: (7, 7), strides: (2, 2), prepadding: (3, 3), postpadding: (3, 3), dilations: (1, 1), numOutputs: 64\n",
      "[07/24/2023-05:44:27] [V] [TRT] Convolution output dimensions: (-1, 64, 112, 112)\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /conv1/Conv_output_0 for ONNX tensor: /conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /conv1/Conv [Conv] outputs: [/conv1/Conv_output_0 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /act1/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /act1/Relu [Relu] inputs: [/conv1/Conv_output_0 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /act1/Relu for ONNX node: /act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /act1/Relu_output_0 for ONNX tensor: /act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /act1/Relu [Relu] outputs: [/act1/Relu_output_0 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /maxpool/MaxPool [MaxPool]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /maxpool/MaxPool [MaxPool] inputs: [/act1/Relu_output_0 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /maxpool/MaxPool for ONNX node: /maxpool/MaxPool\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /maxpool/MaxPool_output_0 for ONNX tensor: /maxpool/MaxPool_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /maxpool/MaxPool [MaxPool] outputs: [/maxpool/MaxPool_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer1/layer1.0/conv1/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /maxpool/MaxPool_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_196\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_197\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.0/conv1/Conv [Conv] inputs: [/maxpool/MaxPool_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_196 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_197 -> (64)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer1/layer1.0/conv1/Conv for ONNX node: /layer1/layer1.0/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer1/layer1.0/conv1/Conv_output_0 for ONNX tensor: /layer1/layer1.0/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.0/conv1/Conv [Conv] outputs: [/layer1/layer1.0/conv1/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer1/layer1.0/act1/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer1/layer1.0/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.0/act1/Relu [Relu] inputs: [/layer1/layer1.0/conv1/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer1/layer1.0/act1/Relu for ONNX node: /layer1/layer1.0/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer1/layer1.0/act1/Relu_output_0 for ONNX tensor: /layer1/layer1.0/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.0/act1/Relu [Relu] outputs: [/layer1/layer1.0/act1/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer1/layer1.0/conv2/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer1/layer1.0/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_199\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_200\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.0/conv2/Conv [Conv] inputs: [/layer1/layer1.0/act1/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_199 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_200 -> (64)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer1/layer1.0/conv2/Conv for ONNX node: /layer1/layer1.0/conv2/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer1/layer1.0/conv2/Conv_output_0 for ONNX tensor: /layer1/layer1.0/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.0/conv2/Conv [Conv] outputs: [/layer1/layer1.0/conv2/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer1/layer1.0/Add [Add]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer1/layer1.0/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /maxpool/MaxPool_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.0/Add [Add] inputs: [/layer1/layer1.0/conv2/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], [/maxpool/MaxPool_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer1/layer1.0/Add for ONNX node: /layer1/layer1.0/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer1/layer1.0/Add_output_0 for ONNX tensor: /layer1/layer1.0/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.0/Add [Add] outputs: [/layer1/layer1.0/Add_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer1/layer1.0/act2/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer1/layer1.0/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.0/act2/Relu [Relu] inputs: [/layer1/layer1.0/Add_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer1/layer1.0/act2/Relu for ONNX node: /layer1/layer1.0/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer1/layer1.0/act2/Relu_output_0 for ONNX tensor: /layer1/layer1.0/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.0/act2/Relu [Relu] outputs: [/layer1/layer1.0/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer1/layer1.1/conv1/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer1/layer1.0/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_202\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_203\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.1/conv1/Conv [Conv] inputs: [/layer1/layer1.0/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_202 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_203 -> (64)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer1/layer1.1/conv1/Conv for ONNX node: /layer1/layer1.1/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer1/layer1.1/conv1/Conv_output_0 for ONNX tensor: /layer1/layer1.1/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.1/conv1/Conv [Conv] outputs: [/layer1/layer1.1/conv1/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer1/layer1.1/act1/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer1/layer1.1/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.1/act1/Relu [Relu] inputs: [/layer1/layer1.1/conv1/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer1/layer1.1/act1/Relu for ONNX node: /layer1/layer1.1/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer1/layer1.1/act1/Relu_output_0 for ONNX tensor: /layer1/layer1.1/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.1/act1/Relu [Relu] outputs: [/layer1/layer1.1/act1/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer1/layer1.1/conv2/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer1/layer1.1/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_205\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_206\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.1/conv2/Conv [Conv] inputs: [/layer1/layer1.1/act1/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_205 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_206 -> (64)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer1/layer1.1/conv2/Conv for ONNX node: /layer1/layer1.1/conv2/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer1/layer1.1/conv2/Conv_output_0 for ONNX tensor: /layer1/layer1.1/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.1/conv2/Conv [Conv] outputs: [/layer1/layer1.1/conv2/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer1/layer1.1/Add [Add]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer1/layer1.1/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer1/layer1.0/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.1/Add [Add] inputs: [/layer1/layer1.1/conv2/Conv_output_0 -> (-1, 64, 56, 56)[FLOAT]], [/layer1/layer1.0/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer1/layer1.1/Add for ONNX node: /layer1/layer1.1/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer1/layer1.1/Add_output_0 for ONNX tensor: /layer1/layer1.1/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.1/Add [Add] outputs: [/layer1/layer1.1/Add_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer1/layer1.1/act2/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer1/layer1.1/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.1/act2/Relu [Relu] inputs: [/layer1/layer1.1/Add_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer1/layer1.1/act2/Relu for ONNX node: /layer1/layer1.1/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer1/layer1.1/act2/Relu_output_0 for ONNX tensor: /layer1/layer1.1/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer1/layer1.1/act2/Relu [Relu] outputs: [/layer1/layer1.1/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer2/layer2.0/conv1/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer1/layer1.1/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_208\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_209\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.0/conv1/Conv [Conv] inputs: [/layer1/layer1.1/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_208 -> (128, 64, 3, 3)[FLOAT]], [onnx::Conv_209 -> (128)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Convolution input dimensions: (-1, 64, 56, 56)\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer2/layer2.0/conv1/Conv for ONNX node: /layer2/layer2.0/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128\n",
      "[07/24/2023-05:44:27] [V] [TRT] Convolution output dimensions: (-1, 128, 28, 28)\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer2/layer2.0/conv1/Conv_output_0 for ONNX tensor: /layer2/layer2.0/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.0/conv1/Conv [Conv] outputs: [/layer2/layer2.0/conv1/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer2/layer2.0/act1/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer2/layer2.0/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.0/act1/Relu [Relu] inputs: [/layer2/layer2.0/conv1/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer2/layer2.0/act1/Relu for ONNX node: /layer2/layer2.0/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer2/layer2.0/act1/Relu_output_0 for ONNX tensor: /layer2/layer2.0/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.0/act1/Relu [Relu] outputs: [/layer2/layer2.0/act1/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer2/layer2.0/conv2/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer2/layer2.0/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_211\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_212\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.0/conv2/Conv [Conv] inputs: [/layer2/layer2.0/act1/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_211 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_212 -> (128)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer2/layer2.0/conv2/Conv for ONNX node: /layer2/layer2.0/conv2/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer2/layer2.0/conv2/Conv_output_0 for ONNX tensor: /layer2/layer2.0/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.0/conv2/Conv [Conv] outputs: [/layer2/layer2.0/conv2/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer2/layer2.0/downsample/downsample.0/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer1/layer1.1/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_214\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_215\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv [Conv] inputs: [/layer1/layer1.1/act2/Relu_output_0 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_214 -> (128, 64, 1, 1)[FLOAT]], [onnx::Conv_215 -> (128)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer2/layer2.0/downsample/downsample.0/Conv for ONNX node: /layer2/layer2.0/downsample/downsample.0/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer2/layer2.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /layer2/layer2.0/downsample/downsample.0/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv [Conv] outputs: [/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer2/layer2.0/Add [Add]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer2/layer2.0/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer2/layer2.0/downsample/downsample.0/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.0/Add [Add] inputs: [/layer2/layer2.0/conv2/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], [/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer2/layer2.0/Add for ONNX node: /layer2/layer2.0/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer2/layer2.0/Add_output_0 for ONNX tensor: /layer2/layer2.0/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.0/Add [Add] outputs: [/layer2/layer2.0/Add_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer2/layer2.0/act2/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer2/layer2.0/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.0/act2/Relu [Relu] inputs: [/layer2/layer2.0/Add_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer2/layer2.0/act2/Relu for ONNX node: /layer2/layer2.0/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer2/layer2.0/act2/Relu_output_0 for ONNX tensor: /layer2/layer2.0/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.0/act2/Relu [Relu] outputs: [/layer2/layer2.0/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer2/layer2.1/conv1/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer2/layer2.0/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_217\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_218\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.1/conv1/Conv [Conv] inputs: [/layer2/layer2.0/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_217 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_218 -> (128)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer2/layer2.1/conv1/Conv for ONNX node: /layer2/layer2.1/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer2/layer2.1/conv1/Conv_output_0 for ONNX tensor: /layer2/layer2.1/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.1/conv1/Conv [Conv] outputs: [/layer2/layer2.1/conv1/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer2/layer2.1/act1/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer2/layer2.1/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.1/act1/Relu [Relu] inputs: [/layer2/layer2.1/conv1/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer2/layer2.1/act1/Relu for ONNX node: /layer2/layer2.1/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer2/layer2.1/act1/Relu_output_0 for ONNX tensor: /layer2/layer2.1/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.1/act1/Relu [Relu] outputs: [/layer2/layer2.1/act1/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer2/layer2.1/conv2/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer2/layer2.1/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_220\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_221\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.1/conv2/Conv [Conv] inputs: [/layer2/layer2.1/act1/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_220 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_221 -> (128)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer2/layer2.1/conv2/Conv for ONNX node: /layer2/layer2.1/conv2/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer2/layer2.1/conv2/Conv_output_0 for ONNX tensor: /layer2/layer2.1/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.1/conv2/Conv [Conv] outputs: [/layer2/layer2.1/conv2/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer2/layer2.1/Add [Add]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer2/layer2.1/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer2/layer2.0/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.1/Add [Add] inputs: [/layer2/layer2.1/conv2/Conv_output_0 -> (-1, 128, 28, 28)[FLOAT]], [/layer2/layer2.0/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer2/layer2.1/Add for ONNX node: /layer2/layer2.1/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer2/layer2.1/Add_output_0 for ONNX tensor: /layer2/layer2.1/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.1/Add [Add] outputs: [/layer2/layer2.1/Add_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer2/layer2.1/act2/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer2/layer2.1/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.1/act2/Relu [Relu] inputs: [/layer2/layer2.1/Add_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer2/layer2.1/act2/Relu for ONNX node: /layer2/layer2.1/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer2/layer2.1/act2/Relu_output_0 for ONNX tensor: /layer2/layer2.1/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer2/layer2.1/act2/Relu [Relu] outputs: [/layer2/layer2.1/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer3/layer3.0/conv1/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer2/layer2.1/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_223\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_224\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.0/conv1/Conv [Conv] inputs: [/layer2/layer2.1/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_223 -> (256, 128, 3, 3)[FLOAT]], [onnx::Conv_224 -> (256)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Convolution input dimensions: (-1, 128, 28, 28)\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer3/layer3.0/conv1/Conv for ONNX node: /layer3/layer3.0/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256\n",
      "[07/24/2023-05:44:27] [V] [TRT] Convolution output dimensions: (-1, 256, 14, 14)\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer3/layer3.0/conv1/Conv_output_0 for ONNX tensor: /layer3/layer3.0/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.0/conv1/Conv [Conv] outputs: [/layer3/layer3.0/conv1/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer3/layer3.0/act1/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer3/layer3.0/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.0/act1/Relu [Relu] inputs: [/layer3/layer3.0/conv1/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer3/layer3.0/act1/Relu for ONNX node: /layer3/layer3.0/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer3/layer3.0/act1/Relu_output_0 for ONNX tensor: /layer3/layer3.0/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.0/act1/Relu [Relu] outputs: [/layer3/layer3.0/act1/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer3/layer3.0/conv2/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer3/layer3.0/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_226\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_227\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.0/conv2/Conv [Conv] inputs: [/layer3/layer3.0/act1/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_226 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_227 -> (256)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer3/layer3.0/conv2/Conv for ONNX node: /layer3/layer3.0/conv2/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer3/layer3.0/conv2/Conv_output_0 for ONNX tensor: /layer3/layer3.0/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.0/conv2/Conv [Conv] outputs: [/layer3/layer3.0/conv2/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer3/layer3.0/downsample/downsample.0/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer2/layer2.1/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_229\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_230\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv [Conv] inputs: [/layer2/layer2.1/act2/Relu_output_0 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_229 -> (256, 128, 1, 1)[FLOAT]], [onnx::Conv_230 -> (256)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer3/layer3.0/downsample/downsample.0/Conv for ONNX node: /layer3/layer3.0/downsample/downsample.0/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer3/layer3.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /layer3/layer3.0/downsample/downsample.0/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv [Conv] outputs: [/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer3/layer3.0/Add [Add]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer3/layer3.0/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer3/layer3.0/downsample/downsample.0/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.0/Add [Add] inputs: [/layer3/layer3.0/conv2/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], [/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer3/layer3.0/Add for ONNX node: /layer3/layer3.0/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer3/layer3.0/Add_output_0 for ONNX tensor: /layer3/layer3.0/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.0/Add [Add] outputs: [/layer3/layer3.0/Add_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer3/layer3.0/act2/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer3/layer3.0/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.0/act2/Relu [Relu] inputs: [/layer3/layer3.0/Add_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer3/layer3.0/act2/Relu for ONNX node: /layer3/layer3.0/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer3/layer3.0/act2/Relu_output_0 for ONNX tensor: /layer3/layer3.0/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.0/act2/Relu [Relu] outputs: [/layer3/layer3.0/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer3/layer3.1/conv1/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer3/layer3.0/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_232\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_233\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.1/conv1/Conv [Conv] inputs: [/layer3/layer3.0/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_232 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_233 -> (256)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer3/layer3.1/conv1/Conv for ONNX node: /layer3/layer3.1/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer3/layer3.1/conv1/Conv_output_0 for ONNX tensor: /layer3/layer3.1/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.1/conv1/Conv [Conv] outputs: [/layer3/layer3.1/conv1/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer3/layer3.1/act1/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer3/layer3.1/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.1/act1/Relu [Relu] inputs: [/layer3/layer3.1/conv1/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer3/layer3.1/act1/Relu for ONNX node: /layer3/layer3.1/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer3/layer3.1/act1/Relu_output_0 for ONNX tensor: /layer3/layer3.1/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.1/act1/Relu [Relu] outputs: [/layer3/layer3.1/act1/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer3/layer3.1/conv2/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer3/layer3.1/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_235\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_236\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.1/conv2/Conv [Conv] inputs: [/layer3/layer3.1/act1/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_235 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_236 -> (256)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer3/layer3.1/conv2/Conv for ONNX node: /layer3/layer3.1/conv2/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer3/layer3.1/conv2/Conv_output_0 for ONNX tensor: /layer3/layer3.1/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.1/conv2/Conv [Conv] outputs: [/layer3/layer3.1/conv2/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer3/layer3.1/Add [Add]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer3/layer3.1/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer3/layer3.0/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.1/Add [Add] inputs: [/layer3/layer3.1/conv2/Conv_output_0 -> (-1, 256, 14, 14)[FLOAT]], [/layer3/layer3.0/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer3/layer3.1/Add for ONNX node: /layer3/layer3.1/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer3/layer3.1/Add_output_0 for ONNX tensor: /layer3/layer3.1/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.1/Add [Add] outputs: [/layer3/layer3.1/Add_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer3/layer3.1/act2/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer3/layer3.1/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.1/act2/Relu [Relu] inputs: [/layer3/layer3.1/Add_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer3/layer3.1/act2/Relu for ONNX node: /layer3/layer3.1/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer3/layer3.1/act2/Relu_output_0 for ONNX tensor: /layer3/layer3.1/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer3/layer3.1/act2/Relu [Relu] outputs: [/layer3/layer3.1/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer4/layer4.0/conv1/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer3/layer3.1/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_238\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_239\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.0/conv1/Conv [Conv] inputs: [/layer3/layer3.1/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_238 -> (512, 256, 3, 3)[FLOAT]], [onnx::Conv_239 -> (512)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Convolution input dimensions: (-1, 256, 14, 14)\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer4/layer4.0/conv1/Conv for ONNX node: /layer4/layer4.0/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 512\n",
      "[07/24/2023-05:44:27] [V] [TRT] Convolution output dimensions: (-1, 512, 7, 7)\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer4/layer4.0/conv1/Conv_output_0 for ONNX tensor: /layer4/layer4.0/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.0/conv1/Conv [Conv] outputs: [/layer4/layer4.0/conv1/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer4/layer4.0/act1/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer4/layer4.0/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.0/act1/Relu [Relu] inputs: [/layer4/layer4.0/conv1/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer4/layer4.0/act1/Relu for ONNX node: /layer4/layer4.0/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer4/layer4.0/act1/Relu_output_0 for ONNX tensor: /layer4/layer4.0/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.0/act1/Relu [Relu] outputs: [/layer4/layer4.0/act1/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer4/layer4.0/conv2/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer4/layer4.0/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_241\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_242\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.0/conv2/Conv [Conv] inputs: [/layer4/layer4.0/act1/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], [onnx::Conv_241 -> (512, 512, 3, 3)[FLOAT]], [onnx::Conv_242 -> (512)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer4/layer4.0/conv2/Conv for ONNX node: /layer4/layer4.0/conv2/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer4/layer4.0/conv2/Conv_output_0 for ONNX tensor: /layer4/layer4.0/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.0/conv2/Conv [Conv] outputs: [/layer4/layer4.0/conv2/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer4/layer4.0/downsample/downsample.0/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer3/layer3.1/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_244\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_245\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv [Conv] inputs: [/layer3/layer3.1/act2/Relu_output_0 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_244 -> (512, 256, 1, 1)[FLOAT]], [onnx::Conv_245 -> (512)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer4/layer4.0/downsample/downsample.0/Conv for ONNX node: /layer4/layer4.0/downsample/downsample.0/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer4/layer4.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /layer4/layer4.0/downsample/downsample.0/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv [Conv] outputs: [/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer4/layer4.0/Add [Add]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer4/layer4.0/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer4/layer4.0/downsample/downsample.0/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.0/Add [Add] inputs: [/layer4/layer4.0/conv2/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], [/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer4/layer4.0/Add for ONNX node: /layer4/layer4.0/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer4/layer4.0/Add_output_0 for ONNX tensor: /layer4/layer4.0/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.0/Add [Add] outputs: [/layer4/layer4.0/Add_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer4/layer4.0/act2/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer4/layer4.0/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.0/act2/Relu [Relu] inputs: [/layer4/layer4.0/Add_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer4/layer4.0/act2/Relu for ONNX node: /layer4/layer4.0/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer4/layer4.0/act2/Relu_output_0 for ONNX tensor: /layer4/layer4.0/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.0/act2/Relu [Relu] outputs: [/layer4/layer4.0/act2/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer4/layer4.1/conv1/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer4/layer4.0/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_247\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_248\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.1/conv1/Conv [Conv] inputs: [/layer4/layer4.0/act2/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], [onnx::Conv_247 -> (512, 512, 3, 3)[FLOAT]], [onnx::Conv_248 -> (512)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer4/layer4.1/conv1/Conv for ONNX node: /layer4/layer4.1/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer4/layer4.1/conv1/Conv_output_0 for ONNX tensor: /layer4/layer4.1/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.1/conv1/Conv [Conv] outputs: [/layer4/layer4.1/conv1/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer4/layer4.1/act1/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer4/layer4.1/conv1/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.1/act1/Relu [Relu] inputs: [/layer4/layer4.1/conv1/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer4/layer4.1/act1/Relu for ONNX node: /layer4/layer4.1/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer4/layer4.1/act1/Relu_output_0 for ONNX tensor: /layer4/layer4.1/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.1/act1/Relu [Relu] outputs: [/layer4/layer4.1/act1/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer4/layer4.1/conv2/Conv [Conv]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer4/layer4.1/act1/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_250\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: onnx::Conv_251\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.1/conv2/Conv [Conv] inputs: [/layer4/layer4.1/act1/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], [onnx::Conv_250 -> (512, 512, 3, 3)[FLOAT]], [onnx::Conv_251 -> (512)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer4/layer4.1/conv2/Conv for ONNX node: /layer4/layer4.1/conv2/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer4/layer4.1/conv2/Conv_output_0 for ONNX tensor: /layer4/layer4.1/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.1/conv2/Conv [Conv] outputs: [/layer4/layer4.1/conv2/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer4/layer4.1/Add [Add]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer4/layer4.1/conv2/Conv_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer4/layer4.0/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.1/Add [Add] inputs: [/layer4/layer4.1/conv2/Conv_output_0 -> (-1, 512, 7, 7)[FLOAT]], [/layer4/layer4.0/act2/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer4/layer4.1/Add for ONNX node: /layer4/layer4.1/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer4/layer4.1/Add_output_0 for ONNX tensor: /layer4/layer4.1/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.1/Add [Add] outputs: [/layer4/layer4.1/Add_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /layer4/layer4.1/act2/Relu [Relu]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer4/layer4.1/Add_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.1/act2/Relu [Relu] inputs: [/layer4/layer4.1/Add_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /layer4/layer4.1/act2/Relu for ONNX node: /layer4/layer4.1/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /layer4/layer4.1/act2/Relu_output_0 for ONNX tensor: /layer4/layer4.1/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /layer4/layer4.1/act2/Relu [Relu] outputs: [/layer4/layer4.1/act2/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /global_pool/pool/GlobalAveragePool [GlobalAveragePool]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /layer4/layer4.1/act2/Relu_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /global_pool/pool/GlobalAveragePool [GlobalAveragePool] inputs: [/layer4/layer4.1/act2/Relu_output_0 -> (-1, 512, 7, 7)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] GlobalAveragePool operators are implemented via Reduce layers rather than Pooling layers\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /global_pool/pool/GlobalAveragePool for ONNX node: /global_pool/pool/GlobalAveragePool\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /global_pool/pool/GlobalAveragePool_output_0 for ONNX tensor: /global_pool/pool/GlobalAveragePool_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /global_pool/pool/GlobalAveragePool [GlobalAveragePool] outputs: [/global_pool/pool/GlobalAveragePool_output_0 -> (-1, 512, 1, 1)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /global_pool/flatten/Flatten [Flatten]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /global_pool/pool/GlobalAveragePool_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /global_pool/flatten/Flatten [Flatten] inputs: [/global_pool/pool/GlobalAveragePool_output_0 -> (-1, 512, 1, 1)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [W] [TRT] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /global_pool/flatten/Flatten for ONNX node: /global_pool/flatten/Flatten\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: /global_pool/flatten/Flatten_output_0 for ONNX tensor: /global_pool/flatten/Flatten_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] /global_pool/flatten/Flatten [Flatten] outputs: [/global_pool/flatten/Flatten_output_0 -> (-1, 512)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Parsing node: /fc/Gemm [Gemm]\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: /global_pool/flatten/Flatten_output_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: fc.weight\n",
      "[07/24/2023-05:44:27] [V] [TRT] Searching for input: fc.bias\n",
      "[07/24/2023-05:44:27] [V] [TRT] /fc/Gemm [Gemm] inputs: [/global_pool/flatten/Flatten_output_0 -> (-1, 512)[FLOAT]], [fc.weight -> (1000, 512)[FLOAT]], [fc.bias -> (1000)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: fc.weight for ONNX node: fc.weight\n",
      "[07/24/2023-05:44:27] [V] [TRT] Using opA: 0 opB: 1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: /fc/Gemm for ONNX node: /fc/Gemm\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering layer: fc.bias for ONNX node: fc.bias\n",
      "[07/24/2023-05:44:27] [V] [TRT] Registering tensor: outputs_1 for ONNX tensor: outputs\n",
      "[07/24/2023-05:44:27] [V] [TRT] /fc/Gemm [Gemm] outputs: [outputs -> (-1, 1000)[FLOAT]], \n",
      "[07/24/2023-05:44:27] [V] [TRT] Marking outputs_1 as output: outputs\n",
      "[07/24/2023-05:44:27] [I] Finished parsing network model. Parse time: 0.0934104\n",
      "[07/24/2023-05:44:27] [V] [TRT] Original: 81 layers\n",
      "[07/24/2023-05:44:27] [V] [TRT] After dead-layer removal: 81 layers\n",
      "[07/24/2023-05:44:27] [V] [TRT] Graph construction completed in 0.00198041 seconds.\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_0 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_2\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_2 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_3\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_3 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_4\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_4 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_1 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_5\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_5 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_7\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_7 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_8\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_8 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_9\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_9 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_6\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_6 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_10\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_10 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_12\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_12 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_13\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_13 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_14\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_14 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_11\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_11 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_15\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_15 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_17\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_17 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_18\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_18 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_19\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_19 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: IdentityToCastTransform on Identity_16\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_16 from IDENTITY to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConstShuffleFusion on fc.bias\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConstShuffleFusion: Fusing fc.bias with (Unnamed Layer* 84) [Shuffle]\n",
      "[07/24/2023-05:44:27] [V] [TRT] After Myelin optimization: 80 layers\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: MatMulToConvTransform on /fc/Gemm\n",
      "[07/24/2023-05:44:27] [V] [TRT] Convert layer type of /fc/Gemm from MATRIX_MULTIPLY to CONVOLUTION\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ShuffleShuffleFusion on /global_pool/flatten/Flatten\n",
      "[07/24/2023-05:44:27] [V] [TRT] ShuffleShuffleFusion: Fusing /global_pool/flatten/Flatten with reshape_before_/fc/Gemm\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ShuffleErasure on /global_pool/flatten/Flatten + reshape_before_/fc/Gemm\n",
      "[07/24/2023-05:44:27] [V] [TRT] Removing /global_pool/flatten/Flatten + reshape_before_/fc/Gemm\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReshapeBiasAddFusion on /fc/Gemm\n",
      "[07/24/2023-05:44:27] [V] [TRT] Applying ScaleNodes fusions.\n",
      "[07/24/2023-05:44:27] [V] [TRT] After scale fusion: 77 layers\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_0\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_0 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_2\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_2 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_3\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_3 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_4\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_4 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_1\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_1 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_5\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_5 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_7\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_7 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_8\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_8 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_9\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_9 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_6\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_6 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_10\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_10 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_12\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_12 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_13\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_13 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_14\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_14 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_11\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_11 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_15\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_15 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_17\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_17 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_18\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_18 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_19\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_19 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: CastToCopyTransform on Identity_16\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of Identity_16 from CAST to CAST\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConstantSplit on onnx::Conv_239\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConstantSplit on onnx::Conv_224\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConstantSplit on onnx::Conv_209\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConstantSplit on onnx::Conv_194\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /conv1/Conv with /act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.0/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.0/conv1/Conv with /layer1/layer1.0/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvEltwiseSumFusion on /layer1/layer1.0/conv2/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer1/layer1.0/conv2/Conv with /layer1/layer1.0/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add with /layer1/layer1.0/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.1/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.1/conv1/Conv with /layer1/layer1.1/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvEltwiseSumFusion on /layer1/layer1.1/conv2/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer1/layer1.1/conv2/Conv with /layer1/layer1.1/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add with /layer1/layer1.1/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.0/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.0/conv1/Conv with /layer2/layer2.0/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvEltwiseSumFusion on /layer2/layer2.0/downsample/downsample.0/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer2/layer2.0/downsample/downsample.0/Conv with /layer2/layer2.0/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add with /layer2/layer2.0/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.1/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.1/conv1/Conv with /layer2/layer2.1/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvEltwiseSumFusion on /layer2/layer2.1/conv2/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer2/layer2.1/conv2/Conv with /layer2/layer2.1/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add with /layer2/layer2.1/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.0/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.0/conv1/Conv with /layer3/layer3.0/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvEltwiseSumFusion on /layer3/layer3.0/downsample/downsample.0/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer3/layer3.0/downsample/downsample.0/Conv with /layer3/layer3.0/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add with /layer3/layer3.0/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.1/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.1/conv1/Conv with /layer3/layer3.1/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvEltwiseSumFusion on /layer3/layer3.1/conv2/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer3/layer3.1/conv2/Conv with /layer3/layer3.1/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add with /layer3/layer3.1/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.0/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.0/conv1/Conv with /layer4/layer4.0/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvEltwiseSumFusion on /layer4/layer4.0/downsample/downsample.0/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer4/layer4.0/downsample/downsample.0/Conv with /layer4/layer4.0/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add with /layer4/layer4.0/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.1/conv1/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.1/conv1/Conv with /layer4/layer4.1/act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvEltwiseSumFusion on /layer4/layer4.1/conv2/Conv\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvEltwiseSumFusion: Fusing /layer4/layer4.1/conv2/Conv with /layer4/layer4.1/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ConvReluFusion on /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add\n",
      "[07/24/2023-05:44:27] [V] [TRT] ConvReluFusion: Fusing /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add with /layer4/layer4.1/act2/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] Running: ReduceToPoolingFusion on /global_pool/pool/GlobalAveragePool\n",
      "[07/24/2023-05:44:27] [V] [TRT] Swap the layer type of /global_pool/pool/GlobalAveragePool from REDUCE to POOLING\n",
      "[07/24/2023-05:44:27] [V] [TRT] After dupe layer removal: 64 layers\n",
      "[07/24/2023-05:44:27] [V] [TRT] After final dead-layer removal: 64 layers\n",
      "[07/24/2023-05:44:27] [V] [TRT] After tensor merging: 64 layers\n",
      "[07/24/2023-05:44:27] [V] [TRT] After vertical fusions: 64 layers\n",
      "[07/24/2023-05:44:27] [V] [TRT] After dupe layer removal: 64 layers\n",
      "[07/24/2023-05:44:27] [V] [TRT] After final dead-layer removal: 64 layers\n",
      "[07/24/2023-05:44:27] [V] [TRT] After tensor merging: 64 layers\n",
      "[07/24/2023-05:44:27] [V] [TRT] After slice removal: 64 layers\n",
      "[07/24/2023-05:44:27] [V] [TRT] After concat removal: 64 layers\n",
      "[07/24/2023-05:44:27] [V] [TRT] Trying to split Reshape and strided tensor\n",
      "[07/24/2023-05:44:27] [I] [TRT] Graph optimization time: 0.0176284 seconds.\n",
      "[07/24/2023-05:44:27] [V] [TRT] Building graph using backend strategy 2\n",
      "[07/24/2023-05:44:27] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[07/24/2023-05:44:27] [V] [TRT] Constructing optimization profile number 0 [1/1].\n",
      "[07/24/2023-05:44:27] [V] [TRT] Applying generic optimizations to the graph for inference.\n",
      "[07/24/2023-05:44:27] [V] [TRT] Reserving memory for host IO tensors. Host: 0 bytes\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_239\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_239_clone_1\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_239_clone_2\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_239_clone_3\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_241\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(4608,9,3,1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_224\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_224_clone_1\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_224_clone_2\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_224_clone_3\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_226\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(2304,9,3,1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_209\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_209_clone_1\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_209_clone_2\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_209_clone_3\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_211\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1152,9,3,1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_194\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_194_clone_1\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_194_clone_2\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_194_clone_3\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for onnx::Conv_199\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination:  -> Float(576,9,3,1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] =============== Computing costs for /conv1/Conv + /act1/Relu\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination: Float(150528,50176,224,1) -> Float(802816,12544,112,1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 1.07935\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.598309\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.78619\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 1.12304\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.799305\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 1.14381\n",
      "[07/24/2023-05:44:27] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.65786\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.790089\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.728942\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.597701\n",
      "[07/24/2023-05:44:27] [V] [TRT] /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0942558 seconds. Fastest Tactic: 0xf64396b97c889179 Time: 0.597701\n",
      "[07/24/2023-05:44:27] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:27] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:27] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CudnnConvolution[0x80000000])\n",
      "[07/24/2023-05:44:27] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xf64396b97c889179\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination: Float(150528,1,672,3) -> Float(802816,1,7168,64) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 1.5044\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.80384\n",
      "[07/24/2023-05:44:27] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 2.6153\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.558958\n",
      "[07/24/2023-05:44:27] [V] [TRT] Fast skip Tactic:0x0a143be7a52f301a which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 2.23866\n",
      "[07/24/2023-05:44:27] [V] [TRT] Fast skip Tactic:0xa6448a1e79f1ca6f which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 1.77357\n",
      "[07/24/2023-05:44:27] [V] [TRT] /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0458245 seconds. Fastest Tactic: 0xf231cca3335919a4 Time: 0.558958\n",
      "[07/24/2023-05:44:27] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:27] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xf231cca3335919a4\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,224,1) -> Float(802816,12544,112,1) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 3.60901\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 3.49872\n",
      "[07/24/2023-05:44:27] [V] [TRT] /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0670879 seconds. Fastest Tactic: 0xe0a307ffe0ffb6a5 Time: 3.49872\n",
      "[07/24/2023-05:44:27] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:27] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xe0a307ffe0ffb6a5\n",
      "[07/24/2023-05:44:27] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,224,1) -> Float(200704,1:4,1792,16) ***************\n",
      "[07/24/2023-05:44:27] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:27] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 2.45321\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 2.77884\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.680494\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.770341\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.586752\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.853431\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.599333\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.677595\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.939557\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.678784\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.655899\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.538194\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.498395\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.490789\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.635611\n",
      "[07/24/2023-05:44:28] [V] [TRT] Fast skip Tactic:0x1acd4f006848c62b which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 1.02131\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.662967\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.589093\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.636782\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.672037\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.631314\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.587872\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.563054\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.597275\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.653605\n",
      "[07/24/2023-05:44:28] [V] [TRT] Fast skip Tactic:0x65e41d81f093b482 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 3.25734\n",
      "[07/24/2023-05:44:28] [V] [TRT] Fast skip Tactic:0x22cadc265a3b2e32 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 1.16506\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.724699\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.581925\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.585435\n",
      "[07/24/2023-05:44:28] [V] [TRT] /conv1/Conv + /act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.254258 seconds. Fastest Tactic: 0x9cb304e2edbc1221 Time: 0.490789\n",
      "[07/24/2023-05:44:28] [V] [TRT] --------------- Timing Runner: /conv1/Conv + /act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:28] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9cb304e2edbc1221\n",
      "[07/24/2023-05:44:28] [V] [TRT] =============== Computing costs for /maxpool/MaxPool\n",
      "[07/24/2023-05:44:28] [V] [TRT] *************** Autotuning format combination: Float(802816,12544,112,1) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:28] [V] [TRT] --------------- Timing Runner: /maxpool/MaxPool (CudnnPooling[0x80000005])\n",
      "[07/24/2023-05:44:28] [V] [TRT] CudnnPooling has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:28] [V] [TRT] --------------- Timing Runner: /maxpool/MaxPool (CaskPooling[0x8000002f])\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads855 Tactic: 0xf86a4e1f189f4821 Time: 0.514779\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads1017 Tactic: 0x7b9e5e445528b90a Time: 0.216969\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll2_tThreads841 Tactic: 0xdcecadaa3ad74a2c Time: 0.335433\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll5_tThreads841 Tactic: 0x7bd883ae684d33e0 Time: 0.272969\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads791 Tactic: 0xa23e43dae6aa4fa6 Time: 0.251465\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads855 Tactic: 0x76d52bcd240dc832 Time: 0.24064\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_nd_NCDHW_kMAX_kGENERIC_3D_POOLING_MODE_kFLOAT_0 Tactic: 0x5faf4a0a8a5670ed Time: 0.19339\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll6_tThreads225 Tactic: 0xdb90d0acdc9fc4e1 Time: 0.44032\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads1017 Tactic: 0x4cf88ed475f74f6e Time: 0.459483\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads791 Tactic: 0x8bb5080c88a2b679 Time: 0.226597\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads513 Tactic: 0xcb3875826530ea38 Time: 0.244736\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll4_tThreads225 Tactic: 0xf21b9b7ab2973d3e Time: 0.418523\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_custom_tP4_tQ32_tRS3_tUV2 Tactic: 0x2639d3932b27ac67 Time: 0.193682\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll4_tThreads255 Tactic: 0xfcb5fcaa68fff7ac Time: 0.350501\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads855 Tactic: 0x5f5e601b4a0531ed Time: 0.231131\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll1_tThreads255 Tactic: 0x720a9978546d77bf Time: 0.362935\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads1017 Tactic: 0x7647ea605d1f4493 Time: 0.223232\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads855 Tactic: 0xd1e105c97697b1fe Time: 0.265655\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll1_tThreads225 Tactic: 0x7ca4fea88e05bd2d Time: 0.432713\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll1_tThreads841 Tactic: 0x28ce1402b45cc05e Time: 0.525001\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll6_tThreads255 Tactic: 0xd53eb77c06f70e73 Time: 0.365714\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll3_tThreads225 Tactic: 0x552fb57ee00d44f2 Time: 0.415305\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll6_tThreads841 Tactic: 0x8ffa3a06e6c6b992 Time: 0.271209\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads1017 Tactic: 0x6df482284d70bfa1 Time: 0.215973\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads513 Tactic: 0x169187fc85b39995 Time: 0.27136\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll5_tThreads255 Tactic: 0x211c0ed4887c8401 Time: 0.356352\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads1017 Tactic: 0x5a9252b86daf49c5 Time: 0.283451\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll3_tThreads841 Tactic: 0x01455fd4da543981 Time: 0.290542\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Max Tactic: 0xb59f9cfb90407c92 Time: 0.194121\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads513 Tactic: 0xe2b33e540b3813e7 Time: 0.411355\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads513 Tactic: 0xb1a5a9f8d729e059 Time: 0.238299\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads791 Tactic: 0xd8a39fa054b345c7 Time: 0.346258\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll2_tThreads255 Tactic: 0x862820d0dae6fdcd Time: 0.346843\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads791 Tactic: 0x2c812608da38cfb5 Time: 0.57973\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads513 Tactic: 0x6c0c5b8637aa93f4 Time: 0.239931\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads791 Tactic: 0x7f97b1a406293c0b Time: 0.237216\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads855 Tactic: 0xab7cd9b3c48ebb9f Time: 0.233472\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads513 Tactic: 0x4587105059a26a2b Time: 0.237568\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll2_tThreads225 Tactic: 0x88864700008e375f Time: 0.414281\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads1017 Tactic: 0x574be69c6598b45c Time: 0.245614\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads791 Tactic: 0x050a6ddeb430366a Time: 0.281746\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads855 Tactic: 0x0c48f7b79614c253 Time: 0.316416\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll3_tThreads255 Tactic: 0x5b81d2ae3a658e60 Time: 0.347575\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll5_tThreads225 Tactic: 0x2fb2690452144e93 Time: 0.426715\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll4_tThreads841 Tactic: 0xa67171d088ce404d Time: 0.276041\n",
      "[07/24/2023-05:44:28] [V] [TRT] /maxpool/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.181443 seconds. Fastest Tactic: 0x5faf4a0a8a5670ed Time: 0.19339\n",
      "[07/24/2023-05:44:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x5faf4a0a8a5670ed\n",
      "[07/24/2023-05:44:28] [V] [TRT] *************** Autotuning format combination: Float(200704,1:4,1792,16) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:28] [V] [TRT] --------------- Timing Runner: /maxpool/MaxPool (CaskPooling[0x8000002f])\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_NOT_PROPAGATE_NAN_3D Tactic: 0xfa211b1cdd504de0 Time: 0.204654\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_PROPAGATE_NAN_3D Tactic: 0xe9d01a2a900075cb Time: 0.247808\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_NOT_PROPAGATE_NAN_2D Tactic: 0xaec8628e8180bced Time: 0.203483\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_PROPAGATE_NAN_3D Tactic: 0xd76bac5638836f8a Time: 0.220329\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_PROPAGATE_NAN_2D Tactic: 0x8382d5c464539e87 Time: 0.214894\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NHWC_Max_CAlign4 Tactic: 0x22fb1bb4a70e340d Time: 0.232155\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_NOT_PROPAGATE_NAN_3D Tactic: 0x2c7251cbae30cf74 Time: 0.232887\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_NOT_PROPAGATE_NAN_2D Tactic: 0x789b2859f2e03e79 Time: 0.19339\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_PROPAGATE_NAN_2D Tactic: 0xbd3963b8ccd084c6 Time: 0.193829\n",
      "[07/24/2023-05:44:28] [V] [TRT] /maxpool/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.0311655 seconds. Fastest Tactic: 0x789b2859f2e03e79 Time: 0.19339\n",
      "[07/24/2023-05:44:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x789b2859f2e03e79\n",
      "[07/24/2023-05:44:28] [V] [TRT] =============== Computing costs for /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu\n",
      "[07/24/2023-05:44:28] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(1) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:28] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.855186\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.463579\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.479776\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.845678\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.565979\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.882688\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.595383\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.422327\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.902729\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.861038\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.454802\n",
      "[07/24/2023-05:44:28] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.3097\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.898546\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.744448\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.634245\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.561591\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.906386\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.825961\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.515365\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.413257\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.371566\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.553838\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.50176\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.477623\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.496681\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.714459\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.414537\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.322706\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.499127\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.812032\n",
      "[07/24/2023-05:44:28] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.220514 seconds. Fastest Tactic: 0x0190806602534cfd Time: 0.322706\n",
      "[07/24/2023-05:44:28] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:28] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0190806602534cfd\n",
      "[07/24/2023-05:44:28] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(1) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:28] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 1.54653\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.537495\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.915017\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.439735\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.790674\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.794624\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.465774\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.442807\n",
      "[07/24/2023-05:44:28] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.00454\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.854455\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.605915\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.780873\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.846994\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.877568\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.498395\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.664576\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.500443\n",
      "[07/24/2023-05:44:28] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.13731 seconds. Fastest Tactic: 0xf48db81f02eca9ee Time: 0.439735\n",
      "[07/24/2023-05:44:28] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:28] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xf48db81f02eca9ee\n",
      "[07/24/2023-05:44:28] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:28] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.642743\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.67803\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.637221\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.63371\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.611035\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.601673\n",
      "[07/24/2023-05:44:28] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0439765 seconds. Fastest Tactic: 0x3104d85fecdd547c Time: 0.601673\n",
      "[07/24/2023-05:44:28] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:28] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3104d85fecdd547c\n",
      "[07/24/2023-05:44:28] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:28] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.821687\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.813495\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.474112\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.518258\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.489618\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.444416\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.328119\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.868777\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.345088\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.505417\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.893957\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.624005\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.616302\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.359131\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.34304\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.342601\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.606062\n",
      "[07/24/2023-05:44:28] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 0.712983\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.603429\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.40843\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.580462\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.483474\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.50176\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.399067\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.582071\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.338254\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.582802\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.393038\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.594651\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.622007\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 0.915895\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.521362\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.578267\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.487717\n",
      "[07/24/2023-05:44:29] [V] [TRT] /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.222704 seconds. Fastest Tactic: 0x3a8712b17741b582 Time: 0.328119\n",
      "[07/24/2023-05:44:29] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:29] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3a8712b17741b582\n",
      "[07/24/2023-05:44:29] [V] [TRT] =============== Computing costs for /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu\n",
      "[07/24/2023-05:44:29] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(1), Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:29] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.867328\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.472818\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.485769\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.852997\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.912581\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.917225\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.904777\n",
      "[07/24/2023-05:44:29] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.35373\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.901266\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.732014\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.633125\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.917783\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.859575\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.523557\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.424229\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.572416\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.505271\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.511854\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.721189\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.50571\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.8192\n",
      "[07/24/2023-05:44:29] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.172516 seconds. Fastest Tactic: 0x94119b4c514b211a Time: 0.424229\n",
      "[07/24/2023-05:44:29] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:29] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a\n",
      "[07/24/2023-05:44:29] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(1), Float(200704,1,3584,64) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:29] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 1.59257\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.532187\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.92901\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.474697\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.815982\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.805888\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.593627\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.463726\n",
      "[07/24/2023-05:44:29] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.00966\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.870263\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.630491\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.800329\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.864293\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.873088\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.49781\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.649362\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.50133\n",
      "[07/24/2023-05:44:29] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.139214 seconds. Fastest Tactic: 0x1da91d865428f237 Time: 0.463726\n",
      "[07/24/2023-05:44:29] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:29] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1da91d865428f237\n",
      "[07/24/2023-05:44:29] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1), Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:29] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.652581\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.717824\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.689591\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.632686\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.665015\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.616594\n",
      "[07/24/2023-05:44:29] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0463983 seconds. Fastest Tactic: 0x3104d85fecdd547c Time: 0.616594\n",
      "[07/24/2023-05:44:29] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:29] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3104d85fecdd547c\n",
      "[07/24/2023-05:44:29] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1), Float(50176,1:4,896,16) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:29] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.821979\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.822272\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.480695\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.527611\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.50688\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.461531\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.34933\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.883712\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.350501\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.518053\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.901243\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.659895\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.650386\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.366885\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.358807\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.355621\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.615863\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 0.723822\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.634587\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.407552\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.592018\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.492544\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.512878\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.385463\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.612059\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.356311\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.592165\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.401408\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.62021\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.652142\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 0.925289\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.526775\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.609426\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.498688\n",
      "[07/24/2023-05:44:29] [V] [TRT] /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.229262 seconds. Fastest Tactic: 0x3a8712b17741b582 Time: 0.34933\n",
      "[07/24/2023-05:44:29] [V] [TRT] --------------- Timing Runner: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:29] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3a8712b17741b582\n",
      "[07/24/2023-05:44:29] [V] [TRT] =============== Computing costs for /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/act1/Relu\n",
      "[07/24/2023-05:44:29] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(1) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:29] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(1) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:29] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:29] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:29] [V] [TRT] =============== Computing costs for /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu\n",
      "[07/24/2023-05:44:29] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(576,9,3,1), Float(1), Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:29] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.862208\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.470309\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.485815\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.853138\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.933157\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.922935\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.911214\n",
      "[07/24/2023-05:44:29] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.3567\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.900827\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.731282\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.634994\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.931255\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.873033\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.529257\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.429349\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.576219\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.524402\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.523557\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.734647\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.508343\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.825051\n",
      "[07/24/2023-05:44:29] [V] [TRT] /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.170074 seconds. Fastest Tactic: 0x94119b4c514b211a Time: 0.429349\n",
      "[07/24/2023-05:44:29] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:29] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a\n",
      "[07/24/2023-05:44:29] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(576,9,3,1), Float(1), Float(200704,1,3584,64) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:29] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 1.61997\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.545792\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.939593\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.468553\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.811154\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.803264\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.602523\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.472064\n",
      "[07/24/2023-05:44:29] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.01786\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.86016\n",
      "[07/24/2023-05:44:29] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.624201\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.792283\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.877403\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.88693\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.51317\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.6656\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.488741\n",
      "[07/24/2023-05:44:30] [V] [TRT] /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.143851 seconds. Fastest Tactic: 0xf48db81f02eca9ee Time: 0.468553\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:30] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xf48db81f02eca9ee\n",
      "[07/24/2023-05:44:30] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(576,9,3,1), Float(1), Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.637659\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.749129\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.722213\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.664553\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.683154\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.635319\n",
      "[07/24/2023-05:44:30] [V] [TRT] /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0473567 seconds. Fastest Tactic: 0x3104d85fecdd547c Time: 0.635319\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:30] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3104d85fecdd547c\n",
      "[07/24/2023-05:44:30] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(576,9,3,1), Float(1), Float(50176,1:4,896,16) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.834706\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.834121\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.488594\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.534821\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.504247\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.473673\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.359278\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.901883\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.357367\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.520923\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.914578\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.627566\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.614546\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.369518\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.360873\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.357669\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.600942\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 0.735086\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.64512\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.420279\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.600795\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.495323\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.515977\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.393851\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.598455\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.363081\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.596261\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.40731\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.624448\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.658725\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 0.933742\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.532919\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.601673\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.50571\n",
      "[07/24/2023-05:44:30] [V] [TRT] /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.224152 seconds. Fastest Tactic: 0xb6f6563c77d057d7 Time: 0.357367\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:30] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xb6f6563c77d057d7\n",
      "[07/24/2023-05:44:30] [V] [TRT] =============== Computing costs for /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu\n",
      "[07/24/2023-05:44:30] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.254679\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.247954\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.251026\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.25205\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.359424\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.452768\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.345527\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.456558\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.249125\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.288521\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.652873\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.248247\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.37376\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.308366\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.309979\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.442514\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.230405\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.250734\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.286427\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.253367\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.278674\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.280137\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.406674\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.265161\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.254683\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.231429\n",
      "[07/24/2023-05:44:30] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.12007 seconds. Fastest Tactic: 0x4efce38acc876f5c Time: 0.230405\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:30] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CudnnConvolution[0x80000000])\n",
      "[07/24/2023-05:44:30] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x4efce38acc876f5c\n",
      "[07/24/2023-05:44:30] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.446903\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.27253\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.466359\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.256146\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.393216\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.389413\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.264631\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.254683\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.496055\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.41867\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.320512\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.388096\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.252782\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.428914\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.256731\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.324635\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.251584\n",
      "[07/24/2023-05:44:30] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0821273 seconds. Fastest Tactic: 0x94a7db94ba744c45 Time: 0.251584\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:30] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94a7db94ba744c45\n",
      "[07/24/2023-05:44:30] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.187122\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.198363\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.189024\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.193243\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.186222\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.187831\n",
      "[07/24/2023-05:44:30] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0185815 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.186222\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:30] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/24/2023-05:44:30] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.283648\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.283063\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.24421\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.266094\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.283502\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.229961\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.176247\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.438418\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.184759\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.293888\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.461239\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.334697\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.329289\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.19061\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.180663\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.180059\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.34304\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 0.357376\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.179493\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.257755\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.311474\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.245422\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.291255\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.254245\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.329874\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.182418\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.310693\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.206555\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.177152\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.185166\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 0.476891\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.29813\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.324315\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.251611\n",
      "[07/24/2023-05:44:30] [V] [TRT] /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.128498 seconds. Fastest Tactic: 0x3a8712b17741b582 Time: 0.176247\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:30] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3a8712b17741b582\n",
      "[07/24/2023-05:44:30] [V] [TRT] =============== Computing costs for /layer2/layer2.0/conv2/Conv\n",
      "[07/24/2023-05:44:30] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(1) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.487424\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.476599\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.480645\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.478181\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.624347\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.86896\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.590702\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.450121\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.888978\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.476448\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.501467\n",
      "[07/24/2023-05:44:30] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.28205\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.47627\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.701294\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.597285\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.540087\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.876544\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.445879\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.486254\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.428178\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.372421\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.550766\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.500882\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.464603\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.54155\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.70027\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.413454\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.357074\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.492631\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.444123\n",
      "[07/24/2023-05:44:30] [V] [TRT] /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.207751 seconds. Fastest Tactic: 0x0190806602534cfd Time: 0.357074\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:30] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0190806602534cfd\n",
      "[07/24/2023-05:44:30] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(1) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:30] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.8448\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.515803\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.45963\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.758784\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.758053\n",
      "[07/24/2023-05:44:30] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.427739\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.472165\n",
      "[07/24/2023-05:44:31] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.00109\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.820224\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.765513\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.458199\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.849774\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.479525\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.63093\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.477915\n",
      "[07/24/2023-05:44:31] [V] [TRT] /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.112841 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.427739\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/24/2023-05:44:31] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.357669\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.375954\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.357522\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.366446\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.337774\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.342455\n",
      "[07/24/2023-05:44:31] [V] [TRT] /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0286824 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.337774\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/24/2023-05:44:31] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.48123\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.47259\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.469577\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.506002\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.33163\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.325632\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.344754\n",
      "[07/24/2023-05:44:31] [V] [TRT] /layer2/layer2.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0375614 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.325632\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/24/2023-05:44:31] [V] [TRT] =============== Computing costs for /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu\n",
      "[07/24/2023-05:44:31] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(1), Float(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.0740366\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.0776069\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.07408\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24 Time: 0.0739794\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.080384\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675 Time: 0.0925257\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.0787017\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.0724846\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd Time: 0.0731269\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.106226\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.0754994\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.0737211\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e Time: 0.0726309\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303 Time: 0.0740206\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.0804571\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338 Time: 0.0852846\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.0753371\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.0774583\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.0740937\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0749211\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.0720457\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96 Time: 0.0743863\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.0738949\n",
      "[07/24/2023-05:44:31] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0586648 seconds. Fastest Tactic: 0xa31d27de74b895ff Time: 0.0720457\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa31d27de74b895ff\n",
      "[07/24/2023-05:44:31] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(1), Float(100352,1,3584,128) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.0651505\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2 Time: 0.0784091\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.0729966\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484 Time: 0.0652434\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86 Time: 0.0710461\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.11147\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb Time: 0.0707535\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.0718263\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.0797143\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4 Time: 0.0658667\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.0613912\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d Time: 0.0645486\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.0663665\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.0652114\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.0726057\n",
      "[07/24/2023-05:44:31] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0406583 seconds. Fastest Tactic: 0x7121ec1db3f80c67 Time: 0.0613912\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7121ec1db3f80c67\n",
      "[07/24/2023-05:44:31] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1), Float(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.072192\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_alignc4 Tactic: 0xc8ad2c0ce0af5623 Time: 0.0675383\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.0697783\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.0608549\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x1d144cf9675b8d6f Time: 0.0612038\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.0612099\n",
      "[07/24/2023-05:44:31] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0166892 seconds. Fastest Tactic: 0xe0a307ffe0ffb6a5 Time: 0.0608549\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xe0a307ffe0ffb6a5\n",
      "[07/24/2023-05:44:31] [V] [TRT] *************** Autotuning format combination: Float(50176,1:4,896,16), Float(1), Float(25088,1:4,896,32) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.0646583\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.0646507\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.0795794\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4 Time: 0.0646583\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d Time: 0.0688518\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7 Time: 0.0785783\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798 Time: 0.0576366\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582 Time: 0.0545112\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232 Time: 0.0789943\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3 Time: 0.0652343\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7 Time: 0.0549059\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638 Time: 0.0698758\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087 Time: 0.0763611\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700 Time: 0.0751863\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067 Time: 0.0754834\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba Time: 0.0544305\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef Time: 0.0541379\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221 Time: 0.0541242\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23 Time: 0.0775314\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b Time: 0.0617493\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b Time: 0.070688\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.0624\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677 Time: 0.0582339\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1 Time: 0.0654552\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422 Time: 0.0628541\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028 Time: 0.067779\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde Time: 0.0578697\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76 Time: 0.075776\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64 Time: 0.054531\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0 Time: 0.0675352\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9 Time: 0.0560274\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.0620312\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32 Time: 0.0734354\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0 Time: 0.0692815\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5 Time: 0.0767269\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd Time: 0.0606111\n",
      "[07/24/2023-05:44:31] [V] [TRT] /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0918598 seconds. Fastest Tactic: 0x9cb304e2edbc1221 Time: 0.0541242\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9cb304e2edbc1221\n",
      "[07/24/2023-05:44:31] [V] [TRT] =============== Computing costs for /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu\n",
      "[07/24/2023-05:44:31] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(1) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.486711\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.476599\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.480695\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.478062\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.625481\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.869262\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.590848\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.449829\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.888978\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.476599\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.501358\n",
      "[07/24/2023-05:44:31] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.28109\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.475447\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.701001\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.596965\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.549595\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.872887\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.44427\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.491374\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.437833\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.381074\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.552814\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.504978\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.468114\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.539392\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.700709\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.413522\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.356791\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.486985\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.437541\n",
      "[07/24/2023-05:44:31] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.213155 seconds. Fastest Tactic: 0x0190806602534cfd Time: 0.356791\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0190806602534cfd\n",
      "[07/24/2023-05:44:31] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(1) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.839387\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.508905\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.453632\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.751762\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.751031\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.405358\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.450121\n",
      "[07/24/2023-05:44:31] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.01069\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.85051\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.790821\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.461376\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.852699\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.485495\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.631077\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.478062\n",
      "[07/24/2023-05:44:31] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.110886 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.405358\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/24/2023-05:44:31] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.35328\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.371671\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.353225\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.354597\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.334117\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.339429\n",
      "[07/24/2023-05:44:31] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0284485 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.334117\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/24/2023-05:44:31] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.468699\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.469577\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.466213\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.500005\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.328997\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.322999\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.341577\n",
      "[07/24/2023-05:44:31] [V] [TRT] /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0405307 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.322999\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/24/2023-05:44:31] [V] [TRT] =============== Computing costs for /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu\n",
      "[07/24/2023-05:44:31] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(1152,9,3,1), Float(1), Float(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.486592\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.476599\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.480841\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.477915\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.868059\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.883858\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.464603\n",
      "[07/24/2023-05:44:31] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.27181\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.503077\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.726747\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.624347\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.896439\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.47429\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.500443\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.393504\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.568142\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.518729\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.560567\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.712558\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.499127\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.461056\n",
      "[07/24/2023-05:44:31] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.141967 seconds. Fastest Tactic: 0x94119b4c514b211a Time: 0.393504\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a\n",
      "[07/24/2023-05:44:31] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(1152,9,3,1), Float(1), Float(100352,1,3584,128) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:31] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.884197\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.537746\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.492105\n",
      "[07/24/2023-05:44:31] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.779438\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.778094\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.478784\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.499858\n",
      "[07/24/2023-05:44:32] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.00762\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.852407\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.792869\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.492544\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.892457\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.535259\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.686373\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.523703\n",
      "[07/24/2023-05:44:32] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.109106 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.478784\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/24/2023-05:44:32] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1152,9,3,1), Float(1), Float(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x9787b83bedcff6a2 Time: 0.395995\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.423643\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.40053\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xe0a307ffe0ffb6a5 Time: 0.398629\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.390144\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x3104d85fecdd547c Time: 0.386574\n",
      "[07/24/2023-05:44:32] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0305768 seconds. Fastest Tactic: 0x3104d85fecdd547c Time: 0.386574\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3104d85fecdd547c\n",
      "[07/24/2023-05:44:32] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1152,9,3,1), Float(1), Float(25088,1:4,896,32) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.509806\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.503269\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.495909\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.53131\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.370949\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.363227\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.381806\n",
      "[07/24/2023-05:44:32] [V] [TRT] /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.037604 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.363227\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/24/2023-05:44:32] [V] [TRT] =============== Computing costs for /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu\n",
      "[07/24/2023-05:44:32] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.24459\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.286322\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.282624\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.242542\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.329582\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.507854\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.364544\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.447781\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.240055\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.269312\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.684325\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.240055\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.372736\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.302235\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.335579\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.464923\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.218135\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.245614\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.341198\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.291109\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.287246\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.272101\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.374578\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.261829\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.248978\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.21899\n",
      "[07/24/2023-05:44:32] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.145836 seconds. Fastest Tactic: 0x4efce38acc876f5c Time: 0.218135\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CudnnConvolution[0x80000000])\n",
      "[07/24/2023-05:44:32] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x4efce38acc876f5c\n",
      "[07/24/2023-05:44:32] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.234789\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.25995\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.230985\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.410464\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.379026\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.203483\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.228498\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.515657\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.435931\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.378423\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.227474\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.419986\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.239616\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.353125\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.234002\n",
      "[07/24/2023-05:44:32] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0714681 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.203483\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/24/2023-05:44:32] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.175104\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.166619\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.163109\n",
      "[07/24/2023-05:44:32] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0100712 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.163109\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/24/2023-05:44:32] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.248558\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.247954\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.26635\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.285211\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.165595\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.163127\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.172229\n",
      "[07/24/2023-05:44:32] [V] [TRT] /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0309821 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.163127\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/24/2023-05:44:32] [V] [TRT] =============== Computing costs for /layer3/layer3.0/conv2/Conv\n",
      "[07/24/2023-05:44:32] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(1) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.486693\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.573733\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.552375\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.470601\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.613083\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.980261\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.684192\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.462409\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.880201\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.46485\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.492398\n",
      "[07/24/2023-05:44:32] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.34758\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.464896\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.694112\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.589271\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.639461\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.900928\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.422912\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.475909\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.438711\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.363813\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.64629\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.584389\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.5376\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.527067\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.693248\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.488594\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.350939\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.480997\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.423589\n",
      "[07/24/2023-05:44:32] [V] [TRT] /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.272863 seconds. Fastest Tactic: 0x0190806602534cfd Time: 0.350939\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0190806602534cfd\n",
      "[07/24/2023-05:44:32] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(1) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.430432\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.504978\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.441051\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.794917\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.744448\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.389413\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.437394\n",
      "[07/24/2023-05:44:32] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.01786\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.847287\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.74357\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.434437\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.835438\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.475282\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.699685\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.46987\n",
      "[07/24/2023-05:44:32] [V] [TRT] /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.118472 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.389413\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/24/2023-05:44:32] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.347575\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.322267\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.315685\n",
      "[07/24/2023-05:44:32] [V] [TRT] /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0161256 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.315685\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/24/2023-05:44:32] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.462117\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.460946\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.523703\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.555008\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.318318\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.312613\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.331922\n",
      "[07/24/2023-05:44:32] [V] [TRT] /layer3/layer3.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0483319 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.312613\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/24/2023-05:44:32] [V] [TRT] =============== Computing costs for /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu\n",
      "[07/24/2023-05:44:32] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(1), Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:32] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.0574903\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.0671451\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.0574903\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24 Time: 0.057571\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.0725577\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675 Time: 0.0465874\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.0656823\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.0460126\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd Time: 0.0499886\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.0955977\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.0586118\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.0534918\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e Time: 0.0459257\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303 Time: 0.047933\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.0751177\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338 Time: 0.0566522\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.061824\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.0668328\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.0487665\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27 Time: 0.062464\n",
      "[07/24/2023-05:44:32] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.048771\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96 Time: 0.0545646\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.0483368\n",
      "[07/24/2023-05:44:33] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0610183 seconds. Fastest Tactic: 0xc0b05b61d128e46e Time: 0.0459257\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc0b05b61d128e46e\n",
      "[07/24/2023-05:44:33] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(1), Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.046264\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.059392\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484 Time: 0.0441406\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86 Time: 0.0478842\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.0564038\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb Time: 0.0478842\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.0479924\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.0710461\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.0524678\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d Time: 0.0434469\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.057504\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.0440903\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.0536869\n",
      "[07/24/2023-05:44:33] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0339681 seconds. Fastest Tactic: 0x55d80c17b1cd982d Time: 0.0434469\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x55d80c17b1cd982d\n",
      "[07/24/2023-05:44:33] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1), Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.0488792\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_alignc4 Tactic: 0xc8ad2c0ce0af5623 Time: 0.044544\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.045568\n",
      "[07/24/2023-05:44:33] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00840439 seconds. Fastest Tactic: 0xc8ad2c0ce0af5623 Time: 0.044544\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc8ad2c0ce0af5623\n",
      "[07/24/2023-05:44:33] [V] [TRT] *************** Autotuning format combination: Float(25088,1:4,896,32), Float(1), Float(12544,1:4,896,64) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.0429486\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.0429794\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.0549547\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4 Time: 0.0426137\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7 Time: 0.0537356\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3 Time: 0.044288\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b Time: 0.0414263\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.041728\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.0419989\n",
      "[07/24/2023-05:44:33] [V] [TRT] /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0234851 seconds. Fastest Tactic: 0x130df49cb195156b Time: 0.0414263\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x130df49cb195156b\n",
      "[07/24/2023-05:44:33] [V] [TRT] =============== Computing costs for /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu\n",
      "[07/24/2023-05:44:33] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(1) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.486953\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.57344\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.552672\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.470601\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.61323\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.979968\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.684201\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.462263\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.880078\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.465042\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.492544\n",
      "[07/24/2023-05:44:33] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.3537\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.46501\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.693687\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.589385\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.639269\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.900978\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.42267\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.476197\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.439442\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.363666\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.647035\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.584265\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.538053\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.527214\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.693541\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.488507\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.350939\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.480987\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.423826\n",
      "[07/24/2023-05:44:33] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.274585 seconds. Fastest Tactic: 0x0190806602534cfd Time: 0.350939\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0190806602534cfd\n",
      "[07/24/2023-05:44:33] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(1) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.429349\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.504686\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.441198\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.794917\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.743717\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.389266\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.437541\n",
      "[07/24/2023-05:44:33] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.01786\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.857381\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.750885\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.436133\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.824466\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.473381\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.703927\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.468699\n",
      "[07/24/2023-05:44:33] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.115518 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.389266\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/24/2023-05:44:33] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.339707\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.321966\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.314514\n",
      "[07/24/2023-05:44:33] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0159773 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.314514\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/24/2023-05:44:33] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.461385\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.460654\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.522825\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.555008\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.318318\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.312759\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.332069\n",
      "[07/24/2023-05:44:33] [V] [TRT] /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0483883 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.312759\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/24/2023-05:44:33] [V] [TRT] =============== Computing costs for /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu\n",
      "[07/24/2023-05:44:33] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(2304,9,3,1), Float(1), Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.488887\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.576105\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.554423\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.473381\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.982715\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.882103\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.468699\n",
      "[07/24/2023-05:44:33] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.35373\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.488448\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.721042\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.618496\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.940617\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.449682\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.499127\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.409458\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.691639\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.622153\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.553979\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.716215\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.487131\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.439296\n",
      "[07/24/2023-05:44:33] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.145303 seconds. Fastest Tactic: 0x94119b4c514b211a Time: 0.409458\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a\n",
      "[07/24/2023-05:44:33] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(2304,9,3,1), Float(1), Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.53947\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.626395\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.565394\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.911506\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.855771\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.504393\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.547255\n",
      "[07/24/2023-05:44:33] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.11514\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.950711\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.846702\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.540526\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.949979\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.587607\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.814423\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.579584\n",
      "[07/24/2023-05:44:33] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.114787 seconds. Fastest Tactic: 0x8014228ec08b4d49 Time: 0.504393\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x8014228ec08b4d49\n",
      "[07/24/2023-05:44:33] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(2304,9,3,1), Float(1), Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.461824\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.440174\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.432567\n",
      "[07/24/2023-05:44:33] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0193912 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.432567\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/24/2023-05:44:33] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(2304,9,3,1), Float(1), Float(12544,1:4,896,64) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.568466\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.567451\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.629906\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.660041\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.425984\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.419296\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.438565\n",
      "[07/24/2023-05:44:33] [V] [TRT] /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0432283 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.419296\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/24/2023-05:44:33] [V] [TRT] =============== Computing costs for /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu\n",
      "[07/24/2023-05:44:33] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:33] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.251465\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.278126\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.280283\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.24208\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.308663\n",
      "[07/24/2023-05:44:33] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.740791\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.340846\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.51083\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.236983\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.251369\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.734939\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.240055\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.429349\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.347867\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.305006\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.496933\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.422181\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.285257\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.352402\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.295936\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.277943\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.269897\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.430958\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.236983\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.287744\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.42267\n",
      "[07/24/2023-05:44:34] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.253952 seconds. Fastest Tactic: 0x5aa723e0481da855 Time: 0.236983\n",
      "[07/24/2023-05:44:34] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:34] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:34] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CudnnConvolution[0x80000000])\n",
      "[07/24/2023-05:44:34] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:34] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5aa723e0481da855\n",
      "[07/24/2023-05:44:34] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:34] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.238885\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.296667\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.226304\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.43637\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.434761\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.388261\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.222939\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.547255\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.463433\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.431543\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.222793\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.480256\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.273701\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.356073\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.269312\n",
      "[07/24/2023-05:44:34] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0998649 seconds. Fastest Tactic: 0xd15dd11d64344e83 Time: 0.222793\n",
      "[07/24/2023-05:44:34] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:34] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:34] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd15dd11d64344e83\n",
      "[07/24/2023-05:44:34] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:34] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.168814\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.160329\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.157102\n",
      "[07/24/2023-05:44:34] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0144847 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.157102\n",
      "[07/24/2023-05:44:34] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:34] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:34] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/24/2023-05:44:34] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:34] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.236992\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.23669\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.266679\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.282917\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.160183\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.157403\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.166766\n",
      "[07/24/2023-05:44:34] [V] [TRT] /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.041031 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.157403\n",
      "[07/24/2023-05:44:34] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:34] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:34] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/24/2023-05:44:34] [V] [TRT] =============== Computing costs for /layer4/layer4.0/conv2/Conv\n",
      "[07/24/2023-05:44:34] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1), Float(1) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:34] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.548718\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.605477\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.548978\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.475415\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.596261\n",
      "[07/24/2023-05:44:34] [V] [TRT] Fast skip Tactic:0xa9366041633a5135 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 1.23699\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.639854\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.884736\n",
      "[07/24/2023-05:44:34] [V] [TRT] Fast skip Tactic:0xcb8a43f748d8a338 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 1.01478\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.464165\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.482158\n",
      "[07/24/2023-05:44:34] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.45408\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.467822\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.805888\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.683154\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.597138\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.963145\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.832489\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.556672\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.877897\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.705243\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.659602\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.645568\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.499273\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.524974\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.800914\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.447195\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.684928\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.559986\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.834706\n",
      "[07/24/2023-05:44:34] [V] [TRT] /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.534615 seconds. Fastest Tactic: 0x3712e3e595645874 Time: 0.447195\n",
      "[07/24/2023-05:44:34] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:34] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:34] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3712e3e595645874\n",
      "[07/24/2023-05:44:34] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512), Float(1) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:34] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.463671\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.57973\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.436809\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.847726\n",
      "[07/24/2023-05:44:34] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.845664\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.762295\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.432859\n",
      "[07/24/2023-05:44:35] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.0711\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.892763\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.852992\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.433737\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.994309\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.536137\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.689883\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.52853\n",
      "[07/24/2023-05:44:35] [V] [TRT] /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.187508 seconds. Fastest Tactic: 0x1da91d865428f237 Time: 0.432859\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:35] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1da91d865428f237\n",
      "[07/24/2023-05:44:35] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(1) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.330167\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.312466\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.30603\n",
      "[07/24/2023-05:44:35] [V] [TRT] /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0317472 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.30603\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:35] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/24/2023-05:44:35] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.470894\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.46197\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.528969\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.555008\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.313051\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.307493\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.32677\n",
      "[07/24/2023-05:44:35] [V] [TRT] /layer4/layer4.0/conv2/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0740406 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.307493\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/conv2/Conv (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:35] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/24/2023-05:44:35] [V] [TRT] =============== Computing costs for /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu\n",
      "[07/24/2023-05:44:35] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(1), Float(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.0462526\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.0436663\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.0469383\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24 Time: 0.0464423\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.087168\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675 Time: 0.041208\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.0684617\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.0373029\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd Time: 0.0457486\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.101083\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.060259\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.0496884\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e Time: 0.0375611\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303 Time: 0.040352\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.0906971\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338 Time: 0.0398263\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.0577829\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.044544\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.04164\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0410011\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.0449486\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96 Time: 0.0558811\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.0621318\n",
      "[07/24/2023-05:44:35] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0818694 seconds. Fastest Tactic: 0x5aa723e0481da855 Time: 0.0373029\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:35] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5aa723e0481da855\n",
      "[07/24/2023-05:44:35] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(1), Float(25088,1,3584,512) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.0412594\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.0571977\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484 Time: 0.0418469\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86 Time: 0.0377669\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.057411\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb Time: 0.036872\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.0376686\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.0709486\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.0541745\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d Time: 0.0384789\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.0602331\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.0389097\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.0490057\n",
      "[07/24/2023-05:44:35] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0357752 seconds. Fastest Tactic: 0x1022069e6f8d9aeb Time: 0.036872\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:35] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1022069e6f8d9aeb\n",
      "[07/24/2023-05:44:35] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1), Float(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.0336128\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_alignc4 Tactic: 0xc8ad2c0ce0af5623 Time: 0.0320649\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.032885\n",
      "[07/24/2023-05:44:35] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00866265 seconds. Fastest Tactic: 0xc8ad2c0ce0af5623 Time: 0.0320649\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:35] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc8ad2c0ce0af5623\n",
      "[07/24/2023-05:44:35] [V] [TRT] *************** Autotuning format combination: Float(12544,1:4,896,64), Float(1), Float(6272,1:4,896,128) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.039456\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.0397897\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.044064\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4 Time: 0.0393874\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7 Time: 0.0432309\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3 Time: 0.0367611\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b Time: 0.0294034\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.0298889\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.0299054\n",
      "[07/24/2023-05:44:35] [V] [TRT] /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0241658 seconds. Fastest Tactic: 0x130df49cb195156b Time: 0.0294034\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:35] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x130df49cb195156b\n",
      "[07/24/2023-05:44:35] [V] [TRT] =============== Computing costs for /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu\n",
      "[07/24/2023-05:44:35] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1), Float(1) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.548718\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.606354\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.548901\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.475136\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.596846\n",
      "[07/24/2023-05:44:35] [V] [TRT] Fast skip Tactic:0xa9366041633a5135 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 1.23802\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.639561\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x32x8_warpsize8x1x1_wngd2x2 Tactic: 0xe47e164f4a743900 Time: 0.885029\n",
      "[07/24/2023-05:44:35] [V] [TRT] Fast skip Tactic:0xcb8a43f748d8a338 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 1.00762\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.464187\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x7fc93550f5b9c127 Time: 0.482158\n",
      "[07/24/2023-05:44:35] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.45715\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.46709\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.806034\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.682862\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.596846\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.962706\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.832366\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.557202\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x16x8_warpsize8x1x1_wngd2x2 Tactic: 0xe38e9dfd56c33779 Time: 0.872594\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.705682\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.659456\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.645998\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0xe640ceafd7d34ca9 Time: 0.498395\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.525605\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.803547\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4_beta0_packed_stride Tactic: 0x3712e3e595645874 Time: 0.447342\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_wngd_f32f32_f32_f32_nchwkcrs_nchw_tilesize8x16x64x8_warpsize8x1x1_wngd2x2 Tactic: 0x0190806602534cfd Time: 0.685061\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.560837\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.834853\n",
      "[07/24/2023-05:44:35] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.536968 seconds. Fastest Tactic: 0x3712e3e595645874 Time: 0.447342\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:35] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3712e3e595645874\n",
      "[07/24/2023-05:44:35] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512), Float(1) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:35] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.463547\n",
      "[07/24/2023-05:44:35] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.579877\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.43691\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.847872\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 0.857088\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.762441\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.433006\n",
      "[07/24/2023-05:44:36] [V] [TRT] Fast skip Tactic:0x4fd3c46622e98342 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.0711\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 0.892635\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.850505\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.431543\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.964608\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.535845\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.689737\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.528384\n",
      "[07/24/2023-05:44:36] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.188038 seconds. Fastest Tactic: 0xd15dd11d64344e83 Time: 0.431543\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:36] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd15dd11d64344e83\n",
      "[07/24/2023-05:44:36] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(1) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.330021\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.312466\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.306176\n",
      "[07/24/2023-05:44:36] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0278035 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.306176\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:36] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/24/2023-05:44:36] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.471333\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.46197\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.528635\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.555154\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.313344\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.307493\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.327058\n",
      "[07/24/2023-05:44:36] [V] [TRT] /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0716944 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.307493\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:36] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/24/2023-05:44:36] [V] [TRT] =============== Computing costs for /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu\n",
      "[07/24/2023-05:44:36] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1), Float(4608,9,3,1), Float(1), Float(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.55029\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7 Time: 0.607086\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.551643\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.478062\n",
      "[07/24/2023-05:44:36] [V] [TRT] Fast skip Tactic:0xa9366041633a5135 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 1.44486\n",
      "[07/24/2023-05:44:36] [V] [TRT] Fast skip Tactic:0xcb8a43f748d8a338 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 1.01376\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.465627\n",
      "[07/24/2023-05:44:36] [V] [TRT] Fast skip Tactic:0x40a12e3938221818 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 1.45818\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006 Time: 0.468992\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.806473\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.684617\n",
      "[07/24/2023-05:44:36] [V] [TRT] Fast skip Tactic:0x9d9fdb5fd9945f64 which exceed time limit during pre-run\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 1.00966\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c Time: 0.876251\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395 Time: 0.580023\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a Time: 0.812325\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.678327\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.662528\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.539941\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x8ad32616b1424be4 Time: 0.809861\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.561993\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.839525\n",
      "[07/24/2023-05:44:36] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.138082 seconds. Fastest Tactic: 0x5aa723e0481da855 Time: 0.465627\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:36] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5aa723e0481da855\n",
      "[07/24/2023-05:44:36] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512), Float(4608,9,3,1), Float(1), Float(25088,1,3584,512) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98 Time: 0.896731\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.996823\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee Time: 0.839973\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 1.25279\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x5953bec563d26434 Time: 1.23202\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 1.16251\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.834267\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 1.46414\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x32059de4888dfdda Time: 1.29331\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 1.24401\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83 Time: 0.836608\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 1.33252\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.936375\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 1.09202\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45 Time: 0.93008\n",
      "[07/24/2023-05:44:36] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.177695 seconds. Fastest Tactic: 0x1da91d865428f237 Time: 0.834267\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:36] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1da91d865428f237\n",
      "[07/24/2023-05:44:36] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(4608,9,3,1), Float(1), Float(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.734501\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.717093\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3_alignc4 Tactic: 0x1323e48791e2f671 Time: 0.710656\n",
      "[07/24/2023-05:44:36] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0239731 seconds. Fastest Tactic: 0x1323e48791e2f671 Time: 0.710656\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:36] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1323e48791e2f671\n",
      "[07/24/2023-05:44:36] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128), Float(4608,9,3,1), Float(1), Float(6272,1:4,896,128) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.873911\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.864695\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9 Time: 0.92949\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.956855\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.715483\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6 Time: 0.708453\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.72821\n",
      "[07/24/2023-05:44:36] [V] [TRT] /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0632038 seconds. Fastest Tactic: 0x999e005e3b016ea6 Time: 0.708453\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:36] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x999e005e3b016ea6\n",
      "[07/24/2023-05:44:36] [V] [TRT] =============== Computing costs for /global_pool/pool/GlobalAveragePool\n",
      "[07/24/2023-05:44:36] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1) -> Float(512,1,1,1) ***************\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /global_pool/pool/GlobalAveragePool (CudnnPooling[0x80000005])\n",
      "[07/24/2023-05:44:36] [V] [TRT] CudnnPooling has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /global_pool/pool/GlobalAveragePool (CaskPooling[0x8000002f])\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll6_tThreads49 Tactic: 0xa4a96ea1892462c7 Time: 0.0187103\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll7_tThreads49 Tactic: 0x489ba15aaac78fba Time: 0.0189874\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll2_tThreads49 Tactic: 0x31d30f1a58b7ea39 Time: 0.0187234\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll3_tThreads49 Tactic: 0xdde1c0e17b540744 Time: 0.0186949\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll4_tThreads49 Tactic: 0xee145e7c61eda6b8 Time: 0.0185411\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll1_tThreads49 Tactic: 0x975cf03c939dc33b Time: 0.01904\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm50_xmma_pooling_nd_NCDHW_kAVERAGE_kGENERIC_3D_POOLING_MODE_kFLOAT_0 Tactic: 0xba33c80addb15739 Time: 0.00585728\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll5_tThreads49 Tactic: 0x02269187420e4bc5 Time: 0.018764\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kAVERAGE_tP1_tQ1_tR7_tS7_tU1_tV1_tUnroll8_tThreads49 Tactic: 0xc342539bbc57213f Time: 0.0188343\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Average_FastDiv Tactic: 0x933eceba7b866d59 Time: 0.00493462\n",
      "[07/24/2023-05:44:36] [V] [TRT] /global_pool/pool/GlobalAveragePool (CaskPooling[0x8000002f]) profiling completed in 0.0200181 seconds. Fastest Tactic: 0x933eceba7b866d59 Time: 0.00493462\n",
      "[07/24/2023-05:44:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x933eceba7b866d59\n",
      "[07/24/2023-05:44:36] [V] [TRT] *************** Autotuning format combination: Float(6272,1:4,896,128) -> Float(128,1:4,128,128) ***************\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /global_pool/pool/GlobalAveragePool (CaskPooling[0x8000002f])\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NHWC_Average_FastDiv_CAlign4 Tactic: 0xfab3e2ee1c085a9a Time: 0.00490575\n",
      "[07/24/2023-05:44:36] [V] [TRT] /global_pool/pool/GlobalAveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00253476 seconds. Fastest Tactic: 0xfab3e2ee1c085a9a Time: 0.00490575\n",
      "[07/24/2023-05:44:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xfab3e2ee1c085a9a\n",
      "[07/24/2023-05:44:36] [V] [TRT] =============== Computing costs for /fc/Gemm\n",
      "[07/24/2023-05:44:36] [V] [TRT] *************** Autotuning format combination: Float(512,1,1,1) -> Float(1000,1,1,1) ***************\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e Time: 0.0586606\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb Time: 0.046856\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff Time: 0.0592\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24 Time: 0.058368\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize1x4x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0xede36641840ce3d2 Time: 0.0441783\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa9366041633a5135 Time: 0.0824137\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize4x1x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x1673e3594ce11cea Time: 0.047709\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675 Time: 0.0606674\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xcb8a43f748d8a338 Time: 0.0275992\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855 Time: 0.0395086\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd Time: 0.0250499\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x40a12e3938221818 Time: 0.0839086\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1 Time: 0.112697\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb0bf940d5e0f9f45 Time: 0.0229042\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982 Time: 0.029813\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage2_warpsize2x2x1_g1_ffma_aligna4_alignc4_beta0_packed_stride Tactic: 0x31aa67f57c5aea77 Time: 0.0449931\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505 Time: 0.0401189\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x828d0ea88c66fce7 Time: 0.06032\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a Time: 0.0261364\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e Time: 0.0392549\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303 Time: 0.0474297\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b Time: 0.050176\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x9d9fdb5fd9945f64 Time: 0.0749714\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x16_stage2_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe52b0ddb126aa135 Time: 0.0330606\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4 Time: 0.060544\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338 Time: 0.0507124\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd Time: 0.0493958\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90 Time: 0.0550964\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90f8f2915f87ed77 Time: 0.0229747\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179 Time: 0.0488594\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x32x16_stage2_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xb2c8ebee321e63d6 Time: 0.0405474\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d Time: 0.047104\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0423109\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff Time: 0.0254568\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96 Time: 0.0526141\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30 Time: 0.118071\n",
      "[07/24/2023-05:44:36] [V] [TRT] /fc/Gemm (CaskConvolution[0x80000009]) profiling completed in 0.187207 seconds. Fastest Tactic: 0xb0bf940d5e0f9f45 Time: 0.0229042\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CublasConvolution[0x80000029])\n",
      "[07/24/2023-05:44:36] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskGemmConvolution[0x8000002e])\n",
      "[07/24/2023-05:44:36] [V] [TRT] CaskGemmConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:36] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CudnnConvolution[0x80000000])\n",
      "[07/24/2023-05:44:36] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xb0bf940d5e0f9f45\n",
      "[07/24/2023-05:44:36] [V] [TRT] *************** Autotuning format combination: Float(512,1,512,512) -> Float(1000,1,1000,1000) ***************\n",
      "[07/24/2023-05:44:36] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0 Time: 0.0219736\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x92ed3100c35fc43e Time: 0.0404114\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484 Time: 0.0571002\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86 Time: 0.0308105\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537 Time: 0.0206851\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0 Time: 0.0311013\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808 Time: 0.030517\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49 Time: 0.0536716\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb Time: 0.0314816\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237 Time: 0.0320759\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x16x8_stage3_warpsize4x1x1_g1_ffma_aligna4_alignc4 Tactic: 0x4fd3c46622e98342 Time: 0.0488107\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939 Time: 0.0537356\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd9eb6ca56ddc3a22 Time: 0.0354578\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fb90698107bb33a Time: 0.0157243\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x7121ec1db3f80c67 Time: 0.0159121\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d Time: 0.0204846\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x0a143be7a52f301a Time: 0.0189834\n",
      "[07/24/2023-05:44:36] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd Time: 0.0209208\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xa6448a1e79f1ca6f Time: 0.0428251\n",
      "[07/24/2023-05:44:37] [V] [TRT] /fc/Gemm (CaskConvolution[0x80000009]) profiling completed in 0.0648432 seconds. Fastest Tactic: 0x1fb90698107bb33a Time: 0.0157243\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:37] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CublasConvolution[0x80000029])\n",
      "[07/24/2023-05:44:37] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1fb90698107bb33a\n",
      "[07/24/2023-05:44:37] [V] [TRT] *************** Autotuning format combination: Float(128,1:4,128,128) -> Float(1000,1,1,1) ***************\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0xe8f7b6a5bab325f8 Time: 0.0414354\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_alignc4 Tactic: 0x440241d9c93d605d Time: 0.0387691\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_alignc4 Tactic: 0xc8ad2c0ce0af5623 Time: 0.0388686\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_alignc4 Tactic: 0x9fd9fe001908ce2e Time: 0.0396434\n",
      "[07/24/2023-05:44:37] [V] [TRT] /fc/Gemm (CaskConvolution[0x80000009]) profiling completed in 0.0143192 seconds. Fastest Tactic: 0x440241d9c93d605d Time: 0.0387691\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:37] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x440241d9c93d605d\n",
      "[07/24/2023-05:44:37] [V] [TRT] *************** Autotuning format combination: Float(128,1:4,128,128) -> Float(250,1:4,250,250) ***************\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskConvolution[0x80000009])\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48 Time: 0.0550065\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811 Time: 0.0556785\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d Time: 0.0365246\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4 Time: 0.0550034\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7 Time: 0.0358766\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3 Time: 0.0312\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b Time: 0.0379246\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462 Time: 0.0378011\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b Time: 0.0386194\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482 Time: 0.039968\n",
      "[07/24/2023-05:44:37] [V] [TRT] /fc/Gemm (CaskConvolution[0x80000009]) profiling completed in 0.0338266 seconds. Fastest Tactic: 0xae0c89d047932ba3 Time: 0.0312\n",
      "[07/24/2023-05:44:37] [V] [TRT] /fc/Gemm: 56 available tactics, 0 unparsable, 28 pruned, 28 remaining after tactic pruning.\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskGemmConvolution[0x8000002e])\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 1 numBuffers: 0 numKernels: 1 Tactic: 0x00000000000203be Time: 0.00846871\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 1 numBuffers: 0 numKernels: 1 Tactic: 0x00000000000202f3 Time: 0.00893857\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 1 numKernels: 1 Tactic: 0x00000000020403be Time: 0.0104764\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 1 numKernels: 1 Tactic: 0x00000000020402f3 Time: 0.0107967\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 1 numBuffers: 0 numKernels: 1 Tactic: 0x000000000002031a Time: 0.0141428\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 1 numBuffers: 0 numKernels: 1 Tactic: 0x00000000000202b8 Time: 0.0138219\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 2 numKernels: 2 Tactic: 0x00000002040403be Time: 0.0117387\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 2 numKernels: 2 Tactic: 0x00000002040402f3 Time: 0.0119981\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 2 numKernels: 1 Tactic: 0x00000000040602f3 Time: 0.0117225\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 2 numKernels: 1 Tactic: 0x00000000040603be Time: 0.0112805\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 3 numKernels: 2 Tactic: 0x00000002060603be Time: 0.0135859\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 3 numKernels: 2 Tactic: 0x00000002060602f3 Time: 0.0138265\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 1 numKernels: 1 Tactic: 0x00000000020402b8 Time: 0.0166999\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 1 numKernels: 1 Tactic: 0x000000000204031a Time: 0.0169879\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 3 numKernels: 1 Tactic: 0x00000000060802f3 Time: 0.0133918\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 3 numKernels: 1 Tactic: 0x00000000060803be Time: 0.0129752\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 2 numKernels: 1 Tactic: 0x00000000040602b8 Time: 0.0205636\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 2 numKernels: 1 Tactic: 0x000000000406031a Time: 0.0210606\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 2 numKernels: 2 Tactic: 0x000000020404031a Time: 0.0185771\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 2 numBuffers: 2 numKernels: 2 Tactic: 0x00000002040402b8 Time: 0.0181943\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 4 numKernels: 2 Tactic: 0x00000002080803be Time: 0.0145435\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 4 numKernels: 2 Tactic: 0x00000002080802f3 Time: 0.0151547\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 3 numKernels: 1 Tactic: 0x00000000060802b8 Time: 0.0221479\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 3 numKernels: 1 Tactic: 0x000000000608031a Time: 0.0231621\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 3 numKernels: 2 Tactic: 0x00000002060602b8 Time: 0.0202971\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 3 numBuffers: 3 numKernels: 2 Tactic: 0x000000020606031a Time: 0.021231\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 4 numKernels: 2 Tactic: 0x000000020808031a Time: 0.0229486\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic Name: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8 numSplitK: 4 numBuffers: 4 numKernels: 2 Tactic: 0x00000002080802b8 Time: 0.0219886\n",
      "[07/24/2023-05:44:37] [V] [TRT] /fc/Gemm (CaskGemmConvolution[0x8000002e]) profiling completed in 0.0757577 seconds. Fastest Tactic: 0x00000000000203be Time: 0.00846871\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CaskFlattenConvolution[0x80000036])\n",
      "[07/24/2023-05:44:37] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: /fc/Gemm (CublasConvolution[0x80000029])\n",
      "[07/24/2023-05:44:37] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[07/24/2023-05:44:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskGemmConvolution Tactic: 0x00000000000203be\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing costs for reshape_after_/fc/Gemm\n",
      "[07/24/2023-05:44:37] [V] [TRT] *************** Autotuning format combination: Float(1000,1,1,1) -> Float(1000,1) ***************\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: reshape_after_/fc/Gemm (Shuffle[0x8000000d])\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00422346\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0184709\n",
      "[07/24/2023-05:44:37] [V] [TRT] reshape_after_/fc/Gemm (Shuffle[0x8000000d]) profiling completed in 0.00352867 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00422346\n",
      "[07/24/2023-05:44:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
      "[07/24/2023-05:44:37] [V] [TRT] *************** Autotuning format combination: Float(250,1:4,250,250) -> Float(1000,1) ***************\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: reshape_after_/fc/Gemm (Shuffle[0x8000000d])\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00402095\n",
      "[07/24/2023-05:44:37] [V] [TRT] reshape_after_/fc/Gemm (Shuffle[0x8000000d]) profiling completed in 0.00194911 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00402095\n",
      "[07/24/2023-05:44:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:37] [V] [TRT] *************** Autotuning Reformat: Float(150528,50176,224,1) -> Float(150528,1,672,3) ***************\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(x -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0635444\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.069437\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.155355\n",
      "[07/24/2023-05:44:37] [V] [TRT] Optimizer Reformat(x -> <out>) (Reformat[0x80000006]) profiling completed in 0.00614189 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0635444\n",
      "[07/24/2023-05:44:37] [V] [TRT] *************** Autotuning Reformat: Float(150528,50176,224,1) -> Float(50176,1:4,224,1) ***************\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(x -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.117102\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0711436\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.156526\n",
      "[07/24/2023-05:44:37] [V] [TRT] Optimizer Reformat(x -> <out>) (Reformat[0x80000006]) profiling completed in 0.00687453 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0711436\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs: Identity_0\n",
      "[07/24/2023-05:44:37] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: Identity_0 (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00349904\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0108611\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00440533\n",
      "[07/24/2023-05:44:37] [V] [TRT] Identity_0 (Reformat[0x80000006]) profiling completed in 0.00547277 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00349904\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: Identity_0 (MyelinReformat[0x80000035])\n",
      "[07/24/2023-05:44:37] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/24/2023-05:44:37] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:37] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0042919\n",
      "[07/24/2023-05:44:37] [V] [TRT] Identity_0 (MyelinReformat[0x80000035]) profiling completed in 0.286963 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0042919\n",
      "[07/24/2023-05:44:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs: Identity_2\n",
      "[07/24/2023-05:44:37] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs: Identity_3\n",
      "[07/24/2023-05:44:37] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs: Identity_4\n",
      "[07/24/2023-05:44:37] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs: Identity_1\n",
      "[07/24/2023-05:44:37] [V] [TRT] *************** Autotuning Reformat: Float(4608,9,3,1) -> Float(4608,9,3,1) ***************\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: Identity_1 (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0589272\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0596678\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0592686\n",
      "[07/24/2023-05:44:37] [V] [TRT] Identity_1 (Reformat[0x80000006]) profiling completed in 0.00577724 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0589272\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: Identity_1 (MyelinReformat[0x80000035])\n",
      "[07/24/2023-05:44:37] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/24/2023-05:44:37] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:37] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.058717\n",
      "[07/24/2023-05:44:37] [V] [TRT] Identity_1 (MyelinReformat[0x80000035]) profiling completed in 0.276528 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.058717\n",
      "[07/24/2023-05:44:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: MyelinReformat Tactic: 0x0000000000000000\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:37] [V] [TRT] =============== Computing reformatting costs: Identity_5\n",
      "[07/24/2023-05:44:37] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: Identity_5 (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00358263\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0111789\n",
      "[07/24/2023-05:44:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00441046\n",
      "[07/24/2023-05:44:37] [V] [TRT] Identity_5 (Reformat[0x80000006]) profiling completed in 0.00542281 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00358263\n",
      "[07/24/2023-05:44:37] [V] [TRT] --------------- Timing Runner: Identity_5 (MyelinReformat[0x80000035])\n",
      "[07/24/2023-05:44:38] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/24/2023-05:44:38] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:38] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00428182\n",
      "[07/24/2023-05:44:38] [V] [TRT] Identity_5 (MyelinReformat[0x80000035]) profiling completed in 0.27557 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00428182\n",
      "[07/24/2023-05:44:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs: Identity_7\n",
      "[07/24/2023-05:44:38] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs: Identity_8\n",
      "[07/24/2023-05:44:38] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs: Identity_9\n",
      "[07/24/2023-05:44:38] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs: Identity_6\n",
      "[07/24/2023-05:44:38] [V] [TRT] *************** Autotuning Reformat: Float(2304,9,3,1) -> Float(2304,9,3,1) ***************\n",
      "[07/24/2023-05:44:38] [V] [TRT] --------------- Timing Runner: Identity_6 (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0163627\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0170225\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0167756\n",
      "[07/24/2023-05:44:38] [V] [TRT] Identity_6 (Reformat[0x80000006]) profiling completed in 0.0115844 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0163627\n",
      "[07/24/2023-05:44:38] [V] [TRT] --------------- Timing Runner: Identity_6 (MyelinReformat[0x80000035])\n",
      "[07/24/2023-05:44:38] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/24/2023-05:44:38] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:38] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0162575\n",
      "[07/24/2023-05:44:38] [V] [TRT] Identity_6 (MyelinReformat[0x80000035]) profiling completed in 0.276127 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0162575\n",
      "[07/24/2023-05:44:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: MyelinReformat Tactic: 0x0000000000000000\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs: Identity_10\n",
      "[07/24/2023-05:44:38] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:38] [V] [TRT] --------------- Timing Runner: Identity_10 (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0035656\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.011257\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00438552\n",
      "[07/24/2023-05:44:38] [V] [TRT] Identity_10 (Reformat[0x80000006]) profiling completed in 0.00557874 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0035656\n",
      "[07/24/2023-05:44:38] [V] [TRT] --------------- Timing Runner: Identity_10 (MyelinReformat[0x80000035])\n",
      "[07/24/2023-05:44:38] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/24/2023-05:44:38] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:38] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00434244\n",
      "[07/24/2023-05:44:38] [V] [TRT] Identity_10 (MyelinReformat[0x80000035]) profiling completed in 0.283298 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00434244\n",
      "[07/24/2023-05:44:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs: Identity_12\n",
      "[07/24/2023-05:44:38] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs: Identity_13\n",
      "[07/24/2023-05:44:38] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs: Identity_14\n",
      "[07/24/2023-05:44:38] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs: Identity_11\n",
      "[07/24/2023-05:44:38] [V] [TRT] *************** Autotuning Reformat: Float(1152,9,3,1) -> Float(1152,9,3,1) ***************\n",
      "[07/24/2023-05:44:38] [V] [TRT] --------------- Timing Runner: Identity_11 (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00367906\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0118509\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00464914\n",
      "[07/24/2023-05:44:38] [V] [TRT] Identity_11 (Reformat[0x80000006]) profiling completed in 0.00539964 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00367906\n",
      "[07/24/2023-05:44:38] [V] [TRT] --------------- Timing Runner: Identity_11 (MyelinReformat[0x80000035])\n",
      "[07/24/2023-05:44:38] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/24/2023-05:44:38] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:38] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00429096\n",
      "[07/24/2023-05:44:38] [V] [TRT] Identity_11 (MyelinReformat[0x80000035]) profiling completed in 0.276486 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00429096\n",
      "[07/24/2023-05:44:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:38] [V] [TRT] =============== Computing reformatting costs: Identity_15\n",
      "[07/24/2023-05:44:38] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:38] [V] [TRT] --------------- Timing Runner: Identity_15 (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00353204\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.011245\n",
      "[07/24/2023-05:44:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00450371\n",
      "[07/24/2023-05:44:38] [V] [TRT] Identity_15 (Reformat[0x80000006]) profiling completed in 0.00540654 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00353204\n",
      "[07/24/2023-05:44:38] [V] [TRT] --------------- Timing Runner: Identity_15 (MyelinReformat[0x80000035])\n",
      "[07/24/2023-05:44:39] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/24/2023-05:44:39] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:39] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00429042\n",
      "[07/24/2023-05:44:39] [V] [TRT] Identity_15 (MyelinReformat[0x80000035]) profiling completed in 0.276196 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00429042\n",
      "[07/24/2023-05:44:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: Identity_17\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: Identity_18\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: Identity_19\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(1) -> Float(1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: Identity_16\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(576,9,3,1) -> Float(576,9,3,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Identity_16 (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00365634\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0117848\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00460871\n",
      "[07/24/2023-05:44:39] [V] [TRT] Identity_16 (Reformat[0x80000006]) profiling completed in 0.00551824 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00365634\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Identity_16 (MyelinReformat[0x80000035])\n",
      "[07/24/2023-05:44:39] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/24/2023-05:44:39] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:39] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0043689\n",
      "[07/24/2023-05:44:39] [V] [TRT] Identity_16 (MyelinReformat[0x80000035]) profiling completed in 0.274322 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0043689\n",
      "[07/24/2023-05:44:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(802816,12544,112,1) -> Float(200704,1:4,1792,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.402432\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.31349\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.310857\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0126297 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.310857\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,7168,64) -> Float(802816,12544,112,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.494126\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.313051\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.310889\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0188275 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.310889\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,7168,64) -> Float(200704,1:4,1792,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.327241\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.312613\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.327241\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0118251 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.312613\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1:4,1792,16) -> Float(802816,12544,112,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.499419\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.313051\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.310857\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0129872 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.310857\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0998857\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0826514\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006]) profiling completed in 0.00610912 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.101303\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0826514\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.079472\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006]) profiling completed in 0.00610387 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.079472\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.116855\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0819269\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006]) profiling completed in 0.00617466 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0840251\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0813349\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.083968\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /maxpool/MaxPool_output_0) (Reformat[0x80000006]) profiling completed in 0.00581277 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0813349\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0997692\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0826491\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0793417\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00618349 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0793417\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.101067\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.08272\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0794331\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00671681 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0794331\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.114761\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0822194\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0797257\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00635526 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0797257\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0840434\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0813349\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0841006\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00615168 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0813349\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.116878\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0820663\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0796503\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00643052 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0796503\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.083968\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0812617\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0839589\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/maxpool/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00563825 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0812617\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer1/layer1.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.114688\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.082016\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer1/layer1.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00604972 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0795794\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer1/layer1.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0841074\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0813806\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.084064\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer1/layer1.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00574203 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0813806\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0584168\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0422469\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0411783\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00548943 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0411783\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0602209\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0422766\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0410229\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00561609 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0410229\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0559893\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0426377\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0408137\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00586617 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0408137\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.04352\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0422331\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0434846\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00607802 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0422331\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0564099\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.042712\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0409234\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00587976 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0409234\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0434469\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0422366\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0434023\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer2/layer2.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00595298 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0422366\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(50176,1:4,896,16) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,3136,56,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1:4,896,16) -> Float(200704,1,3584,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.058368\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.042288\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.041064\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00561755 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.041064\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0601554\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0423109\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0410046\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00604967 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0410046\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0559299\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0426594\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0407669\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.0109277 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0407669\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0434869\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.042224\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0434971\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00572501 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.042224\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0563093\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0426949\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0408903\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00589421 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0408903\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0434457\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0419451\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0428503\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer2/layer2.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00597906 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0419451\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0284526\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0234057\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0217535\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00540748 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0217535\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0287442\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0234906\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0216823\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00522699 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0216823\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0281623\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0229701\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0213734\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00537354 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0213734\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0228624\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0227527\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0229238\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00541949 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0227527\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0283718\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0229936\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0213649\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00536345 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0213649\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0228591\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0228767\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0228839\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer3/layer3.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00542067 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0228591\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(25088,1:4,896,32) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,784,28,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1:4,896,32) -> Float(100352,1,3584,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0284625\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0234175\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0216875\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00545753 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0216875\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0287854\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0237616\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0216614\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00540323 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0216614\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0280922\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.022974\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0213394\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00539586 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0213394\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0228415\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0229244\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0228865\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00545147 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0228415\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0284267\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0229917\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0213642\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00617162 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0213642\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0228428\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0227814\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0228833\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer3/layer3.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00557593 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0227814\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0154793\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0121516\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.010687\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00563201 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.010687\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0158989\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0121874\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0106704\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00568555 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0106704\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0138751\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0137687\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0106514\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00582717 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0106514\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0128267\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0118375\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0128369\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00596532 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0118375\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0140094\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0137509\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0106962\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00581869 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0106962\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0128202\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0117788\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0128133\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/layer4/layer4.0/act1/Relu_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00590596 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0117788\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(12544,1:4,896,64) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,196,14,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(12544,1:4,896,64) -> Float(50176,1,3584,256) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0155374\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0121261\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0106426\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00564169 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0106426\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0158775\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0121383\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0105783\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00564862 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0105783\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0138111\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0137721\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.01064\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00582026 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.01064\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0128232\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0119101\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0128411\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00570722 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0119101\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0140351\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0137775\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0106348\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00571175 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0106348\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0128274\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0117408\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0128274\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(<in> -> /layer4/layer4.0/act2/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.0114723 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0117408\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,1,3584,512) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(6272,1:4,896,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(6272,1:4,896,128) -> Float(25088,49,7,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(512,1,512,512) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00408152\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0115246\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00374658\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0054644 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00374658\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(128,1:4,128,128) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00416901\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0110959\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00378129\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.005294 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00378129\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(128,1:4,128,128) -> Float(512,1,1,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00492267\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0148773\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00742674\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00597419 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00492267\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(128,1:4,128,128) -> Float(512,1,512,512) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00789943\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0151561\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00793702\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/global_pool/pool/GlobalAveragePool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0057603 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00789943\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs: \n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(1000,1,1,1) -> Float(250,1:4,250,250) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00789101\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.015013\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00789726\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006]) profiling completed in 0.00588113 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00789101\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(1000,1,1000,1000) -> Float(1000,1,1,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00783495\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0151712\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00813359\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006]) profiling completed in 0.0057974 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00783495\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(1000,1,1000,1000) -> Float(250,1:4,250,250) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00471904\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0113051\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00438345\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006]) profiling completed in 0.00531631 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00438345\n",
      "[07/24/2023-05:44:39] [V] [TRT] *************** Autotuning Reformat: Float(250,1:4,250,250) -> Float(1000,1,1,1) ***************\n",
      "[07/24/2023-05:44:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006])\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00415517\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0113783\n",
      "[07/24/2023-05:44:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00494487\n",
      "[07/24/2023-05:44:39] [V] [TRT] Optimizer Reformat(/fc/Gemm_out_tensor -> <out>) (Reformat[0x80000006]) profiling completed in 0.00583347 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00415517\n",
      "[07/24/2023-05:44:39] [V] [TRT] =============== Computing reformatting costs\n",
      "[07/24/2023-05:44:39] [I] [TRT] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[07/24/2023-05:44:39] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to /conv1/Conv + /act1/Relu (x) from Float(150528,50176,224,1) to Float(50176,1:4,224,1)\n",
      "[07/24/2023-05:44:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (/layer4/layer4.0/act2/Relu_output_0) from Float(6272,1:4,896,128) to Float(25088,1,3584,512)\n",
      "[07/24/2023-05:44:39] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (/layer4/layer4.0/act2/Relu_output_0) from Float(25088,1,3584,512) to Float(6272,1:4,896,128)\n",
      "[07/24/2023-05:44:39] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 3 to /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (/layer4/layer4.0/act2/Relu_output_0) from Float(25088,1,3584,512) to Float(25088,49,7,1)\n",
      "[07/24/2023-05:44:39] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to /fc/Gemm (/global_pool/pool/GlobalAveragePool_output_0) from Float(512,1,1,1) to Float(128,1:4,128,128)\n",
      "[07/24/2023-05:44:39] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to reshape_after_/fc/Gemm (/fc/Gemm_out_tensor) from Float(250,1:4,250,250) to Float(1000,1,1,1)\n",
      "[07/24/2023-05:44:39] [V] [TRT] Formats and tactics selection completed in 12.1264 seconds.\n",
      "[07/24/2023-05:44:39] [V] [TRT] After reformat layers: 70 layers\n",
      "[07/24/2023-05:44:39] [V] [TRT] Total number of blocks in pre-optimized block assignment: 50\n",
      "[07/24/2023-05:44:39] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[07/24/2023-05:44:40] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/24/2023-05:44:40] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:40] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:40] [V] [TRT]  (foreignNode) Set user's cuda kernel library\n",
      "[07/24/2023-05:44:40] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:40] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /conv1/Conv + /act1/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: Identity_1 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: Identity_6 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /maxpool/MaxPool Host Persistent: 4048 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/act1/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 147456\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer2/layer2.0/conv2/Conv Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu Host Persistent: 5296 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 589824\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer3/layer3.0/conv2/Conv Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 2359296\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer4/layer4.0/conv2/Conv Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu Host Persistent: 5424 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu Host Persistent: 5488 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /global_pool/pool/GlobalAveragePool Host Persistent: 4112 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Layer: /fc/Gemm Host Persistent: 7200 Device Persistent: 0 Scratch Memory: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Skipped printing memory information for 45 layers with 0 memory size i.e. Host Persistent + Device Persistent + Scratch Memory == 0.\n",
      "[07/24/2023-05:44:40] [I] [TRT] Total Host Persistent Memory: 123072\n",
      "[07/24/2023-05:44:40] [I] [TRT] Total Device Persistent Memory: 0\n",
      "[07/24/2023-05:44:40] [I] [TRT] Total Scratch Memory: 2359296\n",
      "[07/24/2023-05:44:40] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 9 MiB, GPU 196 MiB\n",
      "[07/24/2023-05:44:40] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 52 steps to complete.\n",
      "[07/24/2023-05:44:40] [V] [TRT] STILL ALIVE: Started step 26 of 52\n",
      "[07/24/2023-05:44:40] [V] [TRT] STILL ALIVE: Started step 51 of 52\n",
      "[07/24/2023-05:44:40] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 5.5872ms to assign 22 blocks to 52 nodes requiring 169049088 bytes.\n",
      "[07/24/2023-05:44:40] [V] [TRT] Total number of blocks in optimized block assignment: 22\n",
      "[07/24/2023-05:44:40] [I] [TRT] Total Activation Memory: 169049088\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /conv1/Conv + /act1/Relu Set kernel index: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /maxpool/MaxPool Set kernel index: 1\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu Set kernel index: 2\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu Set kernel index: 2\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/act1/Relu Set kernel index: 2\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu Set kernel index: 3\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu Set kernel index: 2\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer2/layer2.0/conv2/Conv Set kernel index: 4\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu Set kernel index: 0\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu Set kernel index: 4\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu Set kernel index: 4\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu Set kernel index: 4\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer3/layer3.0/conv2/Conv Set kernel index: 4\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu Set kernel index: 5\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu Set kernel index: 4\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu Set kernel index: 4\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu Set kernel index: 4\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer4/layer4.0/conv2/Conv Set kernel index: 4\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu Set kernel index: 5\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu Set kernel index: 6\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu Set kernel index: 7\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /global_pool/pool/GlobalAveragePool Set kernel index: 8\n",
      "[07/24/2023-05:44:40] [V] [TRT] Finalize: /fc/Gemm Set kernel index: 9\n",
      "[07/24/2023-05:44:40] [V] [TRT] Total number of generated kernels selected for the engine: 10\n",
      "[07/24/2023-05:44:40] [V] [TRT] Kernel: 0 CASK_STATIC\n",
      "[07/24/2023-05:44:40] [V] [TRT] Kernel: 1 CASK_STATIC\n",
      "[07/24/2023-05:44:40] [V] [TRT] Kernel: 2 CASK_STATIC\n",
      "[07/24/2023-05:44:40] [V] [TRT] Kernel: 3 CASK_STATIC\n",
      "[07/24/2023-05:44:40] [V] [TRT] Kernel: 4 CASK_STATIC\n",
      "[07/24/2023-05:44:40] [V] [TRT] Kernel: 5 CASK_STATIC\n",
      "[07/24/2023-05:44:40] [V] [TRT] Kernel: 6 CASK_STATIC\n",
      "[07/24/2023-05:44:40] [V] [TRT] Kernel: 7 CASK_STATIC\n",
      "[07/24/2023-05:44:40] [V] [TRT] Kernel: 8 CASK_STATIC\n",
      "[07/24/2023-05:44:40] [V] [TRT] Kernel: 9 CASK_STATIC\n",
      "[07/24/2023-05:44:40] [V] [TRT] Disabling unused tactic source: JIT_CONVOLUTIONS\n",
      "[07/24/2023-05:44:40] [V] [TRT] Engine generation completed in 12.9033 seconds.\n",
      "[07/24/2023-05:44:40] [V] [TRT] Deleting timing cache: 156 entries, served 180 hits since creation.\n",
      "[07/24/2023-05:44:40] [V] [TRT] Engine Layer Information:\n",
      "Layer(Constant): onnx::Conv_239, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 0) [Constant]_output (Float[512])\n",
      "Layer(Constant): onnx::Conv_239_clone_1, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 0) [Constant]_output_clone_1 (Float[512])\n",
      "Layer(Constant): onnx::Conv_239_clone_2, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 0) [Constant]_output_clone_2 (Float[512])\n",
      "Layer(Constant): onnx::Conv_239_clone_3, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 0) [Constant]_output_clone_3 (Float[512])\n",
      "Layer(Constant): onnx::Conv_241, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 2) [Constant]_output (Float[512,512,3,3])\n",
      "Layer(Constant): onnx::Conv_224, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 7) [Constant]_output (Float[256])\n",
      "Layer(Constant): onnx::Conv_224_clone_1, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 7) [Constant]_output_clone_1 (Float[256])\n",
      "Layer(Constant): onnx::Conv_224_clone_2, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 7) [Constant]_output_clone_2 (Float[256])\n",
      "Layer(Constant): onnx::Conv_224_clone_3, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 7) [Constant]_output_clone_3 (Float[256])\n",
      "Layer(Constant): onnx::Conv_226, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 9) [Constant]_output (Float[256,256,3,3])\n",
      "Layer(Constant): onnx::Conv_209, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 14) [Constant]_output (Float[128])\n",
      "Layer(Constant): onnx::Conv_209_clone_1, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 14) [Constant]_output_clone_1 (Float[128])\n",
      "Layer(Constant): onnx::Conv_209_clone_2, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 14) [Constant]_output_clone_2 (Float[128])\n",
      "Layer(Constant): onnx::Conv_209_clone_3, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 14) [Constant]_output_clone_3 (Float[128])\n",
      "Layer(Constant): onnx::Conv_211, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 16) [Constant]_output (Float[128,128,3,3])\n",
      "Layer(Constant): onnx::Conv_194, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 21) [Constant]_output (Float[64])\n",
      "Layer(Constant): onnx::Conv_194_clone_1, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 21) [Constant]_output_clone_1 (Float[64])\n",
      "Layer(Constant): onnx::Conv_194_clone_2, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 21) [Constant]_output_clone_2 (Float[64])\n",
      "Layer(Constant): onnx::Conv_194_clone_3, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 21) [Constant]_output_clone_3 (Float[64])\n",
      "Layer(Constant): onnx::Conv_199, Tactic: 0x0000000000000000,  -> (Unnamed Layer* 23) [Constant]_output (Float[64,64,3,3])\n",
      "Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to /conv1/Conv + /act1/Relu, Tactic: 0x00000000000003ea, x (Float[-1,3,224,224]) -> Reformatted Input Tensor 0 to /conv1/Conv + /act1/Relu (Float[-1,3:4,224,224])\n",
      "Layer(CaskConvolution): /conv1/Conv + /act1/Relu, Tactic: 0x9cb304e2edbc1221, Reformatted Input Tensor 0 to /conv1/Conv + /act1/Relu (Float[-1,3:4,224,224]) -> /act1/Relu_output_0 (Float[-1,64:4,112,112])\n",
      "Layer(Reformat): Identity_0, Tactic: 0x00000000000003e8, (Unnamed Layer* 0) [Constant]_output (Float[512]) -> onnx::Conv_251 (Float[512])\n",
      "Layer(Reformat): Identity_2, Tactic: 0x00000000000003e8, (Unnamed Layer* 0) [Constant]_output_clone_1 (Float[512]) -> onnx::Conv_248 (Float[512])\n",
      "Layer(Reformat): Identity_3, Tactic: 0x00000000000003e8, (Unnamed Layer* 0) [Constant]_output_clone_2 (Float[512]) -> onnx::Conv_245 (Float[512])\n",
      "Layer(Reformat): Identity_4, Tactic: 0x00000000000003e8, (Unnamed Layer* 0) [Constant]_output_clone_3 (Float[512]) -> onnx::Conv_242 (Float[512])\n",
      "Layer(MyelinReformat): Identity_1, Tactic: 0x0000000000000000, (Unnamed Layer* 2) [Constant]_output (Float[512,512,3,3]) -> onnx::Conv_250 (Float[512,512,3,3])\n",
      "Layer(Reformat): Identity_5, Tactic: 0x00000000000003e8, (Unnamed Layer* 7) [Constant]_output (Float[256]) -> onnx::Conv_236 (Float[256])\n",
      "Layer(Reformat): Identity_7, Tactic: 0x00000000000003e8, (Unnamed Layer* 7) [Constant]_output_clone_1 (Float[256]) -> onnx::Conv_233 (Float[256])\n",
      "Layer(Reformat): Identity_8, Tactic: 0x00000000000003e8, (Unnamed Layer* 7) [Constant]_output_clone_2 (Float[256]) -> onnx::Conv_230 (Float[256])\n",
      "Layer(Reformat): Identity_9, Tactic: 0x00000000000003e8, (Unnamed Layer* 7) [Constant]_output_clone_3 (Float[256]) -> onnx::Conv_227 (Float[256])\n",
      "Layer(MyelinReformat): Identity_6, Tactic: 0x0000000000000000, (Unnamed Layer* 9) [Constant]_output (Float[256,256,3,3]) -> onnx::Conv_235 (Float[256,256,3,3])\n",
      "Layer(Reformat): Identity_10, Tactic: 0x00000000000003e8, (Unnamed Layer* 14) [Constant]_output (Float[128]) -> onnx::Conv_221 (Float[128])\n",
      "Layer(Reformat): Identity_12, Tactic: 0x00000000000003e8, (Unnamed Layer* 14) [Constant]_output_clone_1 (Float[128]) -> onnx::Conv_218 (Float[128])\n",
      "Layer(Reformat): Identity_13, Tactic: 0x00000000000003e8, (Unnamed Layer* 14) [Constant]_output_clone_2 (Float[128]) -> onnx::Conv_215 (Float[128])\n",
      "Layer(Reformat): Identity_14, Tactic: 0x00000000000003e8, (Unnamed Layer* 14) [Constant]_output_clone_3 (Float[128]) -> onnx::Conv_212 (Float[128])\n",
      "Layer(Reformat): Identity_11, Tactic: 0x00000000000003e8, (Unnamed Layer* 16) [Constant]_output (Float[128,128,3,3]) -> onnx::Conv_220 (Float[128,128,3,3])\n",
      "Layer(Reformat): Identity_15, Tactic: 0x00000000000003e8, (Unnamed Layer* 21) [Constant]_output (Float[64]) -> onnx::Conv_206 (Float[64])\n",
      "Layer(Reformat): Identity_17, Tactic: 0x00000000000003e8, (Unnamed Layer* 21) [Constant]_output_clone_1 (Float[64]) -> onnx::Conv_203 (Float[64])\n",
      "Layer(Reformat): Identity_18, Tactic: 0x00000000000003e8, (Unnamed Layer* 21) [Constant]_output_clone_2 (Float[64]) -> onnx::Conv_200 (Float[64])\n",
      "Layer(Reformat): Identity_19, Tactic: 0x00000000000003e8, (Unnamed Layer* 21) [Constant]_output_clone_3 (Float[64]) -> onnx::Conv_197 (Float[64])\n",
      "Layer(Reformat): Identity_16, Tactic: 0x00000000000003e8, (Unnamed Layer* 23) [Constant]_output (Float[64,64,3,3]) -> onnx::Conv_205 (Float[64,64,3,3])\n",
      "Layer(CaskPooling): /maxpool/MaxPool, Tactic: 0x789b2859f2e03e79, /act1/Relu_output_0 (Float[-1,64:4,112,112]) -> /maxpool/MaxPool_output_0 (Float[-1,64:4,56,56])\n",
      "Layer(CaskConvolution): /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/act1/Relu, Tactic: 0x3a8712b17741b582, /maxpool/MaxPool_output_0 (Float[-1,64:4,56,56]), onnx::Conv_197 (Float[64]) -> /layer1/layer1.0/act1/Relu_output_0 (Float[-1,64:4,56,56])\n",
      "Layer(CaskConvolution): /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/act2/Relu, Tactic: 0x3a8712b17741b582, /layer1/layer1.0/act1/Relu_output_0 (Float[-1,64:4,56,56]), onnx::Conv_200 (Float[64]), /maxpool/MaxPool_output_0 (Float[-1,64:4,56,56]) -> /layer1/layer1.0/act2/Relu_output_0 (Float[-1,64:4,56,56])\n",
      "Layer(CaskConvolution): /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/act1/Relu, Tactic: 0x3a8712b17741b582, /layer1/layer1.0/act2/Relu_output_0 (Float[-1,64:4,56,56]), onnx::Conv_203 (Float[64]) -> /layer1/layer1.1/act1/Relu_output_0 (Float[-1,64:4,56,56])\n",
      "Layer(CaskConvolution): /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/act2/Relu, Tactic: 0xb6f6563c77d057d7, /layer1/layer1.1/act1/Relu_output_0 (Float[-1,64:4,56,56]), onnx::Conv_205 (Float[64,64,3,3]), onnx::Conv_206 (Float[64]), /layer1/layer1.0/act2/Relu_output_0 (Float[-1,64:4,56,56]) -> /layer1/layer1.1/act2/Relu_output_0 (Float[-1,64:4,56,56])\n",
      "Layer(CaskConvolution): /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/act1/Relu, Tactic: 0x3a8712b17741b582, /layer1/layer1.1/act2/Relu_output_0 (Float[-1,64:4,56,56]) -> /layer2/layer2.0/act1/Relu_output_0 (Float[-1,128:4,28,28])\n",
      "Layer(CaskConvolution): /layer2/layer2.0/conv2/Conv, Tactic: 0x999e005e3b016ea6, /layer2/layer2.0/act1/Relu_output_0 (Float[-1,128:4,28,28]), onnx::Conv_212 (Float[128]) -> /layer2/layer2.0/conv2/Conv_output_0 (Float[-1,128:4,28,28])\n",
      "Layer(CaskConvolution): /layer2/layer2.0/downsample/downsample.0/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/act2/Relu, Tactic: 0x9cb304e2edbc1221, /layer1/layer1.1/act2/Relu_output_0 (Float[-1,64:4,56,56]), onnx::Conv_215 (Float[128]), /layer2/layer2.0/conv2/Conv_output_0 (Float[-1,128:4,28,28]) -> /layer2/layer2.0/act2/Relu_output_0 (Float[-1,128:4,28,28])\n",
      "Layer(CaskConvolution): /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/act1/Relu, Tactic: 0x999e005e3b016ea6, /layer2/layer2.0/act2/Relu_output_0 (Float[-1,128:4,28,28]), onnx::Conv_218 (Float[128]) -> /layer2/layer2.1/act1/Relu_output_0 (Float[-1,128:4,28,28])\n",
      "Layer(CaskConvolution): /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/act2/Relu, Tactic: 0x999e005e3b016ea6, /layer2/layer2.1/act1/Relu_output_0 (Float[-1,128:4,28,28]), onnx::Conv_220 (Float[128,128,3,3]), onnx::Conv_221 (Float[128]), /layer2/layer2.0/act2/Relu_output_0 (Float[-1,128:4,28,28]) -> /layer2/layer2.1/act2/Relu_output_0 (Float[-1,128:4,28,28])\n",
      "Layer(CaskConvolution): /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/act1/Relu, Tactic: 0x999e005e3b016ea6, /layer2/layer2.1/act2/Relu_output_0 (Float[-1,128:4,28,28]) -> /layer3/layer3.0/act1/Relu_output_0 (Float[-1,256:4,14,14])\n",
      "Layer(CaskConvolution): /layer3/layer3.0/conv2/Conv, Tactic: 0x999e005e3b016ea6, /layer3/layer3.0/act1/Relu_output_0 (Float[-1,256:4,14,14]), onnx::Conv_227 (Float[256]) -> /layer3/layer3.0/conv2/Conv_output_0 (Float[-1,256:4,14,14])\n",
      "Layer(CaskConvolution): /layer3/layer3.0/downsample/downsample.0/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/act2/Relu, Tactic: 0x130df49cb195156b, /layer2/layer2.1/act2/Relu_output_0 (Float[-1,128:4,28,28]), onnx::Conv_230 (Float[256]), /layer3/layer3.0/conv2/Conv_output_0 (Float[-1,256:4,14,14]) -> /layer3/layer3.0/act2/Relu_output_0 (Float[-1,256:4,14,14])\n",
      "Layer(CaskConvolution): /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/act1/Relu, Tactic: 0x999e005e3b016ea6, /layer3/layer3.0/act2/Relu_output_0 (Float[-1,256:4,14,14]), onnx::Conv_233 (Float[256]) -> /layer3/layer3.1/act1/Relu_output_0 (Float[-1,256:4,14,14])\n",
      "Layer(CaskConvolution): /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/act2/Relu, Tactic: 0x999e005e3b016ea6, /layer3/layer3.1/act1/Relu_output_0 (Float[-1,256:4,14,14]), onnx::Conv_235 (Float[256,256,3,3]), onnx::Conv_236 (Float[256]), /layer3/layer3.0/act2/Relu_output_0 (Float[-1,256:4,14,14]) -> /layer3/layer3.1/act2/Relu_output_0 (Float[-1,256:4,14,14])\n",
      "Layer(CaskConvolution): /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/act1/Relu, Tactic: 0x999e005e3b016ea6, /layer3/layer3.1/act2/Relu_output_0 (Float[-1,256:4,14,14]) -> /layer4/layer4.0/act1/Relu_output_0 (Float[-1,512:4,7,7])\n",
      "Layer(CaskConvolution): /layer4/layer4.0/conv2/Conv, Tactic: 0x999e005e3b016ea6, /layer4/layer4.0/act1/Relu_output_0 (Float[-1,512:4,7,7]), onnx::Conv_242 (Float[512]) -> /layer4/layer4.0/conv2/Conv_output_0 (Float[-1,512:4,7,7])\n",
      "Layer(CaskConvolution): /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu, Tactic: 0x130df49cb195156b, /layer3/layer3.1/act2/Relu_output_0 (Float[-1,256:4,14,14]), onnx::Conv_245 (Float[512]), /layer4/layer4.0/conv2/Conv_output_0 (Float[-1,512:4,7,7]) -> Reformatted Output Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (Float[-1,512:4,7,7])\n",
      "Layer(NoOp): Reformatting CopyNode for Output Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to /layer4/layer4.0/downsample/downsample.0/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/act2/Relu (Float[-1,512:4,7,7]) -> /layer4/layer4.0/act2/Relu_output_0 (Float[-1,512,7,7])\n",
      "Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu, Tactic: 0x0000000000000000, /layer4/layer4.0/act2/Relu_output_0 (Float[-1,512,7,7]) -> Reformatted Input Tensor 0 to /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (Float[-1,512:4,7,7])\n",
      "Layer(CaskConvolution): /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu, Tactic: 0x1323e48791e2f671, Reformatted Input Tensor 0 to /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/act1/Relu (Float[-1,512:4,7,7]), onnx::Conv_248 (Float[512]) -> /layer4/layer4.1/act1/Relu_output_0 (Float[-1,512,7,7])\n",
      "Layer(Reformat): Reformatting CopyNode for Input Tensor 3 to /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu, Tactic: 0x0000000000000000, /layer4/layer4.0/act2/Relu_output_0 (Float[-1,512,7,7]) -> Reformatted Input Tensor 3 to /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (Float[-1,512,7,7])\n",
      "Layer(CaskConvolution): /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu, Tactic: 0x5aa723e0481da855, /layer4/layer4.1/act1/Relu_output_0 (Float[-1,512,7,7]), onnx::Conv_250 (Float[512,512,3,3]), onnx::Conv_251 (Float[512]), Reformatted Input Tensor 3 to /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/act2/Relu (Float[-1,512,7,7]) -> /layer4/layer4.1/act2/Relu_output_0 (Float[-1,512,7,7])\n",
      "Layer(CaskPooling): /global_pool/pool/GlobalAveragePool, Tactic: 0x933eceba7b866d59, /layer4/layer4.1/act2/Relu_output_0 (Float[-1,512,7,7]) -> /global_pool/pool/GlobalAveragePool_output_0 (Float[-1,512,1,1])\n",
      "Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to /fc/Gemm, Tactic: 0x0000000000000000, /global_pool/pool/GlobalAveragePool_output_0 (Float[-1,512,1,1]) -> Reformatted Input Tensor 0 to /fc/Gemm (Float[-1,512:4,1,1])\n",
      "Layer(CaskGemmConvolution): /fc/Gemm, Tactic: 0x00000000000203be, Reformatted Input Tensor 0 to /fc/Gemm (Float[-1,512:4,1,1]) -> /fc/Gemm_out_tensor (Float[-1,1000:4,1,1])\n",
      "Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to reshape_after_/fc/Gemm, Tactic: 0x0000000000000000, /fc/Gemm_out_tensor (Float[-1,1000:4,1,1]) -> Reformatted Input Tensor 0 to reshape_after_/fc/Gemm (Float[-1,1000,1,1])\n",
      "Layer(NoOp): reshape_after_/fc/Gemm, Tactic: 0x0000000000000000, Reformatted Input Tensor 0 to reshape_after_/fc/Gemm (Float[-1,1000,1,1]) -> outputs (Float[-1,1000])\n",
      "[07/24/2023-05:44:40] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +45, now: CPU 0, GPU 45 (MiB)\n",
      "[07/24/2023-05:44:40] [V] [TRT] Adding 1 engine(s) to plan file.\n",
      "[07/24/2023-05:44:40] [I] Engine built in 23.7109 sec.\n",
      "[07/24/2023-05:44:41] [I] [TRT] Loaded engine size: 45 MiB\n",
      "[07/24/2023-05:44:41] [V] [TRT] Deserialization required 22776 microseconds.\n",
      "[07/24/2023-05:44:41] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +44, now: CPU 0, GPU 44 (MiB)\n",
      "[07/24/2023-05:44:41] [I] Engine deserialized in 0.0241904 sec.\n",
      "[07/24/2023-05:44:41] [V] [TRT] Total per-runner device persistent memory is 0\n",
      "[07/24/2023-05:44:41] [V] [TRT] Total per-runner host persistent memory is 123072\n",
      "[07/24/2023-05:44:41] [V] [TRT] Allocated activation device memory of size 169049088\n",
      "[07/24/2023-05:44:41] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +161, now: CPU 0, GPU 205 (MiB)\n",
      "[07/24/2023-05:44:41] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "[07/24/2023-05:44:41] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[07/24/2023-05:44:41] [V] Using enqueueV3.\n",
      "[07/24/2023-05:44:41] [I] Using random values for input x\n",
      "[07/24/2023-05:44:41] [I] Input binding for x with dimensions 16x3x224x224 is created.\n",
      "[07/24/2023-05:44:41] [I] Output binding for outputs with dimensions 16x1000 is created.\n",
      "[07/24/2023-05:44:41] [I] Starting inference\n",
      "[07/24/2023-05:44:44] [I] Warmup completed 33 queries over 200 ms\n",
      "[07/24/2023-05:44:44] [I] Timing trace has 506 queries over 3.01743 s\n",
      "[07/24/2023-05:44:44] [I] \n",
      "[07/24/2023-05:44:44] [I] === Trace details ===\n",
      "[07/24/2023-05:44:44] [I] Trace averages of 10 runs:\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94872 ms - Host latency: 6.75793 ms (enqueue 0.0127792 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.9478 ms - Host latency: 6.74593 ms (enqueue 0.0133789 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94852 ms - Host latency: 6.76497 ms (enqueue 0.0085968 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94944 ms - Host latency: 6.74946 ms (enqueue 0.0118378 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94852 ms - Host latency: 6.74577 ms (enqueue 0.0097229 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94647 ms - Host latency: 6.76643 ms (enqueue 0.00896301 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94954 ms - Host latency: 6.74736 ms (enqueue 0.0138184 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94892 ms - Host latency: 6.74858 ms (enqueue 0.0121033 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.9474 ms - Host latency: 6.76367 ms (enqueue 0.0113281 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94811 ms - Host latency: 6.74741 ms (enqueue 0.0111145 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94801 ms - Host latency: 6.75778 ms (enqueue 0.0327576 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94781 ms - Host latency: 6.74947 ms (enqueue 0.0211121 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94841 ms - Host latency: 6.74901 ms (enqueue 0.0144958 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94749 ms - Host latency: 6.76172 ms (enqueue 0.0239502 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94779 ms - Host latency: 6.74475 ms (enqueue 0.0115845 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94767 ms - Host latency: 6.74779 ms (enqueue 0.0245361 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94646 ms - Host latency: 6.75239 ms (enqueue 0.0190918 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94926 ms - Host latency: 6.75361 ms (enqueue 0.0138184 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94628 ms - Host latency: 6.76638 ms (enqueue 0.0208008 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94839 ms - Host latency: 6.74683 ms (enqueue 0.0119751 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94761 ms - Host latency: 6.74727 ms (enqueue 0.0168701 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.9469 ms - Host latency: 6.76376 ms (enqueue 0.0156982 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.95027 ms - Host latency: 6.75 ms (enqueue 0.0137817 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94927 ms - Host latency: 6.74869 ms (enqueue 0.0116455 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94794 ms - Host latency: 6.77061 ms (enqueue 0.0161255 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94956 ms - Host latency: 6.74899 ms (enqueue 0.0140137 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94711 ms - Host latency: 6.76381 ms (enqueue 0.012146 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94943 ms - Host latency: 6.75114 ms (enqueue 0.0145508 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94791 ms - Host latency: 6.75194 ms (enqueue 0.014856 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94604 ms - Host latency: 6.76324 ms (enqueue 0.011084 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94841 ms - Host latency: 6.7451 ms (enqueue 0.0160767 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.948 ms - Host latency: 6.74615 ms (enqueue 0.0168945 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94853 ms - Host latency: 6.76538 ms (enqueue 0.0148682 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94888 ms - Host latency: 6.74883 ms (enqueue 0.0120605 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.9498 ms - Host latency: 6.74592 ms (enqueue 0.00964355 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94939 ms - Host latency: 6.78088 ms (enqueue 0.0159912 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94863 ms - Host latency: 6.74658 ms (enqueue 0.0145752 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94968 ms - Host latency: 6.79495 ms (enqueue 0.0204102 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94807 ms - Host latency: 6.74634 ms (enqueue 0.013208 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.9478 ms - Host latency: 6.74399 ms (enqueue 0.0125488 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94709 ms - Host latency: 6.78306 ms (enqueue 0.0125244 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94895 ms - Host latency: 6.74792 ms (enqueue 0.0114746 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94688 ms - Host latency: 6.74358 ms (enqueue 0.0103516 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94853 ms - Host latency: 6.7699 ms (enqueue 0.0125977 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94863 ms - Host latency: 6.74973 ms (enqueue 0.0122803 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94639 ms - Host latency: 6.77676 ms (enqueue 0.0297119 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94875 ms - Host latency: 6.75144 ms (enqueue 0.0280273 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94907 ms - Host latency: 6.74849 ms (enqueue 0.0136719 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94717 ms - Host latency: 6.7509 ms (enqueue 0.0175049 ms)\n",
      "[07/24/2023-05:44:44] [I] Average on 10 runs - GPU latency: 5.94785 ms - Host latency: 6.74507 ms (enqueue 0.0215576 ms)\n",
      "[07/24/2023-05:44:44] [I] \n",
      "[07/24/2023-05:44:44] [I] === Performance summary ===\n",
      "[07/24/2023-05:44:44] [I] Throughput: 167.692 qps\n",
      "[07/24/2023-05:44:44] [I] Latency: min = 6.72607 ms, max = 6.98682 ms, mean = 6.75498 ms, median = 6.7467 ms, percentile(90%) = 6.76062 ms, percentile(95%) = 6.78857 ms, percentile(99%) = 6.93994 ms\n",
      "[07/24/2023-05:44:44] [I] Enqueue Time: min = 0.00683594 ms, max = 0.0925293 ms, mean = 0.0152431 ms, median = 0.0138702 ms, percentile(90%) = 0.026123 ms, percentile(95%) = 0.0356445 ms, percentile(99%) = 0.0495605 ms\n",
      "[07/24/2023-05:44:44] [I] H2D Latency: min = 0.783936 ms, max = 1.02295 ms, mean = 0.79797 ms, median = 0.788689 ms, percentile(90%) = 0.801819 ms, percentile(95%) = 0.831787 ms, percentile(99%) = 0.984619 ms\n",
      "[07/24/2023-05:44:44] [I] GPU Compute Time: min = 5.93115 ms, max = 5.95865 ms, mean = 5.94816 ms, median = 5.94843 ms, percentile(90%) = 5.95245 ms, percentile(95%) = 5.95355 ms, percentile(99%) = 5.95557 ms\n",
      "[07/24/2023-05:44:44] [I] D2H Latency: min = 0.00732422 ms, max = 0.0268555 ms, mean = 0.00886391 ms, median = 0.00878906 ms, percentile(90%) = 0.00952148 ms, percentile(95%) = 0.00970459 ms, percentile(99%) = 0.0101624 ms\n",
      "[07/24/2023-05:44:44] [I] Total Host Walltime: 3.01743 s\n",
      "[07/24/2023-05:44:44] [I] Total GPU Compute Time: 3.00977 s\n",
      "[07/24/2023-05:44:44] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[07/24/2023-05:44:44] [V] \n",
      "[07/24/2023-05:44:44] [V] === Explanations of the performance metrics ===\n",
      "[07/24/2023-05:44:44] [V] Total Host Walltime: the host walltime from when the first query (after warmups) is enqueued to when the last query is completed.\n",
      "[07/24/2023-05:44:44] [V] GPU Compute Time: the GPU latency to execute the kernels for a query.\n",
      "[07/24/2023-05:44:44] [V] Total GPU Compute Time: the summation of the GPU Compute Time of all the queries. If this is significantly shorter than Total Host Walltime, the GPU may be under-utilized because of host-side overheads or data transfers.\n",
      "[07/24/2023-05:44:44] [V] Throughput: the observed throughput computed by dividing the number of queries by the Total Host Walltime. If this is significantly lower than the reciprocal of GPU Compute Time, the GPU may be under-utilized because of host-side overheads or data transfers.\n",
      "[07/24/2023-05:44:44] [V] Enqueue Time: the host latency to enqueue a query. If this is longer than GPU Compute Time, the GPU may be under-utilized.\n",
      "[07/24/2023-05:44:44] [V] H2D Latency: the latency for host-to-device data transfers for input tensors of a single query.\n",
      "[07/24/2023-05:44:44] [V] D2H Latency: the latency for device-to-host data transfers for output tensors of a single query.\n",
      "[07/24/2023-05:44:44] [V] Latency: the summation of H2D Latency, GPU Compute Time, and D2H Latency. This is the latency to infer a single query.\n",
      "[07/24/2023-05:44:44] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=resnet18.onnx --minShapes=x:1x3x224x224 --optShapes=x:16x3x224x224 --maxShapes=x:32x3x224x224 --useCudaGraph --saveEngine=resnet18.plan --verbose=true\n"
     ]
    }
   ],
   "source": [
    "# change onnx model to tensorrt using trtexec\n",
    "\n",
    "! trtexec --onnx=resnet18.onnx --minShapes=x:1x3x224x224 --optShapes=x:16x3x224x224 --maxShapes=x:32x3x224x224 --useCudaGraph --saveEngine=resnet18.plan --verbose=true \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorrt_handson_lab.tensorrt_utils import common\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "def infer(engine, input_bindings, output_bindings, batch: Dict[str, np.ndarray], batch_size):\n",
    "    for k, val in batch.items():\n",
    "        if input_bindings[k][\"shape\"][0] > val.shape[0]:\n",
    "            padded = np.zeros(dtype=input_bindings[k][\"dtype\"], shape=input_bindings[k][\"shape\"])\n",
    "            padded[: len(val)] = val\n",
    "            batch_size = val.shape[0]\n",
    "        common.memcpy_host_to_device(\n",
    "            input_bindings[k][\"allocation\"],\n",
    "            np.ascontiguousarray(val.astype(input_bindings[k][\"dtype\"])),\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
